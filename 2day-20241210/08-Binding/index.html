


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://lumi-supercomputer.github.io/LUMI-training-materials/2day-20241210/08-Binding/">
      
      
      
      
      <link rel="icon" href="../../assets/favicon-LUMI.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Process and Thread Distribution and Binding - LUMI training materials</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      
  
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
  
  <style>:root{--md-admonition-icon--demo:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20384%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M0%20192h176V0h-16C71.6%200%200%2071.6%200%20160zm0%2032v128c0%2088.4%2071.6%20160%20160%20160h64c88.4%200%20160-71.6%20160-160V224H0m384-32v-32C384%2071.6%20312.4%200%20224%200h-16v192z%22/%3E%3C/svg%3E');--md-admonition-icon--exercise:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M78.6%205c-9.5-7.4-23-6.5-31.6%202L7%2047c-8.5%208.5-9.4%2022-2.1%2031.6l80%20104c4.5%205.9%2011.6%209.4%2019%209.4H158l109%20109c-14.7%2029-10%2065.4%2014.3%2089.6l112%20112c12.5%2012.5%2032.8%2012.5%2045.3%200l64-64c12.5-12.5%2012.5-32.8%200-45.3l-112-112c-24.2-24.2-60.6-29-89.6-14.3L192%20158v-54.1c0-7.5-3.5-14.5-9.4-19zM19.9%20396.1C7.2%20408.8%200%20426.1%200%20444.1%200%20481.6%2030.4%20512%2067.9%20512c18%200%2035.3-7.2%2048-19.9l117.8-117.8c-7.8-20.9-9-43.6-3.6-65.1l-61.7-61.7zM512%20144c0-10.5-1.1-20.7-3.2-30.5-2.4-11.2-16.1-14.1-24.2-6l-63.9%2063.9c-3%203-7.1%204.7-11.3%204.7L352%20176c-8.8%200-16-7.2-16-16v-57.4c0-4.2%201.7-8.3%204.7-11.3l63.9-63.9c8.1-8.1%205.2-21.8-6-24.2C388.7%201.1%20378.5%200%20368%200c-79.5%200-144%2064.5-144%20144v.8l85.3%2085.3c36-9.1%2075.8.5%20104%2028.7l15.7%2015.7c49-23%2083-72.8%2083-130.5M56%20432a24%2024%200%201%201%2048%200%2024%2024%200%201%201-48%200%22/%3E%3C/svg%3E');--md-admonition-icon--remark:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%20448c141.4%200%20256-93.1%20256-208S397.4%2032%20256%2032%200%20125.1%200%20240c0%2045.1%2017.7%2086.8%2047.7%20120.9-1.9%2024.5-11.4%2046.3-21.4%2062.9-5.5%209.2-11.1%2016.6-15.2%2021.6-2.1%202.5-3.7%204.4-4.9%205.7-.6.6-1%201.1-1.3%201.4l-.3.3c-4.6%204.6-5.9%2011.4-3.4%2017.4s8.3%209.9%2014.8%209.9c28.7%200%2057.6-8.9%2081.6-19.3%2022.9-10%2042.4-21.9%2054.3-30.6%2031.8%2011.5%2067%2017.9%20104.1%2017.9zM128%20208a32%2032%200%201%201%200%2064%2032%2032%200%201%201%200-64m128%200a32%2032%200%201%201%200%2064%2032%2032%200%201%201%200-64m96%2032a32%2032%200%201%201%2064%200%2032%2032%200%201%201-64%200%22/%3E%3C/svg%3E');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20640%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M320%200c17.7%200%2032%2014.3%2032%2032v64h120c39.8%200%2072%2032.2%2072%2072v272c0%2039.8-32.2%2072-72%2072H168c-39.8%200-72-32.2-72-72V168c0-39.8%2032.2-72%2072-72h120V32c0-17.7%2014.3-32%2032-32M208%20384c-8.8%200-16%207.2-16%2016s7.2%2016%2016%2016h32c8.8%200%2016-7.2%2016-16s-7.2-16-16-16zm96%200c-8.8%200-16%207.2-16%2016s7.2%2016%2016%2016h32c8.8%200%2016-7.2%2016-16s-7.2-16-16-16zm96%200c-8.8%200-16%207.2-16%2016s7.2%2016%2016%2016h32c8.8%200%2016-7.2%2016-16s-7.2-16-16-16zM264%20256a40%2040%200%201%200-80%200%2040%2040%200%201%200%2080%200m152%2040a40%2040%200%201%200%200-80%2040%2040%200%201%200%200%2080M48%20224h16v192H48c-26.5%200-48-21.5-48-48v-96c0-26.5%2021.5-48%2048-48m544%200c26.5%200%2048%2021.5%2048%2048v96c0%2026.5-21.5%2048-48%2048h-16V224z%22/%3E%3C/svg%3E');--md-admonition-icon--audience:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20640%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M72%2088a56%2056%200%201%201%20112%200%2056%2056%200%201%201-112%200m-8%20157.7c-10%2011.2-16%2026.1-16%2042.3s6%2031.1%2016%2042.3v-84.7zm144.4-49.3C178.7%20222.7%20160%20261.2%20160%20304c0%2034.3%2012%2065.8%2032%2090.5V416c0%2017.7-14.3%2032-32%2032H96c-17.7%200-32-14.3-32-32v-26.8C26.2%20371.2%200%20332.7%200%20288c0-61.9%2050.1-112%20112-112h32c24%200%2046.2%207.5%2064.4%2020.3zM448%20416v-21.5c20-24.7%2032-56.2%2032-90.5%200-42.8-18.7-81.3-48.4-107.7C449.8%20183.5%20472%20176%20496%20176h32c61.9%200%20112%2050.1%20112%20112%200%2044.7-26.2%2083.2-64%20101.2V416c0%2017.7-14.3%2032-32%2032h-64c-17.7%200-32-14.3-32-32m8-328a56%2056%200%201%201%20112%200%2056%2056%200%201%201-112%200m120%20157.7v84.7c10-11.3%2016-26.1%2016-42.3s-6-31.1-16-42.3zM320%2032a64%2064%200%201%201%200%20128%2064%2064%200%201%201%200-128m-80%20272c0%2016.2%206%2031%2016%2042.3v-84.7c-10%2011.3-16%2026.1-16%2042.3zm144-42.3v84.7c10-11.3%2016-26.1%2016-42.3s-6-31.1-16-42.3zm64%2042.3c0%2044.7-26.2%2083.2-64%20101.2V448c0%2017.7-14.3%2032-32%2032h-64c-17.7%200-32-14.3-32-32v-42.8c-37.8-18-64-56.5-64-101.2%200-61.9%2050.1-112%20112-112h32c61.9%200%20112%2050.1%20112%20112%22/%3E%3C/svg%3E');--md-admonition-icon--solution:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20576%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M234.7%2042.7%20197%2056.8c-3%201.1-5%204-5%207.2s2%206.1%205%207.2l37.7%2014.1%2014.1%2037.7c1.1%203%204%205%207.2%205s6.1-2%207.2-5l14.1-37.7L315%2071.2c3-1.1%205-4%205-7.2s-2-6.1-5-7.2l-37.7-14.1L263.2%205c-1.1-3-4-5-7.2-5s-6.1%202-7.2%205zM46.1%20395.4c-18.7%2018.7-18.7%2049.1%200%2067.9l34.6%2034.6c18.7%2018.7%2049.1%2018.7%2067.9%200l381.3-381.4c18.7-18.7%2018.7-49.1%200-67.9l-34.6-34.5c-18.7-18.7-49.1-18.7-67.9%200zM484.6%2082.6l-105%20105-23.3-23.3%20105-105zM7.5%20117.2C3%20118.9%200%20123.2%200%20128s3%209.1%207.5%2010.8L64%20160l21.2%2056.5c1.7%204.5%206%207.5%2010.8%207.5s9.1-3%2010.8-7.5L128%20160l56.5-21.2c4.5-1.7%207.5-6%207.5-10.8s-3-9.1-7.5-10.8L128%2096l-21.2-56.5c-1.7-4.5-6-7.5-10.8-7.5s-9.1%203-10.8%207.5L64%2096zm352%20256c-4.5%201.7-7.5%206-7.5%2010.8s3%209.1%207.5%2010.8L416%20416l21.2%2056.5c1.7%204.5%206%207.5%2010.8%207.5s9.1-3%2010.8-7.5L480%20416l56.5-21.2c4.5-1.7%207.5-6%207.5-10.8s-3-9.1-7.5-10.8L480%20352l-21.2-56.5c-1.7-4.5-6-7.5-10.8-7.5s-9.1%203-10.8%207.5L416%20352z%22/%3E%3C/svg%3E');--md-admonition-icon--seealso:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20640%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M579.8%20267.7c56.5-56.5%2056.5-148%200-204.5-50-50-128.8-56.5-186.3-15.4l-1.6%201.1c-14.4%2010.3-17.7%2030.3-7.4%2044.6s30.3%2017.7%2044.6%207.4l1.6-1.1c32.1-22.9%2076-19.3%20103.8%208.6%2031.5%2031.5%2031.5%2082.5%200%20114L422.3%20334.8c-31.5%2031.5-82.5%2031.5-114%200-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4%206.9-34.4-7.4-44.6s-34.4-6.9-44.6%207.4l-1.1%201.6C206.5%20251.2%20213%20330%20263%20380c56.5%2056.5%20148%2056.5%20204.5%200zM60.2%20244.3c-56.5%2056.5-56.5%20148%200%20204.5%2050%2050%20128.8%2056.5%20186.3%2015.4l1.6-1.1c14.4-10.3%2017.7-30.3%207.4-44.6s-30.3-17.7-44.6-7.4l-1.6%201.1c-32.1%2022.9-76%2019.3-103.8-8.6C74%20372%2074%20321%20105.5%20289.5l112.2-112.3c31.5-31.5%2082.5-31.5%20114%200%2027.9%2027.9%2031.5%2071.8%208.6%20103.9l-1.1%201.6c-10.3%2014.4-6.9%2034.4%207.4%2044.6s34.4%206.9%2044.6-7.4l1.1-1.6C433.5%20260.8%20427%20182%20377%20132c-56.5-56.5-148-56.5-204.5%200z%22/%3E%3C/svg%3E');--md-admonition-icon--technical:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M160%2096a96%2096%200%201%201%20192%200%2096%2096%200%201%201-192%200m80%20152v264l-48.4-24.2c-20.9-10.4-43.5-17-66.8-19.3l-96-9.6C12.5%20457.2%200%20443.5%200%20427V224c0-17.7%2014.3-32%2032-32h30.3c63.6%200%20125.6%2019.6%20177.7%2056m32%20264V248c52.1-36.4%20114.1-56%20177.7-56H480c17.7%200%2032%2014.3%2032%2032v203c0%2016.4-12.5%2030.2-28.8%2031.8l-96%209.6c-23.2%202.3-45.9%208.9-66.8%2019.3z%22/%3E%3C/svg%3E');--md-admonition-icon--intermediate:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20640%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M320%2032c-8.1%200-16.1%201.4-23.7%204.1L15.8%20137.4C6.3%20140.9%200%20149.9%200%20160s6.3%2019.1%2015.8%2022.6l57.9%2020.9C57.3%20229.3%2048%20259.8%2048%20291.9V320c0%2028.4-10.8%2057.7-22.3%2080.8-6.5%2013-13.9%2025.8-22.5%2037.6-3.2%204.3-4.1%209.9-2.3%2015s6%208.9%2011.2%2010.2l64%2016c4.2%201.1%208.7.3%2012.4-2s6.3-6.1%207.1-10.4c8.6-42.8%204.3-81.2-2.1-108.7-3.2-14.2-7.5-28.7-13.5-42v-24.6c0-30.2%2010.2-58.7%2027.9-81.5%2012.9-15.5%2029.6-28%2049.2-35.7l157-61.7c8.2-3.2%2017.5.8%2020.7%209s-.8%2017.5-9%2020.7l-157%2061.7c-12.4%204.9-23.3%2012.4-32.2%2021.6l159.6%2057.6c7.6%202.7%2015.6%204.1%2023.7%204.1s16.1-1.4%2023.7-4.1l280.6-101c9.5-3.4%2015.8-12.5%2015.8-22.6s-6.3-19.1-15.8-22.6L343.7%2036.1c-7.6-2.7-15.6-4.1-23.7-4.1M128%20408c0%2035.3%2086%2072%20192%2072s192-36.7%20192-72l-15.3-145.4L354.5%20314c-11.1%204-22.8%206-34.5%206s-23.5-2-34.5-6l-142.2-51.4z%22/%3E%3C/svg%3E');--md-admonition-icon--advanced:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M219.3.5c3.1-.6%206.3-.6%209.4%200l200%2040C439.9%2042.7%20448%2052.6%20448%2064s-8.1%2021.3-19.3%2023.5L352%20102.9V160c0%2070.7-57.3%20128-128%20128S96%20230.7%2096%20160v-57.1l-48-9.6v65.1l15.7%2078.4c.9%204.7-.3%209.6-3.3%2013.3S52.8%20256%2048%20256H16c-4.8%200-9.3-2.1-12.4-5.9s-4.3-8.6-3.3-13.3L16%20158.4V86.6C6.5%2083.3%200%2074.3%200%2064c0-11.4%208.1-21.3%2019.3-23.5zM111.9%20327.7c10.5-3.4%2021.8.4%2029.4%208.5l71%2075.5c6.3%206.7%2017%206.7%2023.3%200l71-75.5c7.6-8.1%2018.9-11.9%2029.4-8.5%2065%2020.9%20112%2081.7%20112%20153.6%200%2017-13.8%2030.7-30.7%2030.7H30.7C13.8%20512%200%20498.2%200%20481.3c0-71.9%2047-132.7%20111.9-153.6%22/%3E%3C/svg%3E');--md-admonition-icon--lumi-be:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20640%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M48%200C21.5%200%200%2021.5%200%2048v416c0%2026.5%2021.5%2048%2048%2048h96v-80c0-26.5%2021.5-48%2048-48s48%2021.5%2048%2048v80h96c26.5%200%2048-21.5%2048-48V48c0-26.5-21.5-48-48-48zm16%20240c0-8.8%207.2-16%2016-16h32c8.8%200%2016%207.2%2016%2016v32c0%208.8-7.2%2016-16%2016H80c-8.8%200-16-7.2-16-16zm112-16h32c8.8%200%2016%207.2%2016%2016v32c0%208.8-7.2%2016-16%2016h-32c-8.8%200-16-7.2-16-16v-32c0-8.8%207.2-16%2016-16m80%2016c0-8.8%207.2-16%2016-16h32c8.8%200%2016%207.2%2016%2016v32c0%208.8-7.2%2016-16%2016h-32c-8.8%200-16-7.2-16-16zM80%2096h32c8.8%200%2016%207.2%2016%2016v32c0%208.8-7.2%2016-16%2016H80c-8.8%200-16-7.2-16-16v-32c0-8.8%207.2-16%2016-16m80%2016c0-8.8%207.2-16%2016-16h32c8.8%200%2016%207.2%2016%2016v32c0%208.8-7.2%2016-16%2016h-32c-8.8%200-16-7.2-16-16zm112-16h32c8.8%200%2016%207.2%2016%2016v32c0%208.8-7.2%2016-16%2016h-32c-8.8%200-16-7.2-16-16v-32c0-8.8%207.2-16%2016-16M448%200c-17.7%200-32%2014.3-32%2032v480h64V192h144c8.8%200%2016-7.2%2016-16V48c0-8.8-7.2-16-16-16H480c0-17.7-14.3-32-32-32%22/%3E%3C/svg%3E');--md-admonition-icon--nice-to-know:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20384%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%206.7.2%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202024%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M297.2%20248.9c14.4-20.6%2022.8-45.7%2022.8-72.9%200-70.7-57.3-128-128-128S64%20105.3%2064%20176c0%2027.2%208.4%2052.3%2022.8%2072.9%203.7%205.3%208.1%2011.3%2012.8%2017.7%2012.9%2017.7%2028.3%2038.9%2039.8%2059.8%2010.4%2019%2015.7%2038.8%2018.3%2057.5l-48.7.1c-2.2-12-5.9-23.7-11.8-34.5-9.9-18-22.2-34.9-34.5-51.8-5.2-7.1-10.4-14.2-15.4-21.4C27.6%20247.9%2016%20213.3%2016%20176%2016%2078.8%2094.8%200%20192%200s176%2078.8%20176%20176c0%2037.3-11.6%2071.9-31.4%20100.3-5%207.2-10.2%2014.3-15.4%2021.4-12.3%2016.8-24.6%2033.7-34.5%2051.8-5.9%2010.8-9.6%2022.5-11.8%2034.5h-48.6c2.6-18.7%207.9-38.6%2018.3-57.5%2011.5-20.9%2026.9-42.1%2039.8-59.8%204.7-6.4%209-12.4%2012.7-17.7zM192%20128c-26.5%200-48%2021.5-48%2048%200%208.8-7.2%2016-16%2016s-16-7.2-16-16c0-44.2%2035.8-80%2080-80%208.8%200%2016%207.2%2016%2016s-7.2%2016-16%2016m0%20384c-44.2%200-80-35.8-80-80v-16h160v16c0%2044.2-35.8%2080-80%2080%22/%3E%3C/svg%3E');}</style>



    
    
      
    
    
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/neoteroi-mkdocs.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
  
      

    

  
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      document.body.addEventListener("click", function(ev) {
        if (ev.target instanceof HTMLElement) {
          var el = ev.target.closest("a[href^=http]")
          if (el)
            ga("send", "event", "outbound", "click", el.href)
        }
      })
    })
  </script>

    
    

  
  
  
    
  

  
  

  
  <meta property="og:type" content="website" />
  <meta property="og:title" content="LUMI training materials - Process and Thread Distribution and Binding" />
  <meta property="og:description" content="None" />
  <meta property="og:url" content="https://lumi-supercomputer.github.io/LUMI-training-materials/2day-20241210/08-Binding/" />
  <meta property="og:image" content="https://lumi-supercomputer.github.io/LUMI-training-materials/assets/images/banner.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="1080" />
  <meta property="og:image:height" content="568" />

  
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:site" content="@LUMIhpc" />
  <meta name="twitter:creator" content="@LUMIhpc" />
  <meta name="twitter:title" content="LUMI training materials - Process and Thread Distribution and Binding" />
  <meta name="twitter:description" content="None" />
  <meta name="twitter:image" content="https://lumi-supercomputer.github.io/LUMI-training-materials/assets/images/banner.png" />

  <style>
    [data-md-color-primary="lumi"] {
      --md-primary-fg-color: hsla(0, 0%, 100%, 1);
      --md-primary-fg-color--light: hsla(0, 0%, 100%, 0.7);
      --md-primary-fg-color--dark: hsla(0, 0%, 0%, 0.07);
      --md-primary-bg-color: hsla(0, 0%, 0%, 0.87);
      --md-primary-bg-color--light: hsla(0, 0%, 0%, 0.54);
      --md-button-bg-color: hsla(207,100%,28%, 1);
      --md-button-bg-color--light: hsla(207,100%,38%, 1);
      --md-typeset-a-color: hsla(207,100%,28%, 1);
    }
    
    [data-md-color-accent="lumi"] {
      --md-accent-fg-color: hsla(0,0%,0%, 1);
      --md-accent-fg-color--transparent: hsla(0,0%,0%, 0.1);
      --md-accent-bg-color: hsla(0, 0%, 100%, 1);
      --md-accent-bg-color--light: hsla(0, 0%, 100%, 0.7);
    }
  </style>

  


  
  

    <link
      rel="stylesheet"
      href="../../assets/stylesheets/extra-50959fdd9a.css"
    />
    <link
      rel="stylesheet"
      href="../../assets/stylesheets/overrides-5193e6f6df.css"
    />
    <link
      rel="stylesheet"
      id="typekit-fonts-css"
      href="https://use.typekit.net/nlo5lta.css?ver=5.5.3"
      type="text/css" media="all"
    />

  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="lumi" data-md-color-accent="lumi">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#process-and-thread-distribution-and-binding" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LUMI training materials" class="md-header__button md-logo" aria-label="LUMI training materials" data-md-component="logo">
      
  
  <?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg width="100%" height="100%" viewBox="0 0 723 212" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
    <g id="Layer-2" serif:id="Layer 2">
        <g transform="matrix(1,0,0,1,0,175.883)">
            <path d="M0,-140.304L0,0L102.89,0L102.89,-24.599L26.658,-24.599L26.658,-140.304L0,-140.304Z" style="fill:rgb(29,29,27);fill-rule:nonzero;"/>
        </g>
        <g transform="matrix(1,0,0,1,257.262,35.5793)">
            <path d="M0,140.304C-17.585,140.304 -32.083,135.347 -43.588,125.338C-55,115.423 -60.799,103.357 -60.799,89.14L-60.799,0L-34.328,0L-34.328,81.563C-34.328,92.413 -31.054,100.645 -24.319,105.976C-17.866,111.12 -9.728,113.74 0,113.74C9.728,113.74 17.865,111.12 24.319,105.976C31.054,100.645 34.328,92.413 34.328,81.563L34.328,0L60.799,0L60.799,89.14C60.799,103.357 55.093,115.329 43.588,125.338C32.083,135.347 17.585,140.304 0,140.304" style="fill:rgb(29,29,27);fill-rule:nonzero;"/>
        </g>
        <g transform="matrix(1,0,0,1,550.374,119.668)">
            <path d="M0,-27.874L-43.588,20.671L-87.176,-27.874L-87.176,56.215L-113.74,56.215L-113.74,-84.089L-105.695,-84.089L-43.588,-13.47L18.614,-84.089L26.658,-84.089L26.658,56.215L0.094,56.215L0,-27.874Z" style="fill:rgb(29,29,27);fill-rule:nonzero;"/>
        </g>
        <rect x="695.604" y="35.486" width="26.658" height="140.397" style="fill:rgb(29,29,27);"/>
        <g transform="matrix(-1,0,0,1,722.262,-203.194)">
            <rect x="0" y="203.194" width="722.262" height="8.175" style="fill:rgb(29,29,27);"/>
        </g>
        <g transform="matrix(-1,0,0,1,722.262,203.194)">
            <rect x="0" y="0" width="722.262" height="8.175" style="fill:rgb(29,29,27);"/>
        </g>
    </g>
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LUMI training materials
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Process and Thread Distribution and Binding
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LUMI training materials" class="md-nav__button md-logo" aria-label="LUMI training materials" data-md-component="logo">
      
  
  <?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg width="100%" height="100%" viewBox="0 0 723 212" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
    <g id="Layer-2" serif:id="Layer 2">
        <g transform="matrix(1,0,0,1,0,175.883)">
            <path d="M0,-140.304L0,0L102.89,0L102.89,-24.599L26.658,-24.599L26.658,-140.304L0,-140.304Z" style="fill:rgb(29,29,27);fill-rule:nonzero;"/>
        </g>
        <g transform="matrix(1,0,0,1,257.262,35.5793)">
            <path d="M0,140.304C-17.585,140.304 -32.083,135.347 -43.588,125.338C-55,115.423 -60.799,103.357 -60.799,89.14L-60.799,0L-34.328,0L-34.328,81.563C-34.328,92.413 -31.054,100.645 -24.319,105.976C-17.866,111.12 -9.728,113.74 0,113.74C9.728,113.74 17.865,111.12 24.319,105.976C31.054,100.645 34.328,92.413 34.328,81.563L34.328,0L60.799,0L60.799,89.14C60.799,103.357 55.093,115.329 43.588,125.338C32.083,135.347 17.585,140.304 0,140.304" style="fill:rgb(29,29,27);fill-rule:nonzero;"/>
        </g>
        <g transform="matrix(1,0,0,1,550.374,119.668)">
            <path d="M0,-27.874L-43.588,20.671L-87.176,-27.874L-87.176,56.215L-113.74,56.215L-113.74,-84.089L-105.695,-84.089L-43.588,-13.47L18.614,-84.089L26.658,-84.089L26.658,56.215L0.094,56.215L0,-27.874Z" style="fill:rgb(29,29,27);fill-rule:nonzero;"/>
        </g>
        <rect x="695.604" y="35.486" width="26.658" height="140.397" style="fill:rgb(29,29,27);"/>
        <g transform="matrix(-1,0,0,1,722.262,-203.194)">
            <rect x="0" y="203.194" width="722.262" height="8.175" style="fill:rgb(29,29,27);"/>
        </g>
        <g transform="matrix(-1,0,0,1,722.262,203.194)">
            <rect x="0" y="0" width="722.262" height="8.175" style="fill:rgb(29,29,27);"/>
        </g>
    </g>
</svg>

    </a>
    LUMI training materials
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../User-Updates/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    User Updates
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            User Updates
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../User-Updates/Update-202409/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    August-September 2024
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            August-September 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../User-Updates/Update-202409/202409_FAQ/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FAQ
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../User-Updates/Update-202409/202409_Documentation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Documentation links
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../User-Updates/Update-202311/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    October-November 2023
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../User-Updates/Update-202308/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    August 2023
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            August 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../User-Updates/Update-202308/lumig-lownoise/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Low-noise mode
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../User-Updates/Update-202308/lumig-devg/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    dev-g and eap
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../User-Updates/Update-202308/responsible-use/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Responsible use
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../User-Coffee-Breaks/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    User Coffee Breaks
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            User Coffee Breaks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../2day-20250602/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Supercomputing with LUMI June 2025
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Supercomputing with LUMI June 2025
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2day-20250602/schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2day-20250602/index.html#course-materials" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Course materials
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../ai-20250527/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    AI workshop May 2025
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            AI workshop May 2025
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-20250527/schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-20250527/index.html#course-materials" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Course materials
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Profiling-20250507/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Profiling May 2025
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Profiling May 2025
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../2p3day-20250303/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    LUMI intensive March 2025
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_7" id="__nav_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            LUMI intensive March 2025
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2p3day-20250303/schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2p3day-20250303/index.html#course-materials" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Course materials
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../ai-20250204/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    AI workshop February 2025
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_8" id="__nav_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            AI workshop February 2025
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-20250204/schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-20250204/index.html#course-materials" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Course materials
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Hackathon-20241014/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Hackathon October 2024
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            Hackathon October 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Profiling-20241009/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Profiling October 2024
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            Profiling October 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../paow-20240611/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Performance Analysis & Optimization June 2024
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11" id="__nav_11_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            Performance Analysis & Optimization June 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../paow-20240611/schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../paow-20240611/index.html#course-materials" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Course materials
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_12" >
        
          
          <label class="md-nav__link" for="__nav_12" id="__nav_12_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Partner courses
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_12">
            <span class="md-nav__icon md-icon"></span>
            Partner courses
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://zenodo.org/records/10610643" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GROMACS workshop 2024
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13" >
        
          
          <label class="md-nav__link" for="__nav_13" id="__nav_13_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Archive
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_1" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Supercomputing with LUMI December 2024
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_1" id="__nav_13_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_1">
            <span class="md-nav__icon md-icon"></span>
            Supercomputing with LUMI December 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html#course-materials" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Course materials
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../ai-20241126/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    AI workshop November 2024
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_2" id="__nav_13_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_2">
            <span class="md-nav__icon md-icon"></span>
            AI workshop November 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-20241126/schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-20241126/index.html#course-materials" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Course materials
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../4day-20241028/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Advanced LUMI October 2024
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_3" id="__nav_13_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_3">
            <span class="md-nav__icon md-icon"></span>
            Advanced LUMI October 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../4day-20241028/schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../4day-20241028/index.html#course-materials" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Course materials
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_4" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../2day-20240502/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Supercomputing with LUMI May 2024
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_4" id="__nav_13_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_4">
            <span class="md-nav__icon md-icon"></span>
            Supercomputing with LUMI May 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2day-20240502/schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../2day-20240502/index.html#course-materials" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Course materials
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_5" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../ai-20240529/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    AI workshop May 2024
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_5" id="__nav_13_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_5">
            <span class="md-nav__icon md-icon"></span>
            AI workshop May 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-20240529/schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_6" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../4day-20240423/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Comprehensive LUMI April 2024
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_6" id="__nav_13_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_6">
            <span class="md-nav__icon md-icon"></span>
            Comprehensive LUMI April 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../4day-20240423/schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_7" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../1day-20240208/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    1-day February 2024
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_7" id="__nav_13_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_7">
            <span class="md-nav__icon md-icon"></span>
            1-day February 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_01_LUMI_Architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LUMI Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_02_HPE_Cray_Programming_Environment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HPE Cray PE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_03_Modules_on_LUMI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Modules
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_04_LUMI_Software_Stacks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Software stacks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/05_Exercises_1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exercises 1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_06_Running_Jobs_on_LUMI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Running jobs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/07_Exercises_2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exercises 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_08_Introduction_to_Lustre_and_Best_Practices/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    I/O and file systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_09_LUMI_User_Support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LUMI support
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/A01_Documentation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Documentation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_8" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Profiling-20231122/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Profiling November 2023
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_8">
            <span class="md-nav__icon md-icon"></span>
            Profiling November 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_9" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../4day-20231003/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Comprehensive LUMI October 2023
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_9">
            <span class="md-nav__icon md-icon"></span>
            Comprehensive LUMI October 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_10" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../1day-20230921/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    1-day September 2023
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_10" id="__nav_13_10_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_10">
            <span class="md-nav__icon md-icon"></span>
            1-day September 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/01_Architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LUMI Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/02_CPE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HPE Cray PE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/03_Modules/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Modules
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/04_Software_stacks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Software stacks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/05_Exercises_1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exercises 1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/06_Running_jobs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Running jobs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/07_Exercises_2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exercises 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/08_Lustre_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    I/O and file systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/09_LUMI_support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LUMI support
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/A01_Documentation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Documentation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_11" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../4day-20230530/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Comprehensive LUMI May-June 2023
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_11">
            <span class="md-nav__icon md-icon"></span>
            Comprehensive LUMI May-June 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_12" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../1day-20230509/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    1-day May 2023
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_12" id="__nav_13_12_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_12">
            <span class="md-nav__icon md-icon"></span>
            1-day May 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/01_Architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LUMI Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/02_CPE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HPE Cray PE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/03_Modules/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Modules
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/04_Software_stacks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Software stacks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/05_Exercises_1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exercises 1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/06_Running_jobs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Running jobs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/07_Exercises_2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Exercises 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/08_Lustre_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    I/O and file systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/09_LUMI_support/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LUMI support
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_13" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Hackathon-20230417/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Hackathon April 2023
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_13">
            <span class="md-nav__icon md-icon"></span>
            Hackathon April 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_14" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Profiling-20230413/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Profiling April 2013
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_14">
            <span class="md-nav__icon md-icon"></span>
            Profiling April 2013
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_15" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../4day-20230214/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Comprehensive LUMI February 2023
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_15_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_15">
            <span class="md-nav__icon md-icon"></span>
            Comprehensive LUMI February 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_16" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../LUMI-G-20230111/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    LUMI-G January 2023
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_16">
            <span class="md-nav__icon md-icon"></span>
            LUMI-G January 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_17" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../PEAP-Q-20221123/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    PEAP-Q November 2022
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_17" id="__nav_13_17_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_17">
            <span class="md-nav__icon md-icon"></span>
            PEAP-Q November 2022
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../PEAP-Q-20221123/schedule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Schedule
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_18" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../LUMI-G-20220823/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    LUMI-G August 2022
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_18" id="__nav_13_18_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_18">
            <span class="md-nav__icon md-icon"></span>
            LUMI-G August 2022
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LUMI-G-20220823/hackmd_notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    hackmd notes
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_19" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../EasyBuild-CSC-20220509/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    EasyBuild May 2022
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_19" id="__nav_13_19_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_19_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_19">
            <span class="md-nav__icon md-icon"></span>
            EasyBuild May 2022
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://lumi-supercomputer.github.io/easybuild-tutorial/2022-CSC_and_LO/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Course notes
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13_20" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../PEAP-Q-20220427/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    PEAP-Q April 2022
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_13_20" id="__nav_13_20_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_13_20_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13_20">
            <span class="md-nav__icon md-icon"></span>
            PEAP-Q April 2022
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../PEAP-Q-20220427/hackmd_notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    hackmd notes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../PEAP-Q-20220427/software_stacks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LUMI Software Stacks
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-are-we-talking-about-in-this-session" class="md-nav__link">
    <span class="md-ellipsis">
      What are we talking about in this session?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-do-i-need-this" class="md-nav__link">
    <span class="md-ellipsis">
      Why do I need this?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#core-numbering" class="md-nav__link">
    <span class="md-ellipsis">
      Core numbering
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gpu-numbering" class="md-nav__link">
    <span class="md-ellipsis">
      GPU numbering
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#task-distribution-with-slurm" class="md-nav__link">
    <span class="md-ellipsis">
      Task distribution with Slurm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#task-to-cpu-binding-with-slurm" class="md-nav__link">
    <span class="md-ellipsis">
      Task-to-CPU binding with Slurm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#task-to-gpu-binding-with-slurm" class="md-nav__link">
    <span class="md-ellipsis">
      Task-to-GPU binding with Slurm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mpi-rank-redistribution-with-cray-mpich" class="md-nav__link">
    <span class="md-ellipsis">
      MPI rank redistribution with Cray MPICH
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#refining-core-binding-in-openmp-applications" class="md-nav__link">
    <span class="md-ellipsis">
      Refining core binding in OpenMP applications
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gpu-binding-with-rocr_visible_devices" class="md-nav__link">
    <span class="md-ellipsis">
      GPU binding with ROCR_VISIBLE_DEVICES
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#combining-slurm-task-binding-with-rocr_visible_devices" class="md-nav__link">
    <span class="md-ellipsis">
      Combining Slurm task binding with ROCR_VISIBLE_DEVICES
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Combining Slurm task binding with ROCR_VISIBLE_DEVICES">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#linear-assignment-of-gcd-then-match-the-cores" class="md-nav__link">
    <span class="md-ellipsis">
      Linear assignment of GCD, then match the cores
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linear-assignment-of-the-ccds-then-match-the-gcd" class="md-nav__link">
    <span class="md-ellipsis">
      Linear assignment of the CCDs, then match the GCD
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-green-ring" class="md-nav__link">
    <span class="md-ellipsis">
      The green ring
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-about-allocate-by-resources-partitions" class="md-nav__link">
    <span class="md-ellipsis">
      What about "allocate by resources" partitions?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#further-material" class="md-nav__link">
    <span class="md-ellipsis">
      Further material
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="process-and-thread-distribution-and-binding">Process and Thread Distribution and Binding<a class="headerlink" href="#process-and-thread-distribution-and-binding" title="Permanent link">&para;</a></h1>
<p><em>These notes are a quick revision of the notes of a course in May, given by another presenter.
They have not been thoroughly retested.</em></p>
<h2 id="what-are-we-talking-about-in-this-session">What are we talking about in this session?<a class="headerlink" href="#what-are-we-talking-about-in-this-session" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide What are we talking about" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/WhatAbout.png" /></p>
</figure>
<p><strong>Distribution</strong> is the process of distributing processes and threads across the available
resources of the job (nodes, sockets, NUMA nodes, cores, ...), and <strong>binding</strong> is the process
of ensuring they stay there as naturally processes and threads are only bound to a node 
(OS image) but will migrate between cores. Binding can also ensure that processes cannot use
resources they shouldn't use.</p>
<p>When running a distributed memory program, the process starter - <code>mpirun</code> or <code>mpiexec</code>
on many clusters, or <code>srun</code> on LUMI - will <em>distribute</em>  the processes over the available
nodes. Within a node, it is possible to pin or attach processes or even individual threads
in processes to one or more cores (actually hardware threads) and other resources, 
which is called process binding.</p>
<p>The system software (Linux, ROCm&trade; and Slurm) 
has several mechanisms for that. Slurm uses Linux cgroups or control groups to limit the 
resources that a job can use within a node and thus to isolate jobs from one another on a
node so that one job cannot deplete the resources of another job, and sometimes even
uses control groups at the task level to restrict some resources for a task (currently when 
doing task-level GPU binding via Slurm).
The second mechanism is processor affinity which works at the process and thread level and is
used by Slurm at the task level and 
can be used by the OpenMP runtime to further limit thread migration. It works through affinity masks
which indicate the hardware threads that a thread or process can use. There is also a third
mechanism provided by the ROCm&trade; runtime to control which GPUs can be used.</p>
<p>Some of the tools in the <code>lumi-CPEtools</code> module can show the affinity mask for each thread
(or effectively the process for single-threaded processes) so you can use these tools to
study the affinity masks and check the distribution and binding of processes and threads.
The <code>serial_check</code>, <code>omp_check</code>, <code>mpi_check</code> and <code>hybrid_check</code> programs can be used to
study thread binding. In fact, <code>hybrid_check</code> can be used in all cases, but the other three
show more compact output for serial, shared memory OpenMP and single-threaded MPI processes 
respectively. The <code>gpu_check</code> command can be used to study the steps in GPU binding.</p>
<details class="note">
<summary>Credits for these programs</summary>
<p>The <code>hybrid_check</code> program and its derivatives <code>serial_check</code>, <code>omp_check</code> and <code>mpi_check</code>
are similar to the <a href="https://support.hpe.com/hpesc/public/docDisplay?docId=a00114008en_us&amp;docLocale=en_US&amp;page=Run_an_OpenMP_Application.html"><code>xthi</code> program</a>
used in the 4-day comprehensive LUMI course organised by the LUST in collaboration with 
HPE Cray and AMD. Its main source of inspiration is a very similar program,
<code>acheck</code>, written by Harvey Richardson of HPE Cray and used in an earlier course,
but it is a complete rewrite of that application.</p>
<p>One of the advantages of <code>hybrid_check</code> and its derivatives is that the output is 
sorted internally already and hence is more readable. The tool also has various extensions,
e.g., putting some load on the CPU cores so that you can in some cases demonstrate thread
migration as the Linux scheduler tries to distribute the load in a good way.</p>
<p>The <code>gpu_check</code> program builds upon the 
<a href="https://code.ornl.gov/olcf/hello_jobstep/-/tree/master"><code>hello_jobstep</code> program from ORNL</a>
with several extensions implemented by the LUST.</p>
<p>(ORNL is the national lab that operates Frontier, an exascale supercomputer based on the same
node type as LUMI-G.)</p>
</details>
<figure style="border: 1px solid #000">
<p><img alt="Slide When/where is it done" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/WhenDone.png" /></p>
</figure>
<p>In this section we will consider process and thread distribution and binding at several levels:</p>
<ul>
<li>
<p>When creating an allocation, Slurm will already reserve resources at the node level, but this
    has been discussed already in the Slurm session of the course.</p>
<p>It will also already employ control groups to restrict the access to those reaources on a
per-node per-job basis.</p>
</li>
<li>
<p>When creating a job step, Slurm will distribute the tasks over the available resources,
    bind them to CPUs and depending on how the job step was started, bind them to a subset of the
    GPUs available to the task on the node it is running on.</p>
</li>
<li>
<p>With Cray MPICH, you can change the binding between MPI ranks and Slurm tasks. Normally MPI rank <em>i</em>
    would be assigned to task <em>i</em> in the job step, but sometimes there are reasons to change this.
    The mapping options offered by Cray MPICH are more powerful than what can be obtained with the 
    options to change the task distribution in Slurm.</p>
</li>
<li>
<p>The OpenMP runtime also uses library calls and environment variables to redistribute and pin threads
    within the subset of hardware threads available to the process. Note that different compilers
    use different OpenMP runtimes so the default behaviour will not be the same for all compilers,
    and on LUMI is different for the Cray compiler compared to the GNU and AMD compilers.</p>
</li>
<li>
<p>Finally, the ROCm runtime also can limit the use of GPUs by a process to a subset of the ones that
    are available to the process through the use of the <code>ROCR_VISIBLE_DEVICES</code> environment variable.</p>
</li>
</ul>
<p>Binding almost only makes sense on job-exclusive nodes as only then you have full control over all available 
resources. On <a href="../07-Slurm/#partitions">"allocatable by resources"</a> partitions 
you usually do not know which resources are available. 
The advanced Slurm binding options that we will discuss do not work in those cases, and the options offered
by the MPICH, OpenMP and ROCm runtimes may work very unpredictable, though OpenMP thread binding may still 
help a bit with performance in some cases.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note also that some <code>srun</code> options that we have seen (sometimes already given at the <code>sbatch</code> or <code>salloc</code> level
but picket up by <code>srun</code>) already do a simple binding, so those options <strong>cannot be combined</strong> with the options
that we will discuss in this session. This is the case for <code>--cpus-per-task</code>, <code>--gpus-per-task</code> and <code>--ntasks-per-gpu</code>. 
In fact, the latter two options will also change the numbering of the GPUs visible to the ROCm runtime, so 
using <code>ROCR_VISIBLE_DEVICES</code> may also lead to surprises!</p>
</div>
<h2 id="why-do-i-need-this">Why do I need this?<a class="headerlink" href="#why-do-i-need-this" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Why do I need this" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/WhyNeedThis.png" /></p>
</figure>
<!-- BELGIUM 
As we have seen in the ["LUMI Architecture" session of this course](01-Architecture.md) and is discussed into
even more detail in some other courses lectures in Belgium (in particular the
["Supercomputers for Starters" course](https://klust.github.io/SupercomputersForStarters/) 
given twice a year at VSC@UAntwerpen),
modern supercomputer nodes have increasingly a very hierarchical architecture.  This hierarchical architecture is extremely
pronounced on the AMD EPYC architecture used in LUMI but is also increasingly showing up with Intel processors and the ARM
server processors, and is also relevant but often ignored in GPU clusters.
-->

<!-- GENERAL More general version -->
<p>As we have seen in the <a href="../01-Architecture/">"LUMI Architecture" session of this course</a> and as you may know from other courses,
modern supercomputer nodes have increasingly a very hierarchical architecture. This hierarchical architecture is extremely
pronounced on the AMD EPYC architecture used in LUMI but is also increasingly showing up with Intel processors and the ARM
server processors, and is also relevant but often ignored in GPU clusters.</p>
<p>A proper binding of resources to the application is becoming more and more essential for good performance and 
scalability on supercomputers. </p>
<ul>
<li>
<p>Memory locality is very important, and even if an application would be written to take the NUMA character
    properly into account at the thread level, a bad mapping of these threads to the cores may result into threads
    having to access memory that is far away (with the worst case on a different socket) extensively.</p>
<p>Memory locality at the process level is easy as usually processes share little or no memory. So if you would have
an MPI application where each rank needs 14 GB of memory and so only 16 ranks can run on a regular node, then it is
essential to ensure that these ranks are spread out nicely over the whole node, with one rank per CCD. The default of 
Slurm when allocating 16 single-thread tasks on a node would be to put them all on the first two CCDs,
so the first NUMA-domain, which would give
very poor performance as a lot of memory accesses would have to go across sockets.</p>
</li>
<li>
<p>If threads in a process don't have sufficient memory locality it may be very important to run all threads 
    in as few L3 cache domains as possible, ideally just one, as otherwise you risk having a lot of conflicts
    between the different L3 caches that require resolution and can slow down the process a lot.</p>
<p>This already shows that there is no single works-for-all solution, because if those threads would use all memory on a 
node and each have good memory locality then it would be better to spread them out as much possible. You really need
to understand your application to do proper resource mapping, and the fact that it can be so application-dependent is 
also why Slurm and the various runtimes cannot take care of it automatically.</p>
</li>
<li>
<p>In some cases it is important on the GPU nodes to ensure that tasks are nicely spread out over CCDs with each task
    using the GPU (GCD) that is closest to the CCD the task is running on. This is certainly the case if the application
    would rely on cache-coherent access to GPU memory from the CPU.</p>
</li>
<li>
<p>With careful mapping of MPI ranks on nodes you can often reduce the amount of inter-node data transfer in favour of the
    faster intra-node transfers. This requires some understanding of the communication pattern of your MPI application.</p>
</li>
<li>
<p>For GPU-aware MPI: Check if the intra-node communication pattern can map onto the links between the GCDs.</p>
</li>
</ul>
<h2 id="core-numbering">Core numbering<a class="headerlink" href="#core-numbering" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Core numbering" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/CoreNumbering.png" /></p>
</figure>
<p>Linux core numbering is not hierarchical and may look a bit strange. This is because Linux core numbering was fixed
before hardware threads were added, and later on hardware threads were simply added to the numbering scheme.</p>
<p>As is usual with computers, numbering starts from 0. Core 0 is the first hardware thread (or we could say the actual core)
of the first core of the first CCD (CCD 0) of the first NUMA domain (NUMA domain 0) of the first socket (socket 0). Core 1 is then the first hardware thread of the second
core of the same CCD, and so on, going over all cores in a CCD, then NUMA domain and then socket. So on LUMI-C, core 0 till 63
are on the first socket and core 64 till 127 on the second one. The numbering of the second hardware thread of each core - we could
say the virtual core - then starts where the numbering of the actual cores ends, so 64 for LUMI-G (which has only one socket per node)
or 128 for LUMI-C. This has the advantage that if hardware threading is turned off at the BIOS/UEFI level, the numbering of the actual 
cores does not change. </p>
<p>On LUMI G, core 0 and its second hardware thread 64 are reserved by the low noise mode and cannot be used by Slurm or applications.
This is done to help reduce OS jitter which can kill scalability of large parallel applications. However, it also creates an assymetry
that is hard to deal with. (For this reason they chose to disable the first core of every CCD on Frontier, so core 0, 8, 16, ... and 
corresponding hardware threads 64, 72, ..., but on LUMI this is not yet the case).
Don't be surprised if when running a GPU code you see a lot of activity on core 0. It is caused by the ROCm&trade; driver
and is precisely the reason why that core is reserved, as that activity would break scalability of applications that expect
to have the same amount of available compute power on each core.</p>
<p>Note that even with <code>--hint=nomultithread</code> the hardware threads will still be turned on at the hardware level and be visible in the 
OS (e.g., in <code>/proc/cpuinfo</code>). In fact, the batch job step will use them, but they will not be used by applications in job steps
started with subsequent <code>srun</code> commands.</p>
<!-- Script cpu-numbering-demo1 -->
<details class="technical">
<summary>Slurm under-the-hoods example</summary>
<p>We will use the Linux <code>lstopo</code> and <code>taskset</code> commands to study how a job step sees the system
and how task affinity is used to manage the CPUs for a task. Consider the job script:</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=cpu-numbering-demo1</span>
<span class="c1">#SBATCH --output %x-%j.txt</span>
<span class="c1">#SBATCH --account=project_46YXXXXXX</span>
<span class="c1">#SBATCH --partition=small</span>
<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --cpus-per-task=16</span>
<span class="c1">#SBATCH --hint=nomultithread</span>
<span class="c1">#SBATCH --time=5:00</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/24.03<span class="w"> </span>partition/C<span class="w"> </span>lumi-CPEtools/1.1-cpeGNU-24.03

cat<span class="w"> </span><span class="s">&lt;&lt; EOF &gt; task_lstopo_$SLURM_JOB_ID</span>
<span class="s">#!/bin/bash</span>
<span class="s">echo &quot;Task \$SLURM_LOCALID&quot;                            &gt; output-\$SLURM_JOB_ID-\$SLURM_LOCALID</span>
<span class="s">echo &quot;Output of lstopo:&quot;                              &gt;&gt; output-\$SLURM_JOB_ID-\$SLURM_LOCALID</span>
<span class="s">lstopo -p                                             &gt;&gt; output-\$SLURM_JOB_ID-\$SLURM_LOCALID</span>
<span class="s">echo &quot;Taskset of current shell: \$(taskset -p \$\$)&quot;  &gt;&gt; output-\$SLURM_JOB_ID-\$SLURM_LOCALID</span>
<span class="s">EOF</span>

chmod<span class="w"> </span>+x<span class="w"> </span>./task_lstopo_<span class="nv">$SLURM_JOB_ID</span>

<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nFull lstopo output in the job:\n</span><span class="k">$(</span>lstopo<span class="w"> </span>-p<span class="k">)</span><span class="s2">\n\n&quot;</span>
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;Taskset of the current shell: </span><span class="k">$(</span>taskset<span class="w"> </span>-p<span class="w"> </span><span class="nv">$$</span><span class="k">)</span><span class="s2">\n&quot;</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Running two tasks on 4 cores each, extracting parts from lstopo output in each:&quot;</span>
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">2</span><span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>./task_lstopo_<span class="nv">$SLURM_JOB_ID</span>
<span class="nb">echo</span>
cat<span class="w"> </span>output-<span class="nv">$SLURM_JOB_ID</span>-0
<span class="nb">echo</span>
cat<span class="w"> </span>output-<span class="nv">$SLURM_JOB_ID</span>-1

<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nRunning hybrid_check in the same configuration::&quot;</span>
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">2</span><span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>hybrid_check<span class="w"> </span>-r

/bin/rm<span class="w"> </span>task_lstopo_<span class="nv">$SLURM_JOB_ID</span><span class="w"> </span>output-<span class="nv">$SLURM_JOB_ID</span>-0<span class="w"> </span>output-<span class="nv">$SLURM_JOB_ID</span>-1
</code></pre></div></td></tr></table></div>
<p>It creates a small test program that we will use to run lstopo and gather its output
on two tasks with 4 cores each. All this is done in a job allocation with 16 cores on the 
<code>small</code> partition.</p>
<p>The results of this script will differ strongly between runs as Slurm can give different
valid configurations for this request. Below is one possible output we got.</p>
<p>Let's first look at the output of the <code>lstopo</code> and <code>taskset</code> commands run in the batch
job step:</p>
<div class="highlight"><pre><span></span><code>Full lstopo output in the job:
Machine (251GB total)
  Package P#0
    Group0
      NUMANode P#0 (31GB)
    Group0
      NUMANode P#1 (31GB)
      HostBridge
        PCIBridge
          PCI 41:00.0 (Ethernet)
            Net &quot;nmn0&quot;
    Group0
      NUMANode P#2 (31GB)
      HostBridge
        PCIBridge
          PCI 21:00.0 (Ethernet)
            Net &quot;hsn0&quot;
    Group0
      NUMANode P#3 (31GB)
  Package P#1
    Group0
      NUMANode P#4 (31GB)
    Group0
      NUMANode P#5 (31GB)
    Group0
      NUMANode P#6 (31GB)
      L3 P#12 (32MB)
        L2 P#100 (512KB) + L1d P#100 (32KB) + L1i P#100 (32KB) + Core P#36
          PU P#100
          PU P#228
        L2 P#101 (512KB) + L1d P#101 (32KB) + L1i P#101 (32KB) + Core P#37
          PU P#101
          PU P#229
        L2 P#102 (512KB) + L1d P#102 (32KB) + L1i P#102 (32KB) + Core P#38
          PU P#102
          PU P#230
        L2 P#103 (512KB) + L1d P#103 (32KB) + L1i P#103 (32KB) + Core P#39
          PU P#103
          PU P#231
      L3 P#13 (32MB)
        L2 P#104 (512KB) + L1d P#104 (32KB) + L1i P#104 (32KB) + Core P#40
          PU P#104
          PU P#232
        L2 P#105 (512KB) + L1d P#105 (32KB) + L1i P#105 (32KB) + Core P#41
          PU P#105
          PU P#233
        L2 P#106 (512KB) + L1d P#106 (32KB) + L1i P#106 (32KB) + Core P#42
          PU P#106
          PU P#234
        L2 P#107 (512KB) + L1d P#107 (32KB) + L1i P#107 (32KB) + Core P#43
          PU P#107
          PU P#235
        L2 P#108 (512KB) + L1d P#108 (32KB) + L1i P#108 (32KB) + Core P#44
          PU P#108
          PU P#236
        L2 P#109 (512KB) + L1d P#109 (32KB) + L1i P#109 (32KB) + Core P#45
          PU P#109
          PU P#237
        L2 P#110 (512KB) + L1d P#110 (32KB) + L1i P#110 (32KB) + Core P#46
          PU P#110
          PU P#238
        L2 P#111 (512KB) + L1d P#111 (32KB) + L1i P#111 (32KB) + Core P#47
          PU P#111
          PU P#239
    Group0
      NUMANode P#7 (31GB)
      L3 P#14 (32MB)
        L2 P#112 (512KB) + L1d P#112 (32KB) + L1i P#112 (32KB) + Core P#48
          PU P#112
          PU P#240
        L2 P#113 (512KB) + L1d P#113 (32KB) + L1i P#113 (32KB) + Core P#49
          PU P#113
          PU P#241
        L2 P#114 (512KB) + L1d P#114 (32KB) + L1i P#114 (32KB) + Core P#50
          PU P#114
          PU P#242
        L2 P#115 (512KB) + L1d P#115 (32KB) + L1i P#115 (32KB) + Core P#51
          PU P#115
          PU P#243

Taskset of the current shell: pid 81788&#39;s current affinity mask: ffff0000000000000000000000000000ffff0000000000000000000000000
</code></pre></div>
<p>Note the way the cores are represented. 
There are 16 lines the lines <code>L2 ... + L1d ... + L1i ... + Core ...</code> that represent the
16 cores requested. We have used the <code>-p</code> option of <code>lstopo</code> to ensure that <code>lstopo</code>
would show us the physical number as seen by the bare OS. The numbers indicated after
each core are within the socket but the number indicated right after <code>L2</code> is the global
core numbering within the node as seen by the bare OS.
The two <code>PU</code> lines (Processing Unit) after each core are correspond to the 
hardware threads and are also the numbers as seen by the bare OS.</p>
<p>We see that in this allocation the cores are not spread over the minimal number
of L3 cache domains that would be possible, but across three domains. In this particular
allocation the cores are still consecutive cores, but even that is not guaranteed
in an "Allocatable by resources" partition.
Despite <code>--hint=nomultithread</code> being the default behaviour, at this level we still see
both hardware threads for each physical core in the taskset. </p>
<p>Next look at the output printed by lines 29 and 31:</p>
<div class="highlight"><pre><span></span><code>Task 0
Output of lstopo:
Machine (251GB total)
  Package P#0
    Group0
      NUMANode P#0 (31GB)
    Group0
      NUMANode P#1 (31GB)
      HostBridge
        PCIBridge
          PCI 41:00.0 (Ethernet)
            Net &quot;nmn0&quot;
    Group0
      NUMANode P#2 (31GB)
      HostBridge
        PCIBridge
          PCI 21:00.0 (Ethernet)
            Net &quot;hsn0&quot;
    Group0
      NUMANode P#3 (31GB)
  Package P#1
    Group0
      NUMANode P#4 (31GB)
    Group0
      NUMANode P#5 (31GB)
    Group0
      NUMANode P#6 (31GB)
      L3 P#12 (32MB)
        L2 P#100 (512KB) + L1d P#100 (32KB) + L1i P#100 (32KB) + Core P#36
          PU P#100
          PU P#228
        L2 P#101 (512KB) + L1d P#101 (32KB) + L1i P#101 (32KB) + Core P#37
          PU P#101
          PU P#229
        L2 P#102 (512KB) + L1d P#102 (32KB) + L1i P#102 (32KB) + Core P#38
          PU P#102
          PU P#230
        L2 P#103 (512KB) + L1d P#103 (32KB) + L1i P#103 (32KB) + Core P#39
          PU P#103
          PU P#231
      L3 P#13 (32MB)
        L2 P#104 (512KB) + L1d P#104 (32KB) + L1i P#104 (32KB) + Core P#40
          PU P#104
          PU P#232
        L2 P#105 (512KB) + L1d P#105 (32KB) + L1i P#105 (32KB) + Core P#41
          PU P#105
          PU P#233
        L2 P#106 (512KB) + L1d P#106 (32KB) + L1i P#106 (32KB) + Core P#42
          PU P#106
          PU P#234
        L2 P#107 (512KB) + L1d P#107 (32KB) + L1i P#107 (32KB) + Core P#43
          PU P#107
          PU P#235
    Group0
      NUMANode P#7 (31GB)
Taskset of current shell: pid 82340&#39;s current affinity mask: f0000000000000000000000000

Task 1
Output of lstopo:
Machine (251GB total)
  Package P#0
    Group0
      NUMANode P#0 (31GB)
    Group0
      NUMANode P#1 (31GB)
      HostBridge
        PCIBridge
          PCI 41:00.0 (Ethernet)
            Net &quot;nmn0&quot;
    Group0
      NUMANode P#2 (31GB)
      HostBridge
        PCIBridge
          PCI 21:00.0 (Ethernet)
            Net &quot;hsn0&quot;
    Group0
      NUMANode P#3 (31GB)
  Package P#1
    Group0
      NUMANode P#4 (31GB)
    Group0
      NUMANode P#5 (31GB)
    Group0
      NUMANode P#6 (31GB)
      L3 P#12 (32MB)
        L2 P#100 (512KB) + L1d P#100 (32KB) + L1i P#100 (32KB) + Core P#36
          PU P#100
          PU P#228
        L2 P#101 (512KB) + L1d P#101 (32KB) + L1i P#101 (32KB) + Core P#37
          PU P#101
          PU P#229
        L2 P#102 (512KB) + L1d P#102 (32KB) + L1i P#102 (32KB) + Core P#38
          PU P#102
          PU P#230
        L2 P#103 (512KB) + L1d P#103 (32KB) + L1i P#103 (32KB) + Core P#39
          PU P#103
          PU P#231
      L3 P#13 (32MB)
        L2 P#104 (512KB) + L1d P#104 (32KB) + L1i P#104 (32KB) + Core P#40
          PU P#104
          PU P#232
        L2 P#105 (512KB) + L1d P#105 (32KB) + L1i P#105 (32KB) + Core P#41
          PU P#105
          PU P#233
        L2 P#106 (512KB) + L1d P#106 (32KB) + L1i P#106 (32KB) + Core P#42
          PU P#106
          PU P#234
        L2 P#107 (512KB) + L1d P#107 (32KB) + L1i P#107 (32KB) + Core P#43
          PU P#107
          PU P#235
    Group0
      NUMANode P#7 (31GB)
Taskset of current shell: pid 82341&#39;s current affinity mask: f00000000000000000000000000
</code></pre></div>
<p>The output of <code>lstopo -p</code> is the same for both: we get the same 8 cores. This is because
all cores for all tasks on a node are gathered in a single control group. Instead, 
affinity masks are used to ensure that both tasks of 4 threads are scheduled on different
cores. If we have a look at booth taskset lines:</p>
<div class="highlight"><pre><span></span><code>Taskset of current shell: pid 82340&#39;s current affinity mask: 0f0000000000000000000000000
Taskset of current shell: pid 82341&#39;s current affinity mask: f00000000000000000000000000
</code></pre></div>
<p>we see that they are indeed different (a zero was added to the front of the first to
make the difference clearer). The first task got cores 100 till 103 and the second
task got cores 104 till 107. This also shows an important property: Tasksets are
defined based on the bare OS numbering of the cores, not based on a numbering relative
to the control group, with cores numbered from 0 to 15 in this example. It also implies
that it is not possible to set a taskset manually without knowing which physical cores
can be used!</p>
<p>The output of the <code>srun</code> command on line 34 confirms this:</p>
<div class="highlight"><pre><span></span><code>Running 2 MPI ranks with 4 threads each (total number of threads: 8).

++ hybrid_check: MPI rank   0/2   OpenMP thread   0/4   on cpu 101/256 of nid002040 mask 100-103
++ hybrid_check: MPI rank   0/2   OpenMP thread   1/4   on cpu 102/256 of nid002040 mask 100-103
++ hybrid_check: MPI rank   0/2   OpenMP thread   2/4   on cpu 103/256 of nid002040 mask 100-103
++ hybrid_check: MPI rank   0/2   OpenMP thread   3/4   on cpu 100/256 of nid002040 mask 100-103
++ hybrid_check: MPI rank   1/2   OpenMP thread   0/4   on cpu 106/256 of nid002040 mask 104-107
++ hybrid_check: MPI rank   1/2   OpenMP thread   1/4   on cpu 107/256 of nid002040 mask 104-107
++ hybrid_check: MPI rank   1/2   OpenMP thread   2/4   on cpu 104/256 of nid002040 mask 104-107
++ hybrid_check: MPI rank   1/2   OpenMP thread   3/4   on cpu 105/256 of nid002040 mask 104-107
</code></pre></div>
<p>Note however that this output will depend on the compiler used to compile <code>hybrid_check</code>. The Cray
compiler will produce different output as it has a different default strategy for OpenMP threads 
and will by default pin each thread to a different hardware thread if possible.</p>
</details>
<h2 id="gpu-numbering">GPU numbering<a class="headerlink" href="#gpu-numbering" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU numbering (1)" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/GPUNumbering_1.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU numbering (2)" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/GPUNumbering_2.png" /></p>
</figure>
<p>The numbering of the GPUs is a very tricky thing on LUMI.</p>
<p>The only way to reliably identify the physical GPU is through the PCIe bus ID. This does not change over time 
or in an allocation where access to some resources is limited through cgroups. It is the same on all nodes.</p>
<p>Based on these PICe bus IDs, the OS will assign numbers to the GPU. It are those numbers that are shown
in the figure in the
<a href="../01-Architecture/#building-lumi-what-a-lumi-g-node-really-looks-like"> Architecture chapter - "Building LUMI: What a LUMI-G node really looks like"</a>.
We will call this the <em>bare OS numbering</em> or <em>global numbering</em> in these notes.</p>
<p>Slurm manages GPUs for jobs through the control group mechanism. Now if a job requesting 4 GPUs would
get the GPUs that are numbered 4 to 7 in bare OS numbering, 
it would still see them as GPUs 0 to 3, and this is the numbering that one would have to use
for the <code>ROCR_VISIBLE_DEVICES</code> environment variable that is used to further limit the GPUs that the ROCm runtime
will use in an application. We will call this the <em>job-local numbering</em>.</p>
<p>Inside task of a regular job step, Slurm can further restrict the GPUs that are visible through control
groups at the task level, leading to yet another numbering that starts from 0 which we will call the 
<em>task-local numbering</em>. </p>
<p>Note also that Slurm does take care of setting the <code>ROCR_VISIBLE_DEVICES</code> environment variable. It will be set
at the start of a batch job step giving access to all GPUs that are available in the allocation, and will also
be set by <code>srun</code> for each task. But you don't need to know in your application which numbers these are as, e.g.,
the HIP runtime will number the GPUs that are available from 0 on.</p>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU Numbering - Remarks" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/GPUNumberingRemarks.png" /></p>
</figure>
<!-- Script gpu-numbering-demo1 -->
<!-- TODO re-run this example to check the output. -->
<!-- ``` {.bash linenos=true linenostart=1 .copy}  -->
<details class="technical">
<summary>A more technical example demonstrating what Slurm does (click to expand)</summary>
<p>We will use the Linux <code>lstopo</code>command and the <code>ROCR_VISIBLE_DEVICES</code> environment variable
to study how a job step sees the system
and how task affinity is used to manage the CPUs for a task. 
Consider the job script:</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=gpu-numbering-demo1</span>
<span class="c1">#SBATCH --output %x-%j.txt</span>
<span class="c1">#SBATCH --account=project_46YXXXXXX</span>
<span class="c1">#SBATCH --partition=standard-g</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --hint=nomultithread</span>
<span class="c1">#SBATCH --time=15:00</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/24.03<span class="w"> </span>partition/G<span class="w"> </span>lumi-CPEtools/1.1-cpeCray-24.03

cat<span class="w"> </span><span class="s">&lt;&lt; EOF &gt; task_lstopo_$SLURM_JOB_ID</span>
<span class="s">#!/bin/bash</span>
<span class="s">echo &quot;Task \$SLURM_LOCALID&quot;                                                   &gt; output-\$SLURM_JOB_ID-\$SLURM_LOCALID</span>
<span class="s">echo &quot;Relevant lines of lstopo:&quot;                                             &gt;&gt; output-\$SLURM_JOB_ID-\$SLURM_LOCALID</span>
<span class="s">lstopo -p | awk &#39;/ PCI.*Display/ || /GPU/ || / Core / || /PU L/ {print \$0}&#39; &gt;&gt; output-\$SLURM_JOB_ID-\$SLURM_LOCALID</span>
<span class="s">echo &quot;ROCR_VISIBLE_DEVICES: \$ROCR_VISIBLE_DEVICES&quot;                          &gt;&gt; output-\$SLURM_JOB_ID-\$SLURM_LOCALID</span>
<span class="s">EOF</span>
chmod<span class="w"> </span>+x<span class="w"> </span>./task_lstopo_<span class="nv">$SLURM_JOB_ID</span>

<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nFull lstopo output in the job:\n</span><span class="k">$(</span>lstopo<span class="w"> </span>-p<span class="k">)</span><span class="s2">\n\n&quot;</span>
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;Extract GPU info:\n</span><span class="k">$(</span>lstopo<span class="w"> </span>-p<span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;/ PCI.*Display/ || /GPU/ {print $0}&#39;</span><span class="k">)</span><span class="s2">\n&quot;</span><span class="w"> </span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;ROCR_VISIBLE_DEVICES at the start of the job script: </span><span class="nv">$ROCR_VISIBLE_DEVICES</span><span class="s2">&quot;</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Running two tasks with 4 GPUs each, extracting parts from lstopo output in each:&quot;</span>
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">2</span><span class="w"> </span>-c<span class="w"> </span><span class="m">1</span><span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">4</span><span class="w"> </span>./task_lstopo_<span class="nv">$SLURM_JOB_ID</span>
<span class="nb">echo</span>
cat<span class="w"> </span>output-<span class="nv">$SLURM_JOB_ID</span>-0
<span class="nb">echo</span>
cat<span class="w"> </span>output-<span class="nv">$SLURM_JOB_ID</span>-1

<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nRunning gpu_check in the same configuration::&quot;</span>
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">2</span><span class="w"> </span>-c<span class="w"> </span><span class="m">1</span><span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">4</span><span class="w"> </span>gpu_check<span class="w"> </span>-l

/bin/rm<span class="w"> </span>task_lstopo_<span class="nv">$SLURM_JOB_ID</span><span class="w"> </span>output-<span class="nv">$SLURM_JOB_ID</span>-0<span class="w"> </span>output-<span class="nv">$SLURM_JOB_ID</span>-1
</code></pre></div></td></tr></table></div>
<p>It creates a small test program that is run on two tasks and records some information on the system.
The output is not sent to the screen directly as it could end up mixed between the tasks which is far 
from ideal. </p>
<p>Let's first have a look at the first lines of the <code>lstopo -p</code> output:</p>
<div class="highlight"><pre><span></span><code>Full lstopo output in the job:
Machine (503GB total) + Package P#0
  Group0
    NUMANode P#0 (125GB)
    L3 P#0 (32MB)
      L2 P#1 (512KB) + L1d P#1 (32KB) + L1i P#1 (32KB) + Core P#1
        PU P#1
        PU P#65
      L2 P#2 (512KB) + L1d P#2 (32KB) + L1i P#2 (32KB) + Core P#2
        PU P#2
        PU P#66
      L2 P#3 (512KB) + L1d P#3 (32KB) + L1i P#3 (32KB) + Core P#3
        PU P#3
        PU P#67
      L2 P#4 (512KB) + L1d P#4 (32KB) + L1i P#4 (32KB) + Core P#4
        PU P#4
        PU P#68
      L2 P#5 (512KB) + L1d P#5 (32KB) + L1i P#5 (32KB) + Core P#5
        PU P#5
        PU P#69
      L2 P#6 (512KB) + L1d P#6 (32KB) + L1i P#6 (32KB) + Core P#6
        PU P#6
        PU P#70
      L2 P#7 (512KB) + L1d P#7 (32KB) + L1i P#7 (32KB) + Core P#7
        PU P#7
        PU P#71
      HostBridge
        PCIBridge
          PCI d1:00.0 (Display)
            GPU(RSMI) &quot;rsmi4&quot;
    L3 P#1 (32MB)
      L2 P#9 (512KB) + L1d P#9 (32KB) + L1i P#9 (32KB) + Core P#9
        PU P#9
        PU P#73
      L2 P#10 (512KB) + L1d P#10 (32KB) + L1i P#10 (32KB) + Core P#10
        PU P#10
        PU P#74
      L2 P#11 (512KB) + L1d P#11 (32KB) + L1i P#11 (32KB) + Core P#11
        PU P#11
        PU P#75
      L2 P#12 (512KB) + L1d P#12 (32KB) + L1i P#12 (32KB) + Core P#12
        PU P#12
        PU P#76
      L2 P#13 (512KB) + L1d P#13 (32KB) + L1i P#13 (32KB) + Core P#13
        PU P#13
        PU P#77
      L2 P#14 (512KB) + L1d P#14 (32KB) + L1i P#14 (32KB) + Core P#14
        PU P#14
        PU P#78
      L2 P#15 (512KB) + L1d P#15 (32KB) + L1i P#15 (32KB) + Core P#15
        PU P#15
        PU P#79
      HostBridge
        PCIBridge
          PCI d5:00.0 (Ethernet)
            Net &quot;hsn2&quot;
        PCIBridge
          PCI d6:00.0 (Display)
            GPU(RSMI) &quot;rsmi5&quot;
    HostBridge
      PCIBridge
        PCI 91:00.0 (Ethernet)
          Net &quot;nmn0&quot;
...
</code></pre></div>
<p>We see only 7 cores in the each block (the lines <code>L2 ... + L1d ... + L1i ... + Core ...</code>)
because the first physical core on each CCD is reserved for the OS. </p>
<p>The <code>lstopo -p</code> output also clearly suggests that each GCD has a special link to a particular CCD</p>
<p>Next check the output generated by lines 22 and 23 where we select the lines that show information
about the GPUs and print some more information:</p>
<div class="highlight"><pre><span></span><code>Extract GPU info:
          PCI d1:00.0 (Display)
            GPU(RSMI) &quot;rsmi4&quot;
          PCI d6:00.0 (Display)
            GPU(RSMI) &quot;rsmi5&quot;
          PCI c9:00.0 (Display)
            GPU(RSMI) &quot;rsmi2&quot;
          PCI ce:00.0 (Display)
            GPU(RSMI) &quot;rsmi3&quot;
          PCI d9:00.0 (Display)
            GPU(RSMI) &quot;rsmi6&quot;
          PCI de:00.0 (Display)
            GPU(RSMI) &quot;rsmi7&quot;
          PCI c1:00.0 (Display)
            GPU(RSMI) &quot;rsmi0&quot;
          PCI c6:00.0 (Display)
            GPU(RSMI) &quot;rsmi1&quot;

ROCR_VISIBLE_DEVICES at the start of the job script: 0,1,2,3,4,5,6,7
</code></pre></div>
<p>All 8 GPUs are visible and note the numbering on each line below the line with the PCIe bus ID. 
We also notice that <code>ROCR_VISIBLE_DEVICES</code> was set by Slurm and includes all 8 GPUs.</p>
<p>Next we run two tasks requesting 4 GPUs and a single core without hardware threading each. 
The output of those two tasks is gathered in files that are then sent to the standard 
output in lines 28 and 30:</p>
<div class="highlight"><pre><span></span><code>Task 0
Relevant lines of lstopo:
      L2 P#1 (512KB) + L1d P#1 (32KB) + L1i P#1 (32KB) + Core P#1
      L2 P#2 (512KB) + L1d P#2 (32KB) + L1i P#2 (32KB) + Core P#2
          PCI d1:00.0 (Display)
          PCI d6:00.0 (Display)
          PCI c9:00.0 (Display)
            GPU(RSMI) &quot;rsmi2&quot;
          PCI ce:00.0 (Display)
            GPU(RSMI) &quot;rsmi3&quot;
          PCI d9:00.0 (Display)
          PCI de:00.0 (Display)
          PCI c1:00.0 (Display)
            GPU(RSMI) &quot;rsmi0&quot;
          PCI c6:00.0 (Display)
            GPU(RSMI) &quot;rsmi1&quot;
ROCR_VISIBLE_DEVICES: 0,1,2,3

Task 1
Relevant lines of lstopo:
      L2 P#1 (512KB) + L1d P#1 (32KB) + L1i P#1 (32KB) + Core P#1
      L2 P#2 (512KB) + L1d P#2 (32KB) + L1i P#2 (32KB) + Core P#2
          PCI d1:00.0 (Display)
            GPU(RSMI) &quot;rsmi0&quot;
          PCI d6:00.0 (Display)
            GPU(RSMI) &quot;rsmi1&quot;
          PCI c9:00.0 (Display)
          PCI ce:00.0 (Display)
          PCI d9:00.0 (Display)
            GPU(RSMI) &quot;rsmi2&quot;
          PCI de:00.0 (Display)
            GPU(RSMI) &quot;rsmi3&quot;
          PCI c1:00.0 (Display)
          PCI c6:00.0 (Display)
ROCR_VISIBLE_DEVICES: 0,1,2,3
</code></pre></div>
<p>Each task sees GPUs named 'rsmi0' till 'rsmi3', but look better and you see that these are
not the same. If you compare with the first output of <code>lstopo</code> which we ran in the batch job step,
we notice that task 0 gets the first 4 GPUs in the node while task 1 gets the next 4, that
were named <code>rsmi4</code> till <code>rsmi7</code> before. 
The other 4 GPUs are invisible in each of the tasks. Note also that in both tasks 
<code>ROCR_VISIBLE_DEVICES</code> has the same value <code>0,1,2,3</code> as the numbers detected by <code>lstopo</code> in that
task are used. </p>
<p>The <code>lstopo</code> command does see two cores though for each task (but they are the same) because
the cores are not isolated by cgroups on a per-task level, but on a per-job level.</p>
<p>Finally we have the output of the <code>gpu_check</code> command run in the same configuration. The <code>-l</code> option
that was used prints some extra information that makes it easier to check the mapping: For the hardware
threads it shows the CCD and for each GPU it shows the GCD number based on the physical order of the GPUs
and the corresponding CCD that should be used for best performance:</p>
<div class="highlight"><pre><span></span><code>MPI 000 - OMP 000 - HWT 001 (CCD0) - Node nid005163 - RT_GPU_ID 0,1,2,3 - GPU_ID 0,1,2,3 - Bus_ID c1(GCD0/CCD6),c6(GCD1/CCD7),c9(GCD2/CCD2),cc(GCD3/CCD3)
MPI 001 - OMP 000 - HWT 002 (CCD0) - Node nid005163 - RT_GPU_ID 0,1,2,3 - GPU_ID 0,1,2,3 - Bus_ID d1(GCD4/CCD0),d6(GCD5/CCD1),d9(GCD6/CCD4),dc(GCD7/CCD5)
</code></pre></div>
<p><code>RT_GPU_ID</code> is the numbering of devices used in the program itself, <code>GPU_ID</code> is essentially the value of <code>ROCR_VISIBLE_DEVICES</code>,
the logical numbers of the GPUs in the control group
and <code>Bus_ID</code> shows the relevant part of the PCIe bus ID.</p>
</details>
<p>The above example is very technical and not suited for every reader. One important conclusion though
that is of use when running on LUMI is that <strong>Slurm works differently with CPUs and GPUs on LUMI</strong>. 
Cores and GPUs are treated differently. Cores access is controlled by control groups at the
job step level on each node and at the task level by affinity masks. 
The equivalent for GPUs would be to also use control groups at the job step level and then
<code>ROCR_VISIBLE_DEVICES</code> to further set access to GPUs for each task, but this is not what 
is currently happening in Slurm on LUMI. Instead it is using control groups at the 
task level. </p>
<!-- Script gpu-numbering-demo2 -->
<details class="technical">
<summary>Playing with control group and <code>ROCR_VISIBLE_DEVICES</code> (click to expand)</summary>
<p>Consider the following (tricky and maybe not very realistic) job script.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=gpu-numbering-demo2</span>
<span class="c1">#SBATCH --output %x-%j.txt</span>
<span class="c1">#SBATCH --partition=standard-g</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --hint=nomultithread</span>
<span class="c1">#SBATCH --time=5:00</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/24.03<span class="w"> </span>partition/G<span class="w"> </span>lumi-CPEtools/1.1-cpeCray-24.03

cat<span class="w"> </span><span class="s">&lt;&lt; EOF &gt; select_1gpu_$SLURM_JOB_ID</span>
<span class="s">#!/bin/bash</span>
<span class="s">export ROCR_VISIBLE_DEVICES=\$SLURM_LOCALID</span>
<span class="s">exec \$*</span>
<span class="s">EOF</span>
chmod<span class="w"> </span>+x<span class="w"> </span>./select_1gpu_<span class="nv">$SLURM_JOB_ID</span>

cat<span class="w"> </span><span class="s">&lt;&lt; EOF &gt; task_lstopo_$SLURM_JOB_ID</span>
<span class="s">#!/bin/bash</span>
<span class="s">sleep \$((SLURM_LOCALID * 5))</span>
<span class="s">echo &quot;Task \$SLURM_LOCALID&quot;                                                   &gt; output-\$SLURM_JOB_ID-\$SLURM_LOCALID</span>
<span class="s">echo &quot;Relevant lines of lstopo:&quot;                                             &gt;&gt; output-\$SLURM_JOB_ID-\$SLURM_LOCALID</span>
<span class="s">lstopo -p | awk &#39;/ PCI.*Display/ || /GPU/ || / Core / || /PU L/ {print \$0}&#39; &gt;&gt; output-\$SLURM_JOB_ID-\$SLURM_LOCALID</span>
<span class="s">echo &quot;ROCR_VISIBLE_DEVICES: \$ROCR_VISIBLE_DEVICES&quot;                          &gt;&gt; output-\$SLURM_JOB_ID-\$SLURM_LOCALID</span>
<span class="s">EOF</span>
chmod<span class="w"> </span>+x<span class="w"> </span>./task_lstopo_<span class="nv">$SLURM_JOB_ID</span>

<span class="c1"># Start a background task to pick GPUs with global numbers 0 and 1</span>
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">1</span><span class="w"> </span>--gpus<span class="o">=</span><span class="m">2</span><span class="w"> </span>sleep<span class="w"> </span><span class="m">60</span><span class="w"> </span><span class="p">&amp;</span>
sleep<span class="w"> </span><span class="m">5</span>

<span class="nb">set</span><span class="w"> </span>-x
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">4</span><span class="w"> </span>-c<span class="w"> </span><span class="m">1</span><span class="w"> </span>--gpus<span class="o">=</span><span class="m">4</span><span class="w"> </span>./task_lstopo_<span class="nv">$SLURM_JOB_ID</span>
<span class="nb">set</span><span class="w"> </span>+x

cat<span class="w"> </span>output-<span class="nv">$SLURM_JOB_ID</span>-0

<span class="nb">set</span><span class="w"> </span>-x
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">4</span><span class="w"> </span>-c<span class="w"> </span><span class="m">1</span><span class="w"> </span>--gpus<span class="o">=</span><span class="m">4</span><span class="w"> </span>./select_1gpu_<span class="nv">$SLURM_JOB_ID</span><span class="w"> </span>gpu_check<span class="w"> </span>-l
<span class="nb">set</span><span class="w"> </span>+x

<span class="nb">wait</span>

/bin/rm<span class="w"> </span>select_1gpu_<span class="nv">$SLURM_JOB_ID</span><span class="w"> </span>task_lstopo_<span class="nv">$SLURM_JOB_ID</span><span class="w"> </span>output-<span class="nv">$SLURM_JOB_ID</span>-*
</code></pre></div></td></tr></table></div>
<p>We create two small programs that we will use in here. The first one is used to set
<code>ROCR_VISIBLE_DEVICES</code> to the value of <code>SLURM_LOCALID</code> which is the local task number
within a node of a Slurm task (so always numbered starting from 0 per node). We will use
this to tell the <code>gpu_check</code> program that we will run which GPU should be used by which task.
The second program is one we have seen before already and just shows some relevant output
of <code>lstopo</code> to see which GPUs are in principle available to the task and then also prints
the value of <code>ROCR_VISIBLE_DEVICES</code>. We did have to put in some task-dependent delay 
as it turns out that running multiple <code>lstopo</code> commands on a node together can cause
problems.</p>
<p>The tricky bit is line 29. Here we start an <code>srun</code> command on the background that steals
two GPUs. In this way, we ensure that the next <code>srun</code> command will not be able to get the
GCDs 0 and 1 from the regular full-node numbering. The delay is again to ensure that the
next <code>srun</code> works without conflicts as internally Slurm is still finishing steps from
the first <code>srun</code>.</p>
<p>On line 33 we run our command that extracts info from <code>lstopo</code>.
As we already know from the more technical example above the output will be the same for each
task so in line 36 we only look at the output of the first task:</p>
<div class="highlight"><pre><span></span><code>Relevant lines of lstopo:
      L2 P#2 (512KB) + L1d P#2 (32KB) + L1i P#2 (32KB) + Core P#2
      L2 P#3 (512KB) + L1d P#3 (32KB) + L1i P#3 (32KB) + Core P#3
      L2 P#4 (512KB) + L1d P#4 (32KB) + L1i P#4 (32KB) + Core P#4
      L2 P#5 (512KB) + L1d P#5 (32KB) + L1i P#5 (32KB) + Core P#5
          PCI d1:00.0 (Display)
            GPU(RSMI) &quot;rsmi2&quot;
          PCI d6:00.0 (Display)
            GPU(RSMI) &quot;rsmi3&quot;
          PCI c9:00.0 (Display)
            GPU(RSMI) &quot;rsmi0&quot;
          PCI ce:00.0 (Display)
            GPU(RSMI) &quot;rsmi1&quot;
          PCI d9:00.0 (Display)
          PCI de:00.0 (Display)
          PCI c1:00.0 (Display)
          PCI c6:00.0 (Display)
ROCR_VISIBLE_DEVICES: 0,1,2,3
</code></pre></div>
<p>If you'd compare with output from a full-node <code>lstopo -p</code> shown in the previous example, you'd see that
we actually got the GPUs with regular full node numbering 2 till 5, but they have been renumbered from 
0 to 3. And notice that <code>ROCR_VISIBLE_DEVICES</code> now also refers to this numbering and not the 
regular full node numbering when setting which GPUs can be used. </p>
<p>The <code>srun</code> command on line 40 will now run <code>gpu_check</code> through the <code>select_1gpu_$SLURM_JOB_ID</code>
wrapper that gives task 0 access to GPU 0 in the "local" numbering, which should be GPU2/CCD2
in the regular full node numbering, etc. Its output is</p>
<div class="highlight"><pre><span></span><code>MPI 000 - OMP 000 - HWT 002 (CCD0) - Node nid005350 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)
MPI 001 - OMP 000 - HWT 003 (CCD0) - Node nid005350 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID cc(GCD3/CCD3)
MPI 002 - OMP 000 - HWT 004 (CCD0) - Node nid005350 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID d1(GCD4/CCD0)
MPI 003 - OMP 000 - HWT 005 (CCD0) - Node nid005350 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID d6(GCD5/CCD1)
</code></pre></div>
<p>which confirms that out strategy worked. So in this example we have 4 tasks running in a control group
that in principle gives each task access to all 4 GPUs, but with actual access further restricted to
a different GPU per task via <code>ROCR_VISIBLE_DEVICES</code>.</p>
</details>
<p>This again rather technical example demonstrates another difference between the way one works with 
CPUs and with GPUs. Affinity masks for CPUs refer to the "bare OS" numbering of hardware threads,
while the numbering used for <code>ROCR_VISIBLE_DEVICES</code> which determines which GPUs the ROCm runtime can use,
uses the numbering within the current control group.</p>
<p><strong>Running GPUs in a different control group per task has consequences for the way inter-GPU
communication within a node can be organised so the above examples are important. It is essential
to run MPI applications with optimal efficiency.</strong></p>
<h2 id="task-distribution-with-slurm">Task distribution with Slurm<a class="headerlink" href="#task-distribution-with-slurm" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Task distribution with Slurm" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/SlurmTaskDistribution_1.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide Task distribution with Slurm (2)" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/SlurmTaskDistribution_2.png" /></p>
</figure>
<p>The Slurm <code>srun</code> command offers the <code>--distribution</code> option to influence the distribution of 
tasks across nodes (level 1), sockets or NUMA domains (level 2 and sockets or NUMA) or 
even across cores in the socket or NUMA domain (third level). The first level is the most useful level,
the second level is sometimes used but the third level is very tricky and both the second and third level
are often better replaced with other mechanisms that will also be discussed in this chapter on distribution
and binding.</p>
<p>The <a href="https://slurm.schedmd.com/archive/slurm-23.02.7/srun.html#OPT_distribution">general form of the <code>--distribution</code> option</a> is </p>
<div class="highlight"><pre><span></span><code>--distribution={*|block|cyclic|arbitrary|plane=&lt;size&gt;}[:{*|block|cyclic|fcyclic}[:{*|block|cyclic|fcyclic}]][,{Pack|NoPack}]
</code></pre></div>
<ul>
<li>
<p>Level 1: Distribution across nodes. There are three useful options for LUMI:</p>
<ul>
<li>
<p><code>block</code> which is the default: A number of consecutive tasks is allocated on the first
    node, then another number of consecutive tasks on the second node, and so on till the last
    node of the allocation. Not all nodes may have the same number of tasks and this is determined
    by the optional  <code>pack</code> or <code>nopack</code> parameter at the end.</p>
<ul>
<li>
<p>With <code>pack</code> the first node in the allocation is first filled up as much as possible, then the
    second node, etc.</p>
</li>
<li>
<p>With <code>nopack</code> a more balanced approach is taken filling up all nodes as equally as possible.
    In fact, the number of tasks on each node will correspond to that of the <code>cyclic</code> distribution,
    but the task numbers will be different.</p>
</li>
</ul>
</li>
<li>
<p><code>cyclic</code> assigns the tasks in a round-robin fashion to the nodes of the allocation. The first task
    is allocated to the first node, then the second one to the second node, and so on, and when all nodes
    of the allocation have received one task, the next one will be allocated again on the first node. </p>
</li>
<li>
<p><code>plane=&lt;size&gt;</code> is a combination of both of the former methods: Blocks of <code>&lt;size&gt;</code> consecutive tasks
    are allocated in a cyclic way. </p>
</li>
</ul>
</li>
<li>
<p>Level 2: Here we are distributing and pinning the tasks assigned to a node at level 1 across the sockets
    and cores of that node.</p>
<p>As this option already does a form of binding, it may conflict with other options that we will discuss later
that also perform binding. In practice, this second level is less useful as often other mechanisms will be 
preferred for doing a proper binding, or the default behaviour is OK for simple distribution problems.</p>
<ul>
<li>
<p><code>block</code> will assign whole tasks to consecutive sets of cores on the node. On LUMI-C, it will first fill up
    the first socket before moving on to the second socket.</p>
</li>
<li>
<p><code>cyclic</code> assigns the first task of a node to a set of consecutive cores on the first socket, then the second task to a set 
    of cores on the second socket, etc., in a round-robin way. It will do its best to not allocate tasks across sockets.</p>
</li>
<li>
<p><code>fcyclic</code> is a very strange distribution, where tasks requesting more than 1 CPU per task will see those 
    spread out across sockets. </p>
<p>We cannot see how this is useful on an AMD CPU except for cases where we have only one task per node which accesses
a lot of memory (more than offered by a single socket) but does so in a very NUMA-aware way.</p>
</li>
</ul>
</li>
<li>
<p>Level 3 is beyond the scope of an introductory course and rarely used.</p>
</li>
</ul>
<p>The default behaviour of Slurm depends on LUMI seems to be <code>block:block,nopack</code> if <code>--distribution</code> is not specified,
though it is best to always verify as it can change over time and as the manual indicates that the
default differs according to the number of tasks compared to the number of nodes.
The defaults are also very tricky if a binding option at level 2 (or 3) is replaced with a <code>*</code> to mark
the default behaviour, e.g., <code>--distribution="block:*"</code> gives the result of <code>--distribution=block:cyclic</code>
while <code>--distribution=block</code> has the same effect as <code>--distribution=block:block</code>.</p>
<p><strong>This option only makes sense on job-exclusive nodes.</strong></p>
<h2 id="task-to-cpu-binding-with-slurm">Task-to-CPU binding with Slurm<a class="headerlink" href="#task-to-cpu-binding-with-slurm" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Task-to-CPU binding with Slurm" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/SlurmTaskCPU.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide Task-to-CPU binding with Slurm: Masks" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/SlurmTaskCPUMasks.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide Task-to-CPU binding with Slurm: Examples" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/SlurmTaskCPUExamples.png" /></p>
</figure>
<p>The level 2 and 3 options from the previous section already do some binding. But we will now 
discuss a different option that enables very precise binding of tasks to hardware threads in Slurm.</p>
<p>The mechanism does conflict with some Slurm options that implicitly already do some binding, e.g., 
it will not always work together with <code>--cpus-per-task</code> and <code>--hint=[no]multithread</code> may also not 
act as expected depending on how the options are used. 
Level &#8532; control via <code>--distribution</code> sometimes also make no sense when this option is used
(and will be ignored).</p>
<p>Task-to-CPU binding is controlled through the Slurm option </p>
<div class="highlight"><pre><span></span><code>--cpu-bind=[{quiet|verbose},]&lt;type&gt;
</code></pre></div>
<p>We'll describe a few of the possibilities for the <code>&lt;type&gt;</code> parameter but for a more concrete overview
we refer to the <a href="https://slurm.schedmd.com/archive/slurm-23.02.7/srun.html#OPT_cpu-bind">Slurm <code>srun</code> manual page</a></p>
<ul>
<li>
<p><code>--cpu-bind=threads</code> is the default behaviour on LUMI.</p>
</li>
<li>
<p><code>--cpu-bind=map_cpu:&lt;cpu_id_for_task_0&gt;,&lt;cpu_id_for_task_1&gt;, ...</code> is used when tasks are bound to single
    cores. The first number is the number of the hardware thread for the task with local task ID 0, etc. 
    In other words, this option at the same time also defines the slots that can be used by the 
    <code>--distribution</code> option above and replaces level 2 and level 3 of that option. </p>
<p>E.g.,</p>
<div class="highlight"><pre><span></span><code>salloc --nodes=1 --partition=standard-g
module load LUMI/24.03 partition/G lumi-CPEtools/1.1-cpeGNU-24.03
srun --ntasks=8 --cpu-bind=map_cpu:49,57,17,25,1,9,33,41 mpi_check -r
</code></pre></div>
<p>will run the first task on hardware threads 49, the second task on 57, third on 17, fourth on 
25, fifth on 1, sixth on 9, seventh on 33 and eight on 41.</p>
<p>This may look like a very strange numbering, but we will see an application for it further in
this chapter.</p>
</li>
<li>
<p><code>--cpu-bind=mask_cpu:&lt;mask_for_task_0&gt;,&lt;mask_for_task_1&gt;,...</code> is similar to <code>map_cpu</code>, but now multiple
    hardware threads can be specified per task through a mask. The mask is a hexadecimal number and leading 
    zeros can be omitted. The least significant bit in the mask corresponds to HWT 0, etc. </p>
<p>Masks can become very long, but we shall see that this option is very useful on the nodes of the 
<code>standard-g</code> partition. Just as with <code>map_cpu</code>, this option replaces level 2 and 3 of the <code>--distribution</code>
option. </p>
<p>E.g.,</p>
<div class="highlight"><pre><span></span><code>salloc --nodes=1 --partition=standard-g
module load LUMI/24.03 partition/G lumi-CPEtools/1.1-cpeGNU-24.03
srun --ntasks=8 --cpu-bind=mask_cpu:7e000000000000,7e00000000000000,7e0000,7e000000,7e,7e00,7e00000000,7e0000000000 hybrid_check -r
</code></pre></div>
<p>will run the first task on hardware threads 49-54, the second task on 57-62, third on 17-22, fourth on 
25-30, fifth on 1-6, sixth on 9-14, seventh on 33-38 and eight on 41-46.</p>
</li>
</ul>
<p>The <code>--cpu-bind=map_cpu</code> and <code>--cpu-bind=mask_gpu</code> options also do not go together with <code>-c</code> / <code>--cpus-per-task</code>.
Both commands define a binding (the latter in combination with the default <code>--cpu-bind=threads</code>) 
and these will usually conflict.</p>
<p>There are more options, but these are currently most relevant ones on LUMI. That may change in the future as
LUMI User Support is investigating whether it isn't better to change the concept of "socket" in Slurm given how important it
sometimes is to carefully map onto L3 cache domains for performance.</p>
<details class="note">
<summary>How to understand masks?</summary>
<p>Masks indicate which (virtual) cores can be used. A mask is really a series of bits with 
each bit corresponding to a virtual core. The least significant bit corresponds to core 0,
the next one to core 1, etc. A 1-bit indicates that the corresponding virtual core can be used
while a 0-bit indicates that it cannot be used. </p>
<p>Consider the mask <code>1111000001011010</code>. For readability, we split it up in groups of 4 from right to left: </p>
<p><table style="border-collapse: collapse; border: none;">
  <tbody style="border: none;">
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Core</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; line-height: 0.85; padding:0px 0px 0px 0px; border: none;">
             1111 1100 0000 0000
        </br>5432 1098 7654 3210</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Mask</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      1111 0000 0101 1010</td>
    </tr>
  </tbody>
</table></p>
<p>Hence this masks indicates that cores 1, 3, 4, 6, 12, 13, 14 and 15 can be used.</p>
<p>Now using such long bit strings is awkward. There is a long tradition among computer
scientists to represent such bit strings instead as hexadecimal numbers: numbers base 16,
instead of as bit strings, numbers base 2. Each hexadecimal digit then corresponds with 
4 bits, and we start assigning those again from the 4 least significant bits to the 
most significant bits, adding 0s at the front to get a multiple of 4 bits. The conversion
is given by the following table.</p>
<p><table>
<thead>
  <tr>
    <td style="font-weight: bold;">decimal</tc>
    <td style="font-weight: bold;">binary</td>
    <td style="font-weight: bold;">hexadecimal</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">0</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">0000</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">0</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">1</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">0001</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">1</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">2</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">0010</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">2</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">3</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">0011</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">3</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">4</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">0100</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">4</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">5</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">0101</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">5</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">6</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">0110</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">6</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">7</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">0111</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">7</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">8</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">1000</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">8</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">9</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">1001</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">9</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">10</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">1010</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">a</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">11</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">1011</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">b</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">12</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">1100</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">c</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">13</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">1101</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">d</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">14</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">1110</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">e</td>
  </tr>
  <tr>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">15</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">1111</td>
    <td style="text-align: right; font-family: SFMono-Regular, Consolas, Menlo, monospace;">f</td>
  </tr>
</tbody>
</table></p>
<p>So our mask <code>1111000001011010</code> is more conveniently written as <code>f05a</code> of <code>F05A</code>:</p>
<p><table style="border-collapse: collapse; border: none;">
  <tbody style="border: none;">
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Core</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; line-height: 0.85; padding:0px 0px 0px 0px; border: none;">
             1111 1100 0000 0000
        </br>5432 1098 7654 3210</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Mask</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      1111 0000 0101 1010</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Hexadecimal</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      &nbsp;&nbsp;f&nbsp; &nbsp;&nbsp;0&nbsp; &nbsp;&nbsp;5&nbsp; &nbsp;&nbsp;a </td>
    </tr>
  </tbody>
</table></p>
<p>To indicate that a number is a hexadecimal number, there are several conventions, but the
one which is usually used, is preceding the number with <code>0x</code>, so our mask then becomes 
<code>0xf05a</code>.  This convention is used by bash and can also be used in Slurm masks.</p>
<p>Since CCDs on the zen3-based LUMI CPUs have 8 cores, they correspond to exactly 2 hexadecimal
digits. This also implies that if we want to use the same cores on each CCD, building the mask
becomes simple as we only need to look at CCD 0 and then shift the pattern two positions for each
subsequent CCD. </p>
<p>The primary use case for all this will be core mapping on the GPU nodes to get an optimal binding
between the cores and GCDs used by a Slurm task / MPI rank. On each CCD of a LUMI-G processor,
core 0 cannot be used by the user. So building a mask that includes the first hardware thread of
all available cores (cores 1-7) of a CCD is done as follows:</p>
<p><table style="border-collapse: collapse; border: none;">
  <tbody style="border: none;">
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Core</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">7654 3210</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Mask</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      1111 1110</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Hexadecimal</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      &nbsp;&nbsp;f&nbsp; &nbsp;&nbsp;e</td>
    </tr>
  </tbody>
</table></p>
<p>so the mask is <code>0xfe</code>. Suppose that we want to use all 7 available cores on CCD 1, i.e., cores 9 till 15,
we get</p>
<p><table style="border-collapse: collapse; border: none;">
  <tbody style="border: none;">
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Core</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; line-height: 0.85; padding:0px 0px 0px 0px; border: none;">
             1111 1100 0000 0000
        </br>5432 1098 7654 3210</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Mask</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      1111 1110 0000 0000</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Hexadecimal</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      &nbsp;&nbsp;f&nbsp; &nbsp;&nbsp;e&nbsp; &nbsp;&nbsp;0&nbsp; &nbsp;&nbsp;0 </td>
    </tr>
  </tbody>
</table></p>
<p>of <code>0xfe00</code>, so really just our pattern for CCD 0 shifted by two positions by adding two 
zeros at the end. </p>
<p>For the example above, with 
<code>--cpu-bind=mask_cpu:7e000000000000,7e00000000000000,7e0000,7e000000,7e,7e00,7e00000000,7e0000000000</code>,
the basic building block is <code>0x7e</code>, so</p>
<p><table style="border-collapse: collapse; border: none;">
  <tbody style="border: none;">
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Hexadecimal</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      &nbsp;&nbsp;7&nbsp; &nbsp;&nbsp;e</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Mask</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      0111 1110</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Core</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">7654 3210</td>
    </tr>
  </tbody>
</table></p>
<p>or cores 1 till 6. For the whole mask, we get: </p>
<p><table style="border-collapse: collapse; border: none;">
  <tbody style="border: none;">
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>CCD</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      &nbsp;7 &nbsp;6 &nbsp;5 &nbsp;4 &nbsp;3 &nbsp;2 &nbsp;1 &nbsp;0</td>
      <td style="padding:0px 0px 0px 10px; border: none;"><b>using</b></td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Element 1</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      &nbsp;&nbsp; 7e 00 00 00 00 00 00</td>
      <td style="padding:0px 0px 0px 10px; border: none;">CCD 6, cores 49-54</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Element 2</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      7e 00 00 00 00 00 00 00</td>
      <td style="padding:0px 0px 0px 10px; border: none;">CCD 7, cores 57-62</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Element 3</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; 7e 00 00</td>
      <td style="padding:0px 0px 0px 10px; border: none;">CCD 2, cores 17-22</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Element 4</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; 7e 00 00 00</td>
      <td style="padding:0px 0px 0px 10px; border: none;">CCD 3, cores 25-30</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Element 5</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; 7e</td>
      <td style="padding:0px 0px 0px 10px; border: none;">CCD 0, cores 1-6</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Element 6</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; 7e 00</td>
      <td style="padding:0px 0px 0px 10px; border: none;">CCD 1, cores 9-14</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Element 7</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; 7e 00 00 00 00</td>
      <td style="padding:0px 0px 0px 10px; border: none;">CCD 4, cores 33-38</td>
    </tr>
    <tr style="border: none;">
      <td style="padding:0px 10px 0px 0px; border: none;"><b>Element 1</b></td>
      <td style="font-family: SFMono-Regular, Consolas, Menlo, monospace; padding:0px 0px 0px 0px; border: none;">
      &nbsp;&nbsp; &nbsp;&nbsp; 7e 00 00 00 00 00</td>
      <td style="padding:0px 0px 0px 10px; border: none;">CCD 5, cores 41-46</td>
    </tr>
  </tbody>
</table></p>
<p>So basically this mask means that we are creating slots for 8 tasks that each use 6 cores on a single 
CCD (cores 1 till 6), in the order CCD 6, CCD 7, CCD 2, CCD 3, CCD 0, CCD 1, CCD 4 and CCD 5.</p>
</details>
<h2 id="task-to-gpu-binding-with-slurm">Task-to-GPU binding with Slurm<a class="headerlink" href="#task-to-gpu-binding-with-slurm" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Task-to-GPU binding with Slurm" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/SlurmTaskGPU.png" /></p>
</figure>
<p><strong>Doing the task-to-GPU binding fully via Slurm is currently not recommended on LUMI. 
The problem is that Slurm uses control groups at the task level rather than just <code>ROCR_VISIBLE_DEVICES</code>
with the latter being more or less the equivalent of affinity masks. When using control groups this way,
the other GPUs in a job step on a node become completely invisible to a task, and the
Peer2Peer IPC mechanism for communication cannot be used anymore.</strong></p>
<p>We present the options for completeness, and as it may still help users if the control group setup
is not a problem for the application.</p>
<p>Task-to-GPU binding is done with</p>
<div class="highlight"><pre><span></span><code>--gpu-bind=[verbose,]&lt;type&gt;
</code></pre></div>
<p><a href="https://slurm.schedmd.com/archive/slurm-23.02.7/srun.html#OPT_gpu-bind">(see the Slurm manual)</a>
which is somewhat similar to <code>--cpu-binding</code> (to the extent that that makes sense).</p>
<p>Some options for the <code>&lt;type&gt;</code> parameter that are worth considering:</p>
<ul>
<li>
<p><code>--gpu-bind=closest</code>: This currently does not work well on LUMI. The problem is being investigated
    so the situation may have changed by the time you read this.</p>
</li>
<li>
<p><code>--gpu-bind=none</code>: Turns off the GPU binding of Slurm. This can actually be useful on shared node
    jobs where doing a proper allocation of GPUs is difficult. You can then first use Slurm options such 
    as <code>--gpus-per-task</code> to get a working allocation of GPUs and CPUs, then un-bind and rebind using a 
    different mechanism that we will discuss later.</p>
</li>
<li>
<p><code>--gpu-bind=map_gpu:&lt;list&gt;</code> is the equivalent of <code>--cpu-bind=map_cpu:&lt;list&gt;</code>.
    This option only makes sense on a job-exclusive node and is for jobs that need a single 
    GPU per task. It defines the list of GPUs that should be used, with the task with local ID 0
    using the first one in the list, etc.
    The numbering and topology was already discussed in the "LUMI ARchitecture" chapter, section
    <a href="../01-Architecture/#building-lumi-what-a-lumi-g-node-really-looks-like">"Building LUMI: What a LUMI-G really looks like</a>.</p>
</li>
<li>
<p><code>--gpu-bind=mask_gpu:&lt;list&gt;</code> is the equivalent of <code>--cpu-bind=mask_cpu:&lt;list&gt;</code>. 
    Now the bits in the mask correspond to individual GPUs, with GPU 0 the least significant bit. 
    This option again only makes sense on a job-exclusive node.</p>
</li>
</ul>
<p>Though <code>map_gpu</code> and <code>mask_gpu</code> could be very useful to get a proper mapping taking the topology of the 
node into account, due to the current limitation of creating a control group per task it can not often
be used as it breaks some efficient communication mechanisms between tasks, including the GPU Peer2Peer 
IPC used by Cray MPICH for intro-node MPI transfers if GPU aware MPI support is enabled.</p>
<details class="advanced">
<summary>What do the HPE Cray manuals say about this? (Click to expand)</summary>
<p>From the HPE Cray CoE: 
<em>"Slurm may choose to use cgroups to implement the required
affinity settings. Typically, the use of cgroups has the downside of preventing the use of 
GPU Peer2Peer IPC mechanisms. By default Cray MPI uses IPC for
implementing intra-node, inter-process MPI data movement operations that involve GPU-attached user buffers. 
When Slurm’s cgroups settings are in effect, users are
advised to set <code>MPICH_SMP_SINGLE_COPY_MODE=NONE</code> or <code>MPICH_GPU_IPC_ENABLED=0</code>
to disable the use of IPC-based implementations. 
Disabling IPC also has a noticeable impact on intra-node MPI performance when 
GPU-attached memory regions are involved."</em></p>
<p>This is exactly what Slurm does on LUMI.</p>
</details>
<h2 id="mpi-rank-redistribution-with-cray-mpich">MPI rank redistribution with Cray MPICH<a class="headerlink" href="#mpi-rank-redistribution-with-cray-mpich" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide MPI rank redistribution with Cray MPICH" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/MPICHRankRedistribution.png" /></p>
</figure>
<p>By default MPI rank <em>i</em> will use Slurm task <em>i</em> in a parallel job step. 
With Cray MPICH this can be changed via the environment variable 
<code>MPICH_RANK_REORDER_METHOD</code>. It provides an even more powerful way
of reordering MPI ranks than the Slurm <code>--distribution</code> option as one
can define fully custom orderings.</p>
<p>Rank reordering is an advanced topic that is discussed in more detail in the
4-day LUMI comprehensive courses organised by the LUMI User Support Team.
The <a href="https://lumi-supercomputer.github.io/LUMI-training-materials/comprehensive-latest">material of the latest one can be found via the course archive web page</a>
and is discussed in the  "MPI Topics on the HPE Cray EX Supercomputer"
which is often given on day 3.</p>
<p>Rank reordering can be used to reduce the number of inter-node messages or to spread those
ranks that do parallel I/O over more nodes to increase the I/O bandwidth that can be
obtained in the application.</p>
<p>Possible values for <code>MPICH_RANK_REORDER_METHOD</code> are:</p>
<ul>
<li>
<p><code>export MPICH_RANK_REORDER_METHOD=0</code>: Round-robin placement of the MPI ranks.
    This is the equivalent of the cyclic ordering in Slurm.</p>
</li>
<li>
<p><code>export MPICH_RANK_REORDER_METHOD=1</code>: This is the default and it preserves the
    ordering of Slurm, and the only one that makes sense with other L1 Slurm distributions
    than <code>block</code>.</p>
<p>The Cray MPICH manual confusingly calls this "SMP-style ordering".</p>
</li>
<li>
<p><code>export MPICH_RANK_REORDER_METHOD=2</code>: Folded rank placement. This is somewhat similar 
    to round-robin, but when the last node is reached, the node list is transferred in the 
    opposite direction.</p>
</li>
<li>
<p><code>export MPICH_RANK_REORDER_METHOD=3</code>: Use a custom ordering, given by the 
    <code>MPICH_RANK_ORDER</code> file which gives a comma-separated list of the MPI ranks
    in the order they should be assigned to slots on the nodes. The default filename 
    <code>MPICH_RANK_ORDER</code> can be overwritten through the environment variable 
    <code>MPICH_RANK_REORDER_FILE</code>.</p>
</li>
</ul>
<p>Rank reordering does not always work well if Slurm is not using the (default) block ordering. 
As the <code>lumi-CPEtools</code> <code>mpi_check</code>, <code>hybrid_check</code> and <code>gpu_check</code> commands use Cray MPICH
they can be used to test the Cray MPICH rank reordering also. The MPI ranks that are 
displayed are the MPI ranks as seen through MPI calls and not the value of
<code>SLURM_PROCID</code> which is the Slurm task number.</p>
<p>The HPE Cray Programming Environment actually has profiling tools that help you determine
the optimal rank ordering for a particular run, which is useful if you do a lot of runs with
the same problem size (and hence same number of nodes and tasks).</p>
<details class="example">
<summary>Try the following job script (click to expand)</summary>
<p><!-- Script renumber-demo.slurm -->
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --account=project_46YXXXXXX</span>
<span class="c1">#SBATCH --job-name=renumber-demo</span>
<span class="c1">#SBATCH --output %x-%j.txt</span>
<span class="c1">#SBATCH --partition=standard</span>
<span class="c1">#SBATCH --nodes=2</span>
<span class="c1">#SBATCH --hint=nomultithread</span>
<span class="c1">#SBATCH --time=5:00</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/24.03<span class="w"> </span>partition/C<span class="w"> </span>lumi-CPEtools/1.1-cpeGNU-24.03

<span class="nb">set</span><span class="w"> </span>-x
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nSMP-style distribution on top of block.&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MPICH_RANK_REORDER_METHOD</span><span class="o">=</span><span class="m">1</span>
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>-c<span class="w"> </span><span class="m">32</span><span class="w"> </span>-m<span class="w"> </span>block<span class="w"> </span>mpi_check<span class="w"> </span>-r
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nSMP-style distribution on top of cyclic.&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MPICH_RANK_REORDER_METHOD</span><span class="o">=</span><span class="m">1</span>
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>-c<span class="w"> </span><span class="m">32</span><span class="w"> </span>-m<span class="w"> </span>cyclic<span class="w"> </span>mpi_check<span class="w"> </span>-r
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nRound-robin distribution on top of block.&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MPICH_RANK_REORDER_METHOD</span><span class="o">=</span><span class="m">0</span>
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>-c<span class="w"> </span><span class="m">32</span><span class="w"> </span>-m<span class="w"> </span>block<span class="w"> </span>mpi_check<span class="w"> </span>-r
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nFolded distribution on top of block.&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MPICH_RANK_REORDER_METHOD</span><span class="o">=</span><span class="m">2</span>
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>-c<span class="w"> </span><span class="m">32</span><span class="w"> </span>-m<span class="w"> </span>block<span class="w"> </span>mpi_check<span class="w"> </span>-r
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nCustom distribution on top of block.&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MPICH_RANK_REORDER_METHOD</span><span class="o">=</span><span class="m">3</span>
cat<span class="w"> </span>&gt;MPICH_RANK_ORDER<span class="w"> </span><span class="s">&lt;&lt;EOF</span>
<span class="s">0,1,4,5,2,3,6,7</span>
<span class="s">EOF</span>
cat<span class="w"> </span>MPICH_RANK_ORDER
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>-c<span class="w"> </span><span class="m">32</span><span class="w"> </span>-m<span class="w"> </span>block<span class="w"> </span>mpi_check<span class="w"> </span>-r
/bin/rm<span class="w"> </span>MPICH_RANK_ORDER
<span class="nb">set</span><span class="w"> </span>+x
</code></pre></div></td></tr></table></div></p>
<p>Ths script starts 8 tasks that each take a quarter node. </p>
<ol>
<li>
<p>The first <code>srun</code> command (on line 15) is just the block distribution. 
    The first 4 MPI ranks are
    on the first node, the next 4 on the second node.</p>
<div class="highlight"><pre><span></span><code>+ export MPICH_RANK_REORDER_METHOD=1
+ MPICH_RANK_REORDER_METHOD=1
+ srun -n 8 -c 32 -m block mpi_check -r

Running 8 single-threaded MPI ranks.

++ mpi_check: MPI rank   0/8   on cpu  17/256 of nid001804 mask 0-31
++ mpi_check: MPI rank   1/8   on cpu  32/256 of nid001804 mask 32-63
++ mpi_check: MPI rank   2/8   on cpu  65/256 of nid001804 mask 64-95
++ mpi_check: MPI rank   3/8   on cpu 111/256 of nid001804 mask 96-127
++ mpi_check: MPI rank   4/8   on cpu   0/256 of nid001805 mask 0-31
++ mpi_check: MPI rank   5/8   on cpu  32/256 of nid001805 mask 32-63
++ mpi_check: MPI rank   6/8   on cpu  64/256 of nid001805 mask 64-95
++ mpi_check: MPI rank   7/8   on cpu 120/256 of nid001805 mask 96-127
</code></pre></div>
</li>
<li>
<p>The second <code>srun</code> command, on line 18, is an example where the Slurm cyclic
    distribution is preserved. MPI rank 0 now lands on the first
    32 cores of node 0 of the allocation, MPI rank 1 on the first 32 cores of node 1 of the allocation,
    then task 2 on the second 32 cores of node 0, and so on:</p>
<div class="highlight"><pre><span></span><code>+ export MPICH_RANK_REORDER_METHOD=1
+ MPICH_RANK_REORDER_METHOD=1
+ srun -n 8 -c 32 -m cyclic mpi_check -r

Running 8 single-threaded MPI ranks.

++ mpi_check: MPI rank   0/8   on cpu   0/256 of nid001804 mask 0-31
++ mpi_check: MPI rank   1/8   on cpu   1/256 of nid001805 mask 0-31
++ mpi_check: MPI rank   2/8   on cpu  32/256 of nid001804 mask 32-63
++ mpi_check: MPI rank   3/8   on cpu  33/256 of nid001805 mask 32-63
++ mpi_check: MPI rank   4/8   on cpu  79/256 of nid001804 mask 64-95
++ mpi_check: MPI rank   5/8   on cpu  64/256 of nid001805 mask 64-95
++ mpi_check: MPI rank   6/8   on cpu 112/256 of nid001804 mask 96-127
++ mpi_check: MPI rank   7/8   on cpu 112/256 of nid001805 mask 96-127
</code></pre></div>
</li>
<li>
<p>The third <code>srun</code> command, on line 21, uses Cray MPICH rank reordering instead to get a round-robin ordering
    rather than using the Slurm <code>--distribution=cyclic</code> option. The result is the same
    as in the previous case:</p>
<div class="highlight"><pre><span></span><code>+ export MPICH_RANK_REORDER_METHOD=0
+ MPICH_RANK_REORDER_METHOD=0
+ srun -n 8 -c 32 -m block mpi_check -r

Running 8 single-threaded MPI ranks.

++ mpi_check: MPI rank   0/8   on cpu   0/256 of nid001804 mask 0-31
++ mpi_check: MPI rank   1/8   on cpu   1/256 of nid001805 mask 0-31
++ mpi_check: MPI rank   2/8   on cpu  32/256 of nid001804 mask 32-63
++ mpi_check: MPI rank   3/8   on cpu  47/256 of nid001805 mask 32-63
++ mpi_check: MPI rank   4/8   on cpu  64/256 of nid001804 mask 64-95
++ mpi_check: MPI rank   5/8   on cpu  64/256 of nid001805 mask 64-95
++ mpi_check: MPI rank   6/8   on cpu 112/256 of nid001804 mask 96-127
++ mpi_check: MPI rank   7/8   on cpu 112/256 of nid001805 mask 96-127
</code></pre></div>
</li>
<li>
<p>The fourth <code>srun</code> command, on line 24, demonstrates the folded ordering: Rank 0 runs on the first 32 
    cores of node 0 of the allocation, rank 1 on the first 32 of node 1, then rank 2 runs on 
    the second set of 32 cores again on node 1, with rank 3 then running on the second 32 cores
    of node 0, rank 4 on the third group of 32 cores of node 0, rank 5 on the third group of
    32 cores on rank 1, and so on. So the nodes are filled in the order 0, 1, 1, 0, 0, 1, 1, 0.</p>
<div class="highlight"><pre><span></span><code>+ export MPICH_RANK_REORDER_METHOD=2
+ MPICH_RANK_REORDER_METHOD=2
+ srun -n 8 -c 32 -m block mpi_check -r

Running 8 single-threaded MPI ranks.

++ mpi_check: MPI rank   0/8   on cpu   0/256 of nid001804 mask 0-31
++ mpi_check: MPI rank   1/8   on cpu  17/256 of nid001805 mask 0-31
++ mpi_check: MPI rank   2/8   on cpu  32/256 of nid001805 mask 32-63
++ mpi_check: MPI rank   3/8   on cpu  32/256 of nid001804 mask 32-63
++ mpi_check: MPI rank   4/8   on cpu  64/256 of nid001804 mask 64-95
++ mpi_check: MPI rank   5/8   on cpu  64/256 of nid001805 mask 64-95
++ mpi_check: MPI rank   6/8   on cpu 112/256 of nid001805 mask 96-127
++ mpi_check: MPI rank   7/8   on cpu 112/256 of nid001804 mask 96-127
</code></pre></div>
</li>
<li>
<p>The fifth example ('srun' on line 31) demonstrate a custom reordering. 
    Here we face a 4x2-grid which we want
    to split in 2 2x2 groups. So where the ranks in our grid are numbered as</p>
<div class="highlight"><pre><span></span><code>0 1 2 3
4 5 6 7
</code></pre></div>
<p>we really want the left half of the grid on the first node of the allocation
and the right half on the second node as this gives us less inter-node
communication than when we would put the first line on the first node and
the second line on the second. So basically we want ranks 0, 1, 4 and 5 on 
the first node and ranks 2, 3, 6 and 7 on the second node, which is done
by creating the reorder file with content</p>
<div class="highlight"><pre><span></span><code>0,1,4,5,2,3,6,7
</code></pre></div>
<p>The resulting output is</p>
<div class="highlight"><pre><span></span><code>+ export MPICH_RANK_REORDER_METHOD=3
+ MPICH_RANK_REORDER_METHOD=3
+ cat
+ srun -n 8 -c 32 -m block mpi_check -r

Running 8 single-threaded MPI ranks.

++ mpi_check: MPI rank   0/8   on cpu   0/256 of nid001804 mask 0-31
++ mpi_check: MPI rank   1/8   on cpu  32/256 of nid001804 mask 32-63
++ mpi_check: MPI rank   2/8   on cpu   1/256 of nid001805 mask 0-31
++ mpi_check: MPI rank   3/8   on cpu  32/256 of nid001805 mask 32-63
++ mpi_check: MPI rank   4/8   on cpu  64/256 of nid001804 mask 64-95
++ mpi_check: MPI rank   5/8   on cpu 112/256 of nid001804 mask 96-127
++ mpi_check: MPI rank   6/8   on cpu  64/256 of nid001805 mask 64-95
++ mpi_check: MPI rank   7/8   on cpu 112/256 of nid001805 mask 96-127
</code></pre></div>
</li>
</ol>
</details>
<h2 id="refining-core-binding-in-openmp-applications">Refining core binding in OpenMP applications<a class="headerlink" href="#refining-core-binding-in-openmp-applications" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Refining core binding in OpenMP" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/OpenMPBinding.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide Refining core binding in OpenMP: OMP_PLACES" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/OpenMPBindingPlaces.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide Refining core binding in OpenMP: OMP_PROC_BIND" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/OpenMPBindingProcBind.png" /></p>
</figure>
<p>In a Slurm batch job step, threads of a shared memory process will be contained to all 
hardware threads of all available cores on the first node of your allocation. To contain
a shared memory program to the hardware threads asked for in the allocation (i.e., to ensure
that <code>--hint=[no]multithread</code> has effect) you'd have to start the shared memory program with
<code>srun</code> in a regular job step.</p>
<p>Any multithreaded executable run as a shared memory job or ranks in a hybrid MPI/multithread job,
will - when started properly via <code>srun</code> - get access to a group of cores via an affinity mask.
In some cases you will want to manually refine the way individual threads of each process are
mapped onto the available hardware threads.</p>
<p>In OpenMP, this is usually done through environment variables (it can also be done partially in
the program through library calls). A number of environment variables is standardised in the 
OpenMP standard, but some implementations offer some additional non-standard ones, or
non-standard values for the standard environment variables. 
Below we discuss the more important of the standard ones:</p>
<ul>
<li>
<p><code>OMP_NUM_THREADS</code> is used to set the number of CPU threads OpenMP will use. In its most basic
    form this is a single number (but you can give multiple comma-separated numbers for nested
    parallelism). </p>
<p>OpenMP programs on LUMI will usually correctly detect how many hardware threads are available to
the task and use one OpenMP thread per hardware thread. There are cases where you may want to ask
for a certain number of hardware threads when allocating resources, e.g., to easily get a good mapping
of tasks on cores, but do not want to use them all, e.g., because your application is too memory bandwidth
or cache constrained and using fewer threads actually gives better overall performance on a per-node basis.</p>
</li>
<li>
<p><code>OMP_PLACES</code> is used to restrict each OpenMP thread to a group of hardware threads. Possible values
    include: </p>
<ul>
<li><code>OMP_PLACES=threads</code> to restrict OpenMP threads to a single hardware thread</li>
<li><code>OMP_PLACES=cores</code> to restrict each OpenMP threads to a single core (but all hardware threads associated with that core)</li>
<li><code>OMP_PLACES=sockets</code> to restrict each OpenMP thread to the hardware threads of a single socket</li>
<li>
<p>And it is possible to give a list with explicit values, e.g.,</p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_PLACES</span><span class="o">=</span><span class="s2">&quot;{0:4}:3:8&quot;</span>
</code></pre></div>
<p>which is also equivalent to</p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_PLACES</span><span class="o">=</span><span class="s2">&quot;{0,1,2,3},{8,9,10,11},{16,17,18,19}&quot;</span>
</code></pre></div>
<p>so each OpenMP thread is restricted to a different group of 4 hardware threads. The numbers in the list are not
the physical Linux hardware thread numbers, but are relative to the hardware threads available in the 
affinity mask of the task. </p>
<p>More general, <code>{a:b}:c:d</code> means b numbers starting from a (so a, a+1, ..., a+b-1), repeated c times,
at every repeat shifted by d. There are more variants to generate lists of places and we show another
one in the example below. But in all the syntax may look strange and there are manuals that give
the wrong information (including some versions of the manual for the GNU OpenMP runtime).</p>
<p>Note that this is different from the core numbers that would be used in <code>--cpu-bind=map_cpu</code>
or <code>--gpu-bind=mask_cpu</code> which sets the CPUs or groups of CPUs available to each thread and which always use
the physical numbering and not a numbering that is local to the job allocation.</p>
</li>
</ul>
</li>
<li>
<p><code>OMP_PROC_BIND</code>: Sets how threads are distributed over the places. Possible values are:</p>
<ul>
<li>
<p><code>OMP_PROC_BIND=false</code>: Turn off OpenMP thread binding. Each thread will get access to all hardware threads
    available in to the task (and defined by a Linux affinity mask in Slurm).</p>
</li>
<li>
<p><code>OMP_PROC_BIND=close</code>: If more places are available than there are OpenMP threads, then try
    to put the OpenMP threads in different places as close as possible to the master thread. 
    In general, bind as close as possible
    to the master thread while still distributing for load balancing.</p>
</li>
<li>
<p><code>OMP_PROC_BIND=spread</code>: Spread threads out as evenly as possible over the places available
    to the task.</p>
</li>
<li>
<p><code>OMP_PROC_BIND=master</code>: Bind threads to the same place as the master thread. The place is determined by the
    <code>OMP_PLACES</code> environment variable and it is clear this makes no sense if that place is just a single hardware
    thread or single core as all threads would then be competing for the resources of a single core.</p>
</li>
</ul>
<p>Multiple values of <code>close</code>, <code>spread</code> and <code>master</code> in a comma-separated list are possible
to organise nested OpenMP parallelism, but this is outside of the scope of this tutorial. </p>
<p>The Cray Compilation Environment also has an additional non-standard option <code>auto</code> which is actually the default and tries to
do a reasonable job for most cases. On the other compilers on LUMI, the default behaviour is <code>false</code> unless the
next environment variable, <code>OMP_PLACES</code>, is specified.</p>
</li>
<li>
<p><code>OMP_DISPLAY_AFFINITY</code>: When set tot <code>TRUE</code> information about the affinity binding of each thread will be 
     shown which is good for debugging purposes.</p>
</li>
</ul>
<p>For single-level OpenMP parallelism, the <code>omp_check</code> and <code>hybrid_check</code> programs from the <code>lumi-CPEtools</code> modules
can also be used to check the OpenMP thread binding.</p>
<details class="example">
<summary>Some examples (click to expand)</summary>
<p>Consider the following job script:</p>
<p><!-- TODO: Improve, always specify all three variables -->
<!-- Script omp-demo.slurm -->
<div class="highlight"><pre><span></span><code>#!/bin/bash
#SBATCH --account=project_46YXXXXXX
#SBATCH --job-name=omp-demo
#SBATCH --output %x-%j.txt
#SBATCH --partition=standard
#SBATCH --nodes=1
#SBATCH --hint=multithread
#SBATCH --time=5:00

module load LUMI/24.03 partition/C lumi-CPEtools/1.1-cpeCray-24.03

set -x
export OMP_NUM_THREADS=4
export OMP_PROC_BIND=false
srun -n 1 -c 32 --hint=multithread   omp_check -r
srun -n 1 -c 16 --hint=nomultithread omp_check -r

export OMP_NUM_THREADS=4
unset OMP_PROC_BIND
srun -n 1 -c 32 --hint=multithread   omp_check -r
srun -n 1 -c 16 --hint=nomultithread omp_check -r

export OMP_NUM_THREADS=4
export OMP_PROC_BIND=close
srun -n 1 -c 32 --hint=multithread   omp_check -r
srun -n 1 -c 16 --hint=nomultithread omp_check -r

export OMP_NUM_THREADS=4
export OMP_PROC_BIND=spread
srun -n 1 -c 32 --hint=multithread   omp_check -r
srun -n 1 -c 16 --hint=nomultithread omp_check -r

export OMP_NUM_THREADS=4
export OMP_PROC_BIND=close
export OMP_PLACES=threads
srun -n 1 -c 32 --hint=multithread   omp_check -r
export OMP_PLACES=cores
srun -n 1 -c 32 --hint=multithread   omp_check -r

export OMP_NUM_THREADS=4
export OMP_PROC_BIND=close
export OMP_PLACES=&quot;{0:8}:4:8&quot;
srun -n 1 -c 32 --hint=multithread   omp_check -r

export OMP_PLACES=&quot;{0:4,16:4}:4:4&quot;
srun -n 1 -c 32 --hint=multithread   omp_check -r
set +x
</code></pre></div></p>
<p>Let's check the output step by step:</p>
<p>In the first block we run 2 <code>srun</code> commands that actually both use 16 cores, but first with
hardware threading enabled in Slurm and then with multithread mode off in Slurm:</p>
<div class="highlight"><pre><span></span><code>+ export OMP_NUM_THREADS=4
+ OMP_NUM_THREADS=4
+ export OMP_PROC_BIND=false
+ OMP_PROC_BIND=false
+ srun -n 1 -c 32 --hint=multithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-15, 128-143
++ omp_check: OpenMP thread   1/4   on cpu 137/256 of nid001077 mask 0-15, 128-143
++ omp_check: OpenMP thread   2/4   on cpu 129/256 of nid001077 mask 0-15, 128-143
++ omp_check: OpenMP thread   3/4   on cpu 143/256 of nid001077 mask 0-15, 128-143

+ srun -n 1 -c 16 --hint=nomultithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-15
++ omp_check: OpenMP thread   1/4   on cpu  15/256 of nid001077 mask 0-15
++ omp_check: OpenMP thread   2/4   on cpu   1/256 of nid001077 mask 0-15
++ omp_check: OpenMP thread   3/4   on cpu  14/256 of nid001077 mask 0-15
</code></pre></div>
<p><code>OMP_PROC_BIND</code> was explicitly set to false to disable the Cray Compilation Environment default behaviour.
The masks reported by <code>omp_check</code> cover all hardware threads available to the task in Slurm: Both hardware
threads for the 16 first cores in the multithread case and just the primary hardware thread on the first 16 cores
in the second case. So each OpenMP thread can in principle migrate over all available hardware threads.</p>
<p>In the second block we unset the <code>PROC_BIND</code> environment variable to demonstrate the behaviour of the
Cray Compilation Environment. The output would be different had we used the cpeGNU or cpeAOCC version.</p>
<div class="highlight"><pre><span></span><code>+ export OMP_NUM_THREADS=4
+ OMP_NUM_THREADS=4
+ unset OMP_PROC_BIND
+ srun -n 1 -c 32 --hint=multithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   1/256 of nid001077 mask 0-3, 128-131
++ omp_check: OpenMP thread   1/4   on cpu   4/256 of nid001077 mask 4-7, 132-135
++ omp_check: OpenMP thread   2/4   on cpu   8/256 of nid001077 mask 8-11, 136-139
++ omp_check: OpenMP thread   3/4   on cpu 142/256 of nid001077 mask 12-15, 140-143

+ srun -n 1 -c 16 --hint=nomultithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-3
++ omp_check: OpenMP thread   1/4   on cpu   4/256 of nid001077 mask 4-7
++ omp_check: OpenMP thread   2/4   on cpu   9/256 of nid001077 mask 8-11
++ omp_check: OpenMP thread   3/4   on cpu  15/256 of nid001077 mask 12-15
</code></pre></div>
<p>The default behaviour of the CCE is very nice: Threads are nicely spread out over the available cores and
then all get access to their own group of hardware threads that in this case with 4 threads for 16 cores
spans 4 cores for each thread. In fact, also in other cases the default behaviour of CCE will be a binding 
that works well for many cases. </p>
<p>In the next experiment we demonstrate the <code>close</code> binding:</p>
<div class="highlight"><pre><span></span><code>+ export OMP_NUM_THREADS=4
+ OMP_NUM_THREADS=4
+ export OMP_PROC_BIND=close
+ OMP_PROC_BIND=close
+ srun -n 1 -c 32 --hint=multithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0
++ omp_check: OpenMP thread   1/4   on cpu 128/256 of nid001077 mask 128
++ omp_check: OpenMP thread   2/4   on cpu   1/256 of nid001077 mask 1
++ omp_check: OpenMP thread   3/4   on cpu 129/256 of nid001077 mask 129

+ srun -n 1 -c 16 --hint=nomultithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0
++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001077 mask 1
++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001077 mask 2
++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001077 mask 3
</code></pre></div>
<p>In the first case, with Slurm multithreading mode on, we see that the 4 threads are now
concentrated on only 2 cores but each gets pinned to its own hardware thread. In general 
this behaviour is not what one wants if more cores are available as on each core two threads
will now be competing for available resources. In the second case, with Slurm multithreading 
disabled, the threads are bound to the first 4 cores, with one core for each thread.</p>
<p>Next we demonstrate the <code>spread</code> binding:</p>
<div class="highlight"><pre><span></span><code>+ export OMP_NUM_THREADS=4
+ OMP_NUM_THREADS=4
+ export OMP_PROC_BIND=spread
+ OMP_PROC_BIND=spread
+ srun -n 1 -c 32 --hint=multithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0
++ omp_check: OpenMP thread   1/4   on cpu   4/256 of nid001077 mask 4
++ omp_check: OpenMP thread   2/4   on cpu   8/256 of nid001077 mask 8
++ omp_check: OpenMP thread   3/4   on cpu  12/256 of nid001077 mask 12

+ srun -n 1 -c 16 --hint=nomultithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0
++ omp_check: OpenMP thread   1/4   on cpu   4/256 of nid001077 mask 4
++ omp_check: OpenMP thread   2/4   on cpu   8/256 of nid001077 mask 8
++ omp_check: OpenMP thread   3/4   on cpu  12/256 of nid001077 mask 12
</code></pre></div>
<p>The result is now the same in both cases as we have fewer threads than physical cores.
Each OpenMP thread is bound to a single core, but these cores are spread out over the
first 16 cores of the node. </p>
<p>Next we return to the <code>close</code> binding but try both <code>threads</code> and <code>cores</code> as places
with Slurm multithreading turned on for both cases:</p>
<div class="highlight"><pre><span></span><code>+ export OMP_NUM_THREADS=4
+ OMP_NUM_THREADS=4
+ export OMP_PROC_BIND=close
+ OMP_PROC_BIND=close
+ export OMP_PLACES=threads
+ OMP_PLACES=threads
+ srun -n 1 -c 32 --hint=multithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0
++ omp_check: OpenMP thread   1/4   on cpu 128/256 of nid001077 mask 128
++ omp_check: OpenMP thread   2/4   on cpu   1/256 of nid001077 mask 1
++ omp_check: OpenMP thread   3/4   on cpu 129/256 of nid001077 mask 129

+ export OMP_PLACES=cores
+ OMP_PLACES=cores
+ srun -n 1 -c 32 --hint=multithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0, 128
++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001077 mask 1, 129
++ omp_check: OpenMP thread   2/4   on cpu 130/256 of nid001077 mask 2, 130
++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001077 mask 3, 131
</code></pre></div>
<p>With <code>threads</code> as places we get again the distribution with two OpenMP threads on each
physical core, each with their own hardware thread. With cores as places, we get only one
thread per physical core, but each thread has access to both hardware threads of that physical core.</p>
<p>And lastly we play a bit with custom placements:</p>
<div class="highlight"><pre><span></span><code>+ export OMP_NUM_THREADS=4
+ OMP_NUM_THREADS=4
+ export OMP_PROC_BIND=close
+ OMP_PROC_BIND=close
+ export &#39;OMP_PLACES={0:8}:4:8&#39;
+ OMP_PLACES=&#39;{0:8}:4:8&#39;
+ srun -n 1 -c 32 --hint=multithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-7
++ omp_check: OpenMP thread   1/4   on cpu   8/256 of nid001077 mask 8-15
++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001077 mask 128-135
++ omp_check: OpenMP thread   3/4   on cpu 136/256 of nid001077 mask 136-143
</code></pre></div>
<p><code>OMP_PLACES='{0:8}:4:8</code> means: Take starting from core with logical number 0 8 cores and 
repeat this 4 times, shifting by 8 each time, so effectively</p>
<div class="highlight"><pre><span></span><code>OMP_PLACES=&quot;{ 0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15},{16,17,18,19,20,21,22,23},{24,25,26,27,27,28,30,31}&quot;
</code></pre></div>
<p><code>omp_check</code> however shows the OS numbering for the hardware threads so we can see what this places variable means:
the first thread can get scheduled on the first hardware thread of the first 8 cores, the second thread on the first
hardware thread of the next 8 cores, the third OpenMP thread on the second thread of the first 8 cores, and the 
fourth OpenMP thread on the second hardware thread of the next 8 cores. In other words, the logical numbering of the 
threads follows the same ordering as at the OS level: First the first hardware thread of each core, then the second 
hardware thread.</p>
<p>When trying another variant with</p>
<div class="highlight"><pre><span></span><code>OMP_PACES={0:4,16:4}:4:4
</code></pre></div>
<p>which is equivalent to</p>
<div class="highlight"><pre><span></span><code>OMP_PLACES={0,1,2,3,16,17,18,19},{4,5,6,7,20,21,22,23},{8,9,10,11,24,25,26,27},{12,13,14,15,28,29,30,31}&quot;
</code></pre></div>
<p>we get a much nicer distribution:</p>
<div class="highlight"><pre><span></span><code>+ export &#39;OMP_PLACES={0:4,16:4}:4:4&#39;
+ OMP_PLACES=&#39;{0:4,16:4}:4:4&#39;
+ srun -n 1 -c 32 --hint=multithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-3, 128-131
++ omp_check: OpenMP thread   1/4   on cpu 132/256 of nid001077 mask 4-7, 132-135
++ omp_check: OpenMP thread   2/4   on cpu 136/256 of nid001077 mask 8-11, 136-139
++ omp_check: OpenMP thread   3/4   on cpu 140/256 of nid001077 mask 12-15, 140-143
</code></pre></div>
</details>
<p>We only discussed a subset of the environment variables defined in the OpenMP standard. Several implementations
also offer additional environment variables, e.g., 
<a href="https://gcc.gnu.org/onlinedocs/libgomp/Environment-Variables.html">a number of <code>GOMP_*</code> environment variables in the GNU Compiler Collection implementation</a> or <code>KMP_*</code> variables in the Intel compiler (not available on LUMI).</p>
<p>Some further documentation:</p>
<ul>
<li>
<p>The <code>OMP_*</code> environment variables and a number of environment variables specific for the runtime libraries
    of the Cray Compiling Environment are discussed in the 
    <a href="https://cpe.ext.hpe.com/docs/cce/man7/intro_openmp.7.html#environment-variables"><code>intro_openmp</code> manual page, section "Environment variables"</a>.</p>
</li>
<li>
<p><a href="https://www.openmp.org/spec-html/5.1/openmpch6.html#x323-4980006">A list of OMP_ environment variables in the OpenMP 5.1 standard</a> 
    (as the current list in the HTML version of the 5.2 standard has some problems).</p>
</li>
</ul>
<h2 id="gpu-binding-with-rocr_visible_devices">GPU binding with ROCR_VISIBLE_DEVICES<a class="headerlink" href="#gpu-binding-with-rocr_visible_devices" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding with ROCR_VISIBLE_DEVICES" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRGPU.png" /></p>
</figure>
<p>The <code>ROCR_VISIBLE_DEVICES</code> environment variable restricts access to GPUs at the ROCm platform runtime 
level. Contrary to control groups however this mechanism is compatible with the Peer2Peer IPC used by
GPU-aware Cray MPI for intra-node communication.</p>
<p>The value of the <code>ROCR_VISIBLE_DEVICES</code> environment variable is a list of device indices that will be
exposed to the applications. The device indices do depend on the control group. Visible devices in a control
group are always numbered from 0.</p>
<p>So though <code>ROCR_VISIBLE_DEVICES</code> has the same function as affinity masks for CPUs, it is different in
many respects.</p>
<ol>
<li>
<p>Affinity masks are part of the Linux kernel and fully OS-controlled, while 
    <code>ROCR_VISIBLE_DEVICES</code> is interpreted in the ROCm&trade; stack.</p>
</li>
<li>
<p>Affinity masks are set through an OS call and that call can enforce that the new
    mask cannot be less restrictive than the parent mask. <code>ROCR_VISIBLE_DEVICES</code> is just
    an environment variable, so at the time that you try to set it to a value that you 
    shouldn't use, there is no check.</p>
</li>
<li>
<p>Affinity masks always use the global numbering of hardware threads while 
    <code>ROCR_VISIBLE_DEVICES</code> uses the local numbering in the currently active control group.
    So the GPU that corresponds to 0 in <code>ROCR_VISIBLE_DEVICES</code> is not always the same GPU.</p>
</li>
</ol>
<details class="advanced">
<summary>Alternative values for <code>ROCR_VISIBLE_DEVICES</code></summary>
<p>Instead of device indices, <code>ROCR_VISIBLE_DEVICES</code> also accepts GPU UUIDs that are unique to each
GPU. This is less practical then it seems as the UUIDs of GPUs are different on each node so one
would need to discover them first before they can be used.</p>
</details>
<h2 id="combining-slurm-task-binding-with-rocr_visible_devices">Combining Slurm task binding with ROCR_VISIBLE_DEVICES<a class="headerlink" href="#combining-slurm-task-binding-with-rocr_visible_devices" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Optimal mapping (1)" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRGPUMap_1.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Optimal mapping (2)" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRGPUMap_2.png" /></p>
</figure>
<p>In the chapter on <a href="../01-Architecture/">the architecture of LUMI</a> we discussed 
<a href="../01-Architecture/#building-lumi-what-a-lumi-g-node-really-looks-like">what a LUMI-G really looks like</a>.</p>
<p>The full topology of a LUMI-G compute node is shown in the figure:</p>
<figure>
  <img 
    src="https://462000265.lumidata.eu/2day-20241210/img/lumig-node-architecture-rings.svg" 
    width="842"
    alt="LUMI-G compute nodes overview"
  >
</figure>

<p>Note that the numbering of GCDs does not correspond to the numbering of CCDs/cores. However, for optimal
memory transfers (and certainly if cache-coherent memory access from CPU to GPU would be used) it is 
better to ensure that each GCD collaborates with the matched CCD in an MPI rank. So we have the mapping:</p>
<table>
<thead>
<tr>
<th style="text-align: right;">CCD</th>
<th style="text-align: left;">HWTs</th>
<th style="text-align: left;">Available HWTs</th>
<th style="text-align: right;">GCD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">0</td>
<td style="text-align: left;">0-7, 64-71</td>
<td style="text-align: left;">1-7, 65-71</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: right;">1</td>
<td style="text-align: left;">8-15, 72-79</td>
<td style="text-align: left;">9-15, 73-79</td>
<td style="text-align: right;">5</td>
</tr>
<tr>
<td style="text-align: right;">2</td>
<td style="text-align: left;">16-23, 80-87</td>
<td style="text-align: left;">17-23, 81-87</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: right;">3</td>
<td style="text-align: left;">24-32, 88-95</td>
<td style="text-align: left;">25-32, 89-95</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: right;">4</td>
<td style="text-align: left;">32-39, 96-103</td>
<td style="text-align: left;">33-39, 97-103</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: right;">5</td>
<td style="text-align: left;">40-47, 104-111</td>
<td style="text-align: left;">41-47, 105-111</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: right;">6</td>
<td style="text-align: left;">48-55, 112-119</td>
<td style="text-align: left;">49-55, 113-119</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: right;">7</td>
<td style="text-align: left;">56-63, 120-127</td>
<td style="text-align: left;">57-63, 121-127</td>
<td style="text-align: right;">1</td>
</tr>
</tbody>
</table>
<p>or the reverse mapping</p>
<table>
<thead>
<tr>
<th style="text-align: right;">GCD</th>
<th style="text-align: right;">CCD</th>
<th style="text-align: left;">HWTs</th>
<th style="text-align: left;">Available HWTs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">0</td>
<td style="text-align: right;">6</td>
<td style="text-align: left;">48-55, 112-119</td>
<td style="text-align: left;">49-55, 113-119</td>
</tr>
<tr>
<td style="text-align: right;">1</td>
<td style="text-align: right;">7</td>
<td style="text-align: left;">56-63, 120-127</td>
<td style="text-align: left;">57-63, 121-127</td>
</tr>
<tr>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">16-23, 80-87</td>
<td style="text-align: left;">17-23, 81-87</td>
</tr>
<tr>
<td style="text-align: right;">3</td>
<td style="text-align: right;">3</td>
<td style="text-align: left;">24-32, 88-95</td>
<td style="text-align: left;">25-32, 89-95</td>
</tr>
<tr>
<td style="text-align: right;">4</td>
<td style="text-align: right;">0</td>
<td style="text-align: left;">0-7, 64-71</td>
<td style="text-align: left;">1-7, 65-71</td>
</tr>
<tr>
<td style="text-align: right;">5</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">8-15, 72-79</td>
<td style="text-align: left;">9-15, 73-79</td>
</tr>
<tr>
<td style="text-align: right;">6</td>
<td style="text-align: right;">4</td>
<td style="text-align: left;">32-39, 96-103</td>
<td style="text-align: left;">33-39, 97-103</td>
</tr>
<tr>
<td style="text-align: right;">7</td>
<td style="text-align: right;">5</td>
<td style="text-align: left;">40-47, 104-111</td>
<td style="text-align: left;">41-47, 105-111</td>
</tr>
</tbody>
</table>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Embedded rings" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRGPURing.png" /></p>
</figure>
<p>Moreover, if you look more carefully at the topology, you can see that the connections between the 
GCDs contain a number of rings:</p>
<ol>
<li>
<p>Green ring: 0 - 1 - 3 - 2 - 4 - 5 - 7 - 6 - 0</p>
</li>
<li>
<p>Red ring:   0 - 1 - 5 - 4 - 6 - 7 - 3 - 2 - 0</p>
</li>
<li>
<p>Sharing some connections with the previous ones, but can be combined with the green ring: 
    0 - 1 - 5 - 4 - 2 - 3 - 7 - 6 - 0</p>
</li>
</ol>
<p>So if your application would use a ring mapping for communication and use communication from GPU buffers 
for that, than it may be advantageous to map the MPI ranks on one of those rings which would mean that neither
the order of the CCDs nor the order of the GCDs is trivial.</p>
<p>Some other topologies can also be mapped on these connections (but unfortunately not a 3D cube).</p>
<p>Note: The red ring and green ring correspond to the red and green rings on page 6 of the
<a href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna2-white-paper.pdf">"Introducing AMD CDNA&trade; 2 Architecture" whitepaper</a>.</p>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Implementation" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRMechanism.png" /></p>
</figure>
<p>To implement a proper CCD-to-GCD mapping we will use two mechanisms:</p>
<ul>
<li>
<p>On the CPU side we'll use Slurm <code>--cpu-bind</code>. Sometimes we can also simply use <code>-c</code> or 
    <code>--cpus-per-task</code> (in particular in the case below with linear ordering of the CCDs and 
    7 cores per task)</p>
</li>
<li>
<p>On the GPU side we will manually assign GPUs via a different value of <code>ROCR_VISIBLE_DEVICES</code> for each
    thread. To accomplish this we will have to write a wrapper script which we will generate in the job script.</p>
</li>
</ul>
<p>Let us start with the simplest case:</p>
<h3 id="linear-assignment-of-gcd-then-match-the-cores">Linear assignment of GCD, then match the cores<a class="headerlink" href="#linear-assignment-of-gcd-then-match-the-cores" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Implementation: Linear GCD, match CPU, no OpenMP" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRMechanismLinearGCD1.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Implementation: Linear GCD, match CPU, OpenMP" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRMechanismLinearGCD2.png" /></p>
</figure>
<p>One possible job script to accomplish this is:</p>
<!-- map-linear-GCD.slurm -->
<div class="highlight"><pre><span></span><code>#!/bin/bash
#SBATCH --account=project_46YXXXXXX
#SBATCH --job-name=map-linear-GCD
#SBATCH --output %x-%j.txt
#SBATCH --partition=standard-g
#SBATCH --gpus-per-node=8
#SBATCH --nodes=1
#SBATCH --time=5:00

module load LUMI/24.03 partition/G lumi-CPEtools/1.1-cpeCray-24.03

cat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID
#!/bin/bash
export ROCR_VISIBLE_DEVICES=\$SLURM_LOCALID
exec \$*
EOF
chmod +x select_gpu_$SLURM_JOB_ID

CPU_BIND1=&quot;map_cpu:49,57,17,25,1,9,33,41&quot;

CPU_BIND2=&quot;mask_cpu:0xfe000000000000,0xfe00000000000000&quot;
CPU_BIND2=&quot;$CPU_BIND2,0xfe0000,0xfe000000&quot;
CPU_BIND2=&quot;$CPU_BIND2,0xfe,0xfe00&quot;
CPU_BIND2=&quot;$CPU_BIND2,0xfe00000000,0xfe0000000000&quot;

export MPICH_GPU_SUPPORT_ENABLED=0

echo -e &quot;\nPure MPI:\n&quot;
srun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID mpi_check -r
srun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID gpu_check -l

echo -e &quot;\nHybrid:\n&quot;
srun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID hybrid_check -r
srun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID gpu_check -l

/bin/rm -f select_gpu_$SLURM_JOB_ID
</code></pre></div>
<p>To select the GPUs we either use a map with numbers of cores (ideal for pure MPI programs)
or masks (the only option for hybrid programs). The mask that we give in the example uses
7 cores per CCD and always skips the first core, as is required on LUMI as
the first core of each chiplet
is reserved and not available to Slurm jobs. To select the right GPU for <code>ROCR_VISIBLE_DEVICES</code> 
we can use the Slurm local task ID which is 
also what the MPI rank will be. 
We use a so-called <a href="https://tldp.org/LDP/abs/html/here-docs.html">"bash here document"</a> 
to generate the script. Note that in the bash here document
we needed to protect the <code>$</code> with a backslash (so use <code>\$</code>) as otherwise the variables would
already be expanded when generating the script file.</p>
<p>Instead of the somewhat complicated <code>--ntasks</code> with <code>srun</code> we could have specified <code>--ntasks-per-node=8</code>
on a <code>#SBATCH</code> line which would have fixed the structure for all <code>srun</code> commands. Even though we want
to use all GPUs in the node, <code>--gpus-per-node</code> or an equivalent option has to be specified either
as an <code>#SBATCH</code> line or with each <code>srun</code> command or no GPUs will be made available to the tasks 
started by the <code>srun</code> command.</p>
<p>Note the output of the second <code>srun</code> command:</p>
<div class="highlight"><pre><span></span><code>MPI 000 - OMP 000 - HWT 049 (CCD6) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)
MPI 001 - OMP 000 - HWT 057 (CCD7) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)
MPI 002 - OMP 000 - HWT 017 (CCD2) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)
MPI 003 - OMP 000 - HWT 025 (CCD3) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)
MPI 004 - OMP 000 - HWT 001 (CCD0) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1(GCD4/CCD0)
MPI 005 - OMP 000 - HWT 009 (CCD1) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6(GCD5/CCD1)
MPI 006 - OMP 000 - HWT 033 (CCD4) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9(GCD6/CCD4)
MPI 007 - OMP 000 - HWT 041 (CCD5) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID dc(GCD7/CCD5)
</code></pre></div>
<p>With the <code>-l</code> option we also print some information about the CCD that a core belongs to and the 
GCD and corresponding optimal CCD for each PCIe bus ID, which makes it very easy to check if the
mapping is as intended. Note that the GCDs are indeed in the linear order starting with GCD0.</p>
<h3 id="linear-assignment-of-the-ccds-then-match-the-gcd">Linear assignment of the CCDs, then match the GCD<a class="headerlink" href="#linear-assignment-of-the-ccds-then-match-the-gcd" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Implementation: Linear CCD, match GCD, no OpenMP" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRMechanismLinearCCD1.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Implementation: Linear CCD, match GCD, OpenMP" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRMechanismLinearCCD2.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Implementation: Linear CCD, match GCD, with cpus-per-task" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRMechanismLinearCCD3.png" /></p>
</figure>
<p>To modify the order of the GPUs, we now use an array with the desired order in the <code>select_gpu</code> script.
With the current setup of LUMI, with one core reserved on each chiplet, there are now two options
to get the proper CPUs:</p>
<ol>
<li>
<p>We can use masks to define the cores for each slot, but they will now look more regular, or</p>
</li>
<li>
<p>we can simply use <code>--cpus-per-task=7</code> and then further restrict the number of threads per task
    with <code>OMP_NUM_THREADS</code>.</p>
</li>
</ol>
<p>The job script (for option 1) now becomes:</p>
<!-- map-linear-CCD.slurm -->
<div class="highlight"><pre><span></span><code>#!/bin/bash
#SBATCH --account=project_46YXXXXXX
#SBATCH --job-name=map-linear-CCD
#SBATCH --output %x-%j.txt
#SBATCH --partition=standard-g
#SBATCH --gpus-per-node=8
#SBATCH --nodes=1
#SBATCH --time=5:00

module load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12

cat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID
#!/bin/bash
GPU_ORDER=(4 5 2 3 6 7 0 1)
export ROCR_VISIBLE_DEVICES=\${GPU_ORDER[\$SLURM_LOCALID]}
exec \$*
EOF
chmod +x select_gpu_$SLURM_JOB_ID

CPU_BIND1=&quot;map_cpu:1,9,17,25,33,41,49,57&quot;

CPU_BIND2=&quot;mask_cpu&quot;
CPU_BIND2=&quot;$CPU_BIND2:0x00000000000000fe,0x000000000000fe00&quot;
CPU_BIND2=&quot;$CPU_BIND2,0x0000000000fe0000,0x00000000fe000000&quot;
CPU_BIND2=&quot;$CPU_BIND2,0x000000fe00000000,0x0000fe0000000000&quot;
CPU_BIND2=&quot;$CPU_BIND2,0x00fe000000000000,0xfe00000000000000&quot;

export MPICH_GPU_SUPPORT_ENABLED=0

echo -e &quot;\nPure MPI:\n&quot;
srun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID mpi_check -r
srun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID gpu_check -l

echo -e &quot;\nHybrid:\n&quot;
srun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID hybrid_check -r
srun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID gpu_check -l

/bin/rm -f select_gpu_$SLURM_JOB_ID
</code></pre></div>
<p>The leading zeros in the masks in the <code>CPU_BIND2</code> environment variable are not needed but we added
them as it makes it easier to see which chiplet is used in what position.</p>
<h3 id="the-green-ring">The green ring<a class="headerlink" href="#the-green-ring" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Implementation: Green ring, OpenMP, slide 1" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRMechanismGreenRing1.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Implementation: Green ring, OpenMP, slide 2" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRMechanismGreenRing2.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Implementation: Green ring, OpenMP, slide 3" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRMechanismGreenRing3.png" /></p>
</figure>
<p>As a final example for whole node allocations, lets bind tasks such that the MPI ranks are
mapped upon the green ring which is GCD 0 - 1 - 3 - 2 - 4 - 5 - 7 - 6 - 0. In other words,
we want to create the mapping</p>
<table>
<thead>
<tr>
<th style="text-align: right;">Task</th>
<th style="text-align: right;">GCD</th>
<th style="text-align: right;">CCD</th>
<th style="text-align: left;">Available cores</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">6</td>
<td style="text-align: left;">49-55, 113-119</td>
</tr>
<tr>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">7</td>
<td style="text-align: left;">57-63, 121-127</td>
</tr>
<tr>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">3</td>
<td style="text-align: left;">25-32, 89-95</td>
</tr>
<tr>
<td style="text-align: right;">3</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">17-23, 81-87</td>
</tr>
<tr>
<td style="text-align: right;">4</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">0</td>
<td style="text-align: left;">1-7, 65-71</td>
</tr>
<tr>
<td style="text-align: right;">5</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">9-15, 73-79</td>
</tr>
<tr>
<td style="text-align: right;">6</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">5</td>
<td style="text-align: left;">41-47, 105-111</td>
</tr>
<tr>
<td style="text-align: right;">7</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">4</td>
<td style="text-align: left;">33-39, 97-103</td>
</tr>
</tbody>
</table>
<p>This mapping would be useful when using GPU-to-GPU communication in a scenario where task <em>i</em>
only communicates with tasks <em>i-1</em> and <em>i+1</em> (module 8), so the communication pattern is a ring.</p>
<p>Now we need to reorder both the cores and the GCDs, so we basically combine the approach taken
in the two scripts above:</p>
<!-- Script map-ring-green.slurm -->
<div class="highlight"><pre><span></span><code>#!/bin/bash
#SBATCH --account=project_46YXXXXXX
#SBATCH --job-name=map-ring-green
#SBATCH --output %x-%j.txt
#SBATCH --partition=standard-g
#SBATCH --gpus-per-node=8
#SBATCH --nodes=1
#SBATCH --time=5:00

module load LUMI/24.03 partition/G lumi-CPEtools/1.1-cpeCray-24.03

# Mapping:
# | Task | GCD | CCD | Available cores |
# |-----:|----:|----:|:----------------|
# |    0 |   0 |   6 | 49-55, 113-119  |
# |    1 |   1 |   7 | 57-63, 121-127  |
# |    2 |   3 |   3 | 25-32, 89-95    |
# |    3 |   2 |   2 | 17-23, 81-87    |
# |    4 |   4 |   0 | 1-7, 65-71      |
# |    5 |   5 |   1 | 9-15, 73-79     |
# |    6 |   7 |   5 | 41-47, 105-111  |
# |    7 |   6 |   4 | 33-39, 97-103   |

cat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID
#!/bin/bash
GPU_ORDER=(0 1 3 2 4 5 7 6)
export ROCR_VISIBLE_DEVICES=\${GPU_ORDER[\$SLURM_LOCALID]}
exec \$*
EOF
chmod +x select_gpu_$SLURM_JOB_ID

CPU_BIND1=&quot;map_cpu:49,57,25,17,1,9,41,33&quot;

CCD_MASK=( 0x00000000000000fe \
           0x000000000000fe00 \
           0x0000000000fe0000 \
           0x00000000fe000000 \
           0x000000fe00000000 \
           0x0000fe0000000000 \
           0x00fe000000000000 \
           0xfe00000000000000 )
CPU_BIND2=&quot;mask_cpu&quot;
CPU_BIND2=&quot;$CPU_BIND2:${CCD_MASK[6]},${CCD_MASK[7]}&quot;
CPU_BIND2=&quot;$CPU_BIND2,${CCD_MASK[3]},${CCD_MASK[2]}&quot;
CPU_BIND2=&quot;$CPU_BIND2,${CCD_MASK[0]},${CCD_MASK[1]}&quot;
CPU_BIND2=&quot;$CPU_BIND2,${CCD_MASK[5]},${CCD_MASK[4]}&quot;

export MPICH_GPU_SUPPORT_ENABLED=0

echo -e &quot;\nPure MPI:\n&quot;
srun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID mpi_check -r
srun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID gpu_check -l

echo -e &quot;\nHybrid:\n&quot;
srun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID hybrid_check -r
srun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID gpu_check -l

/bin/rm -f select_gpu_$SLURM_JOB_ID
</code></pre></div>
<p>The values for <code>GPU_ORDER</code> are easily read from the second column of the table with the mapping
that we prepared. The cores to use for the pure MPI run are also easily read from the table:
simply take the first core of each line. Finally, to build the mask,
we used some bash trickery. We first define the bash array <code>CCD_MASK</code> with the mask for each chiplet.
As this has a regular structure, this is easy to build. Then we compose the mask list for the CPUs
by indexing in that array, where the indices are easily read from the third column in the mapping.</p>
<p>The alternative code to build <code>CPU_BIND2</code> is</p>
<div class="highlight"><pre><span></span><code>CPU_BIND2=&quot;mask_cpu&quot;
CPU_BIND2=&quot;$CPU_BIND2:0x00fe000000000000,0xfe00000000000000&quot;
CPU_BIND2=&quot;$CPU_BIND2,0x00000000fe000000,0x0000000000fe0000&quot;
CPU_BIND2=&quot;$CPU_BIND2,0x00000000000000fe,0x000000000000fe00&quot;
CPU_BIND2=&quot;$CPU_BIND2,0x0000fe0000000000,0x000000fe00000000&quot;
</code></pre></div>
<p>which may be shorter, but requires some puzzling to build and hence is more prone to error.</p>
<p>The output of the second <code>srun</code> command is now</p>
<div class="highlight"><pre><span></span><code>MPI 000 - OMP 000 - HWT 049 (CCD6) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)
MPI 001 - OMP 000 - HWT 057 (CCD7) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)
MPI 002 - OMP 000 - HWT 025 (CCD3) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)
MPI 003 - OMP 000 - HWT 017 (CCD2) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)
MPI 004 - OMP 000 - HWT 001 (CCD0) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1(GCD4/CCD0)
MPI 005 - OMP 000 - HWT 009 (CCD1) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6(GCD5/CCD1)
MPI 006 - OMP 000 - HWT 041 (CCD5) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID dc(GCD7/CCD5)
MPI 007 - OMP 000 - HWT 033 (CCD4) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9(GCD6/CCD4)
</code></pre></div>
<p>Checking the last column, we see that the GCDs are indeed in the desired order for the green ring, and 
is is also easy to check that each task is also mapped on the optimal CCD for the GCD.</p>
<details class="example">
<summary>Job script with some more advanced bash</summary>
<p><!-- map-advanced-multiple.slurm -->
<div class="highlight"><pre><span></span><code>#!/bin/bash
#SBATCH --job-name=map-advanced-multiple
#SBATCH --output %x-%j.txt
#SBATCH --partition=standard-g
#SBATCH --gpus-per-node=8
#SBATCH --nodes=1
#SBATCH --time=5:00

module load LUMI/24.03 partition/G lumi-CPEtools/1.1-cpeCray-24.03

#
# Define the order of the GPUs and the core mask for CCD0
# It is important that the order of the GPUs is a string with the numbers separated by spaces.
#
GCD_ORDER=&quot;0 1 5 4 6 7 3 2&quot;
coremask=&#39;2#00000010&#39;  # Can use the binary representation, hexadecimal with 0x, or decimal

#
# run_gpu script, takes the string with GCDs as the first argument.
#
cat &lt;&lt; EOF &gt; run_gpu_$SLURM_JOB_ID
#!/bin/bash
GCD_ORDER=( \$1 )
shift
export ROCR_VISIBLE_DEVICES=\${GCD_ORDER[\$SLURM_LOCALID]}
exec &quot;\$@&quot;
EOF
chmod +x run_gpu_$SLURM_JOB_ID

#
# Build the CPU binding
# Argument one is mask, all other arguments are treated as an array of GCD numbers.
#

function generate_mask {

    # First argument is the mask for CCD0
    mask=$1

    # Other arguments are either a string already with the GCDs, or just one GCD per argument.
    shift
    GCDs=( &quot;$@&quot; )
    # Fully expand (doesn&#39;t matter as the loop can deal with it, but good if we want to check the number)
    GCDs=( ${GCDs[@]} )

    # For each GCD, the corresponding CCD number in the optimal mapping.
    MAP_to_CCD=( 6 7 2 3 0 1 4 5 )

    CPU_BIND=&quot;&quot;

    # Loop over the GCDs in the order of the list to compute the corresponding
    # CPU mask.
    for GCD in ${GCDs[@]}
    do
        # Get the matching CCD for this GCD
        CCD=${MAP_to_CCD[$GCD]}

        # Shift the mask for CCD0 to the position for CCD $CCD
        printf -v tmpvar &quot;0x%016x&quot; $((mask &lt;&lt; $((CCD*8))))

        # Add to CPU_BIND. We&#39;ll remove the leading , this creates later.
        CPU_BIND=&quot;$CPU_BIND,$tmpvar&quot;
    done

    # Strip the leading ,
    CPU_BIND=&quot;${CPU_BIND#,}&quot;

    # Return the result by printing to stdout
    printf &quot;$CPU_BIND&quot;

}

#
# Running the check programs
#

export MPICH_GPU_SUPPORT_ENABLED=1

# Some mappings:
linear_CCD=&quot;4 5 2 3 6 7 0 1&quot;
linear_GCD=&quot;0 1 2 3 4 5 6 7&quot; 
ring_green=&quot;0 1 3 2 4 5 7 6&quot;
ring_red=&quot;0 1 5 4 6 7 3 2&quot;

echo -e &quot;\nTest runs:\n&quot;

echo -e &quot;\nConsecutive CCDs:\n&quot;
srun --ntasks=$((SLURM_NNODES*8)) \
     --cpu-bind=mask_cpu:$(generate_mask $coremask $linear_CCD) \
     ./run_gpu_$SLURM_JOB_ID &quot;$linear_CCD&quot; gpu_check -l

echo -e &quot;\nConsecutive GCDs:\n&quot;
srun --ntasks=$((SLURM_NNODES*8)) \
     --cpu-bind=mask_cpu:$(generate_mask $coremask $linear_GCD) \
     ./run_gpu_$SLURM_JOB_ID &quot;$linear_GCD&quot; gpu_check -l

echo -e &quot;\nGreen ring:\n&quot;
srun --ntasks=$((SLURM_NNODES*8)) \
     --cpu-bind=mask_cpu:$(generate_mask $coremask $ring_green) \
     ./run_gpu_$SLURM_JOB_ID &quot;$ring_green&quot; gpu_check -l

echo -e &quot;\nRed ring:\n&quot;
srun --ntasks=$((SLURM_NNODES*8)) \
     --cpu-bind=mask_cpu:$(generate_mask $coremask $ring_red) \
     ./run_gpu_$SLURM_JOB_ID &quot;$ring_red&quot; gpu_check -l

echo -e &quot;\nFirst two CPU NUMA domains (assuming one node in the allocation):&quot;
half=&quot;4 5 2 3&quot;
srun --ntasks=4 \
     --cpu-bind=mask_cpu:$(generate_mask $coremask $half) \
     ./run_gpu_$SLURM_JOB_ID &quot;$half&quot; gpu_check -l

/bin/rm -f run_gpu_$SLURM_JOB_ID
</code></pre></div></p>
<p>In this script, we have modified the and renamed the usual <code>select_gpu</code> script (renamed to <code>run_cpu</code>) to take
as the first argument a string with a space-separated list of the GCDs to use. This has
been combined with the bash function <code>generate_mask</code> (which could have been transformed in a script as well)
that computes the CPU mask starting from the mask for CCD0 and shifting that mask as needed.
The input is the mask to use and then the GCDs to use, either as a single string or as a
series of arguments (e.g., resulting from an array expansion).</p>
<p>Both commands are then combined in the <code>srun</code> command. The <code>generate_mask</code> function is used to generate
the mask for <code>--gpu-bind</code> while the <code>run_gpu</code> script is used to set <code>ROCR_VISIBLE_DEVICES</code> for each task.
The examples also show how easy it is to experiment with different mappings. The one limitation of the 
script and function is that there can be only 1 GPU per task and one task per GPU, and the CPU mask is also
limited to a single CCD (which makes sense with the GPU restriction). Generating masks that also include the
second hardware thread is not supported yet. (We use bash arithmetic internally which is limited to 64-bit integers).</p>
</details>
<h3 id="what-about-allocate-by-resources-partitions">What about "allocate by resources" partitions?<a class="headerlink" href="#what-about-allocate-by-resources-partitions" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide GPU binding: Allocatable-by-resources partitions" loading="lazy" src="https://462000265.lumidata.eu/2day-20241210/img/LUMI-2day-20241210-08-Binding/ROCRMechanismAllocateByResource.png" /></p>
</figure>
<!-- Experiments in smallg-binding-exp*.slurm -->
<p>On partitions that are "allocatable by resource", e.g., <code>small-g</code>, you are never guaranteed that tasks 
will be spread in a reasonable way over the CCDs and that the matching GPUs will be available to your job.
Creating an optimal mapping or taking the topology into account is hence impossible. </p>
<p>What is possible though is work around the fact that with the usual options for such resource allocations,
Slurm will lock up the GPUs for individual tasks in control groups so that the Peer2Peer IPC intra-node
communication mechanism has to be turned off. We can do this for job steps that follow the pattern of 
resources allocated via the <code>sbatch</code> arguments (usually <code>#SBATCH</code> lines), and rely on three elements for that:</p>
<ol>
<li>
<p>We can turn off the Slurm GPU binding mechanism with <code>--gpu-bind=none</code>.</p>
</li>
<li>
<p>Even then, the GPUs will still be locked up in a control group on each node for the job and hence on each node
    be numbered starting from zero.</p>
</li>
<li>
<p>And each task also has a local ID that can be used to map the appropriate number of GPUs to each task.</p>
</li>
</ol>
<div class="admonition demo">
<p class="admonition-title">This can be demonstrated with the following job script:</p>
<p><!-- map-smallg-1gpt.slurm -->
<div class="highlight"><pre><span></span><code>#! /bin/bash
#SBATCH --account=project_46YXXXXXX
#SBATCH --job-name=map-smallg-1gpt
#SBATCH --output %x-%j.txt
#SBATCH --partition=small-g
#SBATCH --ntasks=12
#SBATCH --cpus-per-task=2
#SBATCH --gpus-per-task=1
#SBATCH --hint=nomultithread
#SBATCH --time=5:00

module load LUMI/24.03 partition/G lumi-CPEtools/1.1-cpeCray-24.03

cat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID
#!/bin/bash
export ROCR_VISIBLE_DEVICES=\$SLURM_LOCALID
exec \$*
EOF
chmod +x ./select_gpu_$SLURM_JOB_ID

cat &lt;&lt; EOF &gt; echo_dev_$SLURM_JOB_ID
#!/bin/bash
printf -v task &quot;%02d&quot; \$SLURM_PROCID
echo &quot;Task \$task or node.local_id \$SLURM_NODEID.\$SLURM_LOCALID sees ROCR_VISIBLE_DEVICES=\$ROCR_VISIBLE_DEVICES&quot;
EOF
chmod +x ./echo_dev_$SLURM_JOB_ID

set -x
srun gpu_check -l
srun ./echo_dev_$SLURM_JOB_ID | sort
srun --gpu-bind=none ./echo_dev_$SLURM_JOB_ID | sort
srun --gpu-bind=none ./select_gpu_$SLURM_JOB_ID ./echo_dev_$SLURM_JOB_ID | sort
srun --gpu-bind=none ./select_gpu_$SLURM_JOB_ID gpu_check -l
set +x

/bin/rm -f select_gpu_$SLURM_JOB_ID echo_dev_$SLURM_JOB_ID
</code></pre></div></p>
<p>To run this job successfully, we need 12 GPUs so obviously the tasks will be spread over more than 
one node. The <code>echo_dev</code> command in this script only shows us the value of <code>ROCR_VISIBLE_DEVICES</code> 
for the task at that point, something that <code>gpu_check</code> in fact also reports as <code>GPU_ID</code>, but this
is just in case you don't believe...</p>
<p>The output of the first <code>srun</code> command is:</p>
<div class="highlight"><pre><span></span><code>+ srun gpu_check -l
MPI 000 - OMP 000 - HWT 001 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)
MPI 000 - OMP 001 - HWT 002 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)
MPI 001 - OMP 000 - HWT 003 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6(GCD1/CCD7)
MPI 001 - OMP 001 - HWT 004 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6(GCD1/CCD7)
MPI 002 - OMP 000 - HWT 005 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)
MPI 002 - OMP 001 - HWT 006 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)
MPI 003 - OMP 000 - HWT 007 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID cc(GCD3/CCD3)
MPI 003 - OMP 001 - HWT 008 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID cc(GCD3/CCD3)
MPI 004 - OMP 000 - HWT 009 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1(GCD4/CCD0)
MPI 004 - OMP 001 - HWT 010 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1(GCD4/CCD0)
MPI 005 - OMP 000 - HWT 011 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6(GCD5/CCD1)
MPI 005 - OMP 001 - HWT 012 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6(GCD5/CCD1)
MPI 006 - OMP 000 - HWT 013 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9(GCD6/CCD4)
MPI 006 - OMP 001 - HWT 014 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9(GCD6/CCD4)
MPI 007 - OMP 000 - HWT 015 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc(GCD7/CCD5)
MPI 007 - OMP 001 - HWT 016 (CCD2) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc(GCD7/CCD5)
MPI 008 - OMP 000 - HWT 001 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)
MPI 008 - OMP 001 - HWT 002 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)
MPI 009 - OMP 000 - HWT 003 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6(GCD1/CCD7)
MPI 009 - OMP 001 - HWT 004 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6(GCD1/CCD7)
MPI 010 - OMP 000 - HWT 005 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)
MPI 010 - OMP 001 - HWT 006 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)
MPI 011 - OMP 000 - HWT 007 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID cc(GCD3/CCD3)
MPI 011 - OMP 001 - HWT 008 (CCD1) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID cc(GCD3/CCD3)
</code></pre></div>
<p>In other words, we see that we did get cores on two nodes that obviously are not well aligned with
the GCDs, and 8 GPUS on the first and 4 on the second node.</p>
<p>The output of the second <code>srun</code> is:</p>
<div class="highlight"><pre><span></span><code>+ srun ./echo_dev_4359428
+ sort
Task 00 or node.local_id 0.0 sees ROCR_VISIBLE_DEVICES=0
Task 01 or node.local_id 0.1 sees ROCR_VISIBLE_DEVICES=0
Task 02 or node.local_id 0.2 sees ROCR_VISIBLE_DEVICES=0
Task 03 or node.local_id 0.3 sees ROCR_VISIBLE_DEVICES=0
Task 04 or node.local_id 0.4 sees ROCR_VISIBLE_DEVICES=0
Task 05 or node.local_id 0.5 sees ROCR_VISIBLE_DEVICES=0
Task 06 or node.local_id 0.6 sees ROCR_VISIBLE_DEVICES=0
Task 07 or node.local_id 0.7 sees ROCR_VISIBLE_DEVICES=0
Task 08 or node.local_id 1.0 sees ROCR_VISIBLE_DEVICES=0
Task 09 or node.local_id 1.1 sees ROCR_VISIBLE_DEVICES=0
Task 10 or node.local_id 1.2 sees ROCR_VISIBLE_DEVICES=0
Task 11 or node.local_id 1.3 sees ROCR_VISIBLE_DEVICES=0
</code></pre></div>
<p>It is normal that each task sees <code>ROCR_VISIBLE_DEVICES=0</code> even though we have seen that they all use a
different GPU. This is because each task is locked up in a control group with only one GPU, which then 
gets number 0.</p>
<p>The output of the third <code>srun</code> command is:</p>
<div class="highlight"><pre><span></span><code>+ sort
Task 00 or node.local_id 0.0 sees ROCR_VISIBLE_DEVICES=
Task 01 or node.local_id 0.1 sees ROCR_VISIBLE_DEVICES=
Task 02 or node.local_id 0.2 sees ROCR_VISIBLE_DEVICES=
Task 03 or node.local_id 0.3 sees ROCR_VISIBLE_DEVICES=
Task 04 or node.local_id 0.4 sees ROCR_VISIBLE_DEVICES=
Task 05 or node.local_id 0.5 sees ROCR_VISIBLE_DEVICES=
Task 06 or node.local_id 0.6 sees ROCR_VISIBLE_DEVICES=
Task 07 or node.local_id 0.7 sees ROCR_VISIBLE_DEVICES=
Task 08 or node.local_id 1.0 sees ROCR_VISIBLE_DEVICES=
Task 09 or node.local_id 1.1 sees ROCR_VISIBLE_DEVICES=
Task 10 or node.local_id 1.2 sees ROCR_VISIBLE_DEVICES=
Task 11 or node.local_id 1.3 sees ROCR_VISIBLE_DEVICES=
</code></pre></div>
<p>Slurm in fact did not set <code>ROCR_VISIBLE_DEVICES</code> because we turned binding off.</p>
<p>In the next <code>srun</code> command we set <code>ROCR_VISIBLE_DEVICES</code> based on the local task ID and get:</p>
<div class="highlight"><pre><span></span><code>+ srun --gpu-bind=none ./select_gpu_4359428 ./echo_dev_4359428
+ sort
Task 00 or node.local_id 0.0 sees ROCR_VISIBLE_DEVICES=0
Task 01 or node.local_id 0.1 sees ROCR_VISIBLE_DEVICES=1
Task 02 or node.local_id 0.2 sees ROCR_VISIBLE_DEVICES=2
Task 03 or node.local_id 0.3 sees ROCR_VISIBLE_DEVICES=3
Task 04 or node.local_id 0.4 sees ROCR_VISIBLE_DEVICES=4
Task 05 or node.local_id 0.5 sees ROCR_VISIBLE_DEVICES=5
Task 06 or node.local_id 0.6 sees ROCR_VISIBLE_DEVICES=6
Task 07 or node.local_id 0.7 sees ROCR_VISIBLE_DEVICES=7
Task 08 or node.local_id 1.0 sees ROCR_VISIBLE_DEVICES=0
Task 09 or node.local_id 1.1 sees ROCR_VISIBLE_DEVICES=1
Task 10 or node.local_id 1.2 sees ROCR_VISIBLE_DEVICES=2
Task 11 or node.local_id 1.3 sees ROCR_VISIBLE_DEVICES=3
</code></pre></div>
<p>Finally, we run <code>gpu_check</code> again and see the same assignment of physical GPUs again as when we
started, but now with different logical device numbers passed by <code>ROCR_VISIBLE_DEVICES</code>. The device
number for the hip runtime is always 0 though which is normal as <code>ROCR_VISIBLE_DEVICES</code> restricts the
access of the hip runtime to one GPU.</p>
<div class="highlight"><pre><span></span><code>+ srun --gpu-bind=none ./select_gpu_4359428 gpu_check -l
MPI 000 - OMP 000 - HWT 001 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)
MPI 000 - OMP 001 - HWT 002 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)
MPI 001 - OMP 000 - HWT 003 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)
MPI 001 - OMP 001 - HWT 004 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)
MPI 002 - OMP 000 - HWT 005 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)
MPI 002 - OMP 001 - HWT 006 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)
MPI 003 - OMP 000 - HWT 007 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)
MPI 003 - OMP 001 - HWT 008 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)
MPI 004 - OMP 000 - HWT 009 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1(GCD4/CCD0)
MPI 004 - OMP 001 - HWT 010 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1(GCD4/CCD0)
MPI 005 - OMP 000 - HWT 011 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6(GCD5/CCD1)
MPI 005 - OMP 001 - HWT 012 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6(GCD5/CCD1)
MPI 006 - OMP 000 - HWT 013 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9(GCD6/CCD4)
MPI 006 - OMP 001 - HWT 014 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9(GCD6/CCD4)
MPI 007 - OMP 000 - HWT 015 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID dc(GCD7/CCD5)
MPI 007 - OMP 001 - HWT 016 (CCD2) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID dc(GCD7/CCD5)
MPI 008 - OMP 000 - HWT 001 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)
MPI 008 - OMP 001 - HWT 002 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)
MPI 009 - OMP 000 - HWT 003 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)
MPI 009 - OMP 001 - HWT 004 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)
MPI 010 - OMP 000 - HWT 005 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)
MPI 010 - OMP 001 - HWT 006 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)
MPI 011 - OMP 000 - HWT 007 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)
MPI 011 - OMP 001 - HWT 008 (CCD1) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)
</code></pre></div>
</div>
<details class="example">
<summary>Example job script when using 2 GPUs per task.</summary>
<p><!-- map-smallg-2gpt.slurm -->
<div class="highlight"><pre><span></span><code>#! /bin/bash
#SBATCH --account=project_46YXXXXXX
#SBATCH --job-name=map-smallg-2gpt
#SBATCH --output %x-%j.txt
#SBATCH --partition=small-g
#SBATCH --ntasks=6
#SBATCH --cpus-per-task=2
#SBATCH --gpus-per-task=2
#SBATCH --hint=nomultithread
#SBATCH --time=5:00

module load LUMI/24.03 partition/G lumi-CPEtools/1.1-cpeCray-24.03

cat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID
#!/bin/bash
export ROCR_VISIBLE_DEVICES=\$((SLURM_LOCALID*2)),\$((SLURM_LOCALID*2+1))
exec \$*
EOF
chmod +x ./select_gpu_$SLURM_JOB_ID

cat &lt;&lt; EOF &gt; echo_dev_$SLURM_JOB_ID
#!/bin/bash
printf -v task &quot;%02d&quot; \$SLURM_PROCID
echo &quot;Task \$task or node.local_id \$SLURM_NODEID.\$SLURM_LOCALID sees ROCR_VISIBLE_DEVICES=\$ROCR_VISIBLE_DEVICES&quot;
EOF
chmod +x ./echo_dev_$SLURM_JOB_ID

set -x
srun gpu_check -l
srun ./echo_dev_$SLURM_JOB_ID | sort
srun --gpu-bind=none ./echo_dev_$SLURM_JOB_ID | sort
srun --gpu-bind=none ./select_gpu_$SLURM_JOB_ID ./echo_dev_$SLURM_JOB_ID | sort
srun --gpu-bind=none ./select_gpu_$SLURM_JOB_ID gpu_check -l
set +x

/bin/rm -f select_gpu_$SLURM_JOB_ID echo_dev_$SLURM_JOB_ID
</code></pre></div></p>
<p>The changes that were required are only minimal. We now assign 2 GPUs to <code>ROCR_VISIBLE_DEVICES</code> which 
is easily done with some bash arithmetic.</p>
</details>
<h2 id="further-material">Further material<a class="headerlink" href="#further-material" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Distribution and binding is discussed in more detail in our
    <a href="https://lumi-supercomputer.github.io/LUMI-training-materials/comprehensive-latest">4-day comprehensive LUMI courses</a>.
    Check for the lecture on "Advanced Placement" which is usually
    given on day 2 of the course.</p>
<p>Material of this presentation is available to all LUMI users on the system. Check the course
website for the names of the files.</p>
</li>
<li>
<p>Rank reordering in Cray MPICH is discussed is also discussed in more detail in our
    <a href="https://lumi-supercomputer.github.io/LUMI-training-materials/comprehensive-latest">4-day comprehensive LUMI courses</a>,
    but in the lecture on "MPI Topics on the HPE Cray EX Supercomputer" (often on day 3 of the course)
    that discusses more advanced MPI on LUMI, including loads of environment variables that can be used to
    improve the performance.</p>
</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        




<footer class="md-footer">

  

  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">

      
    <div class="md-footer-copyright">
      <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">
        <img alt="Creative Commons License" 
             style="border-width:0" 
             src="https://i.creativecommons.org/l/by/4.0/80x15.png"
        />
      </a>
      This work is licensed under a&nbsp;
      <a rel="license" 
         href="http://creativecommons.org/licenses/by/4.0/"
      >
        Creative Commons Attribution 4.0 International License
      </a>
    </div>

      
      <div class="md-social">
  
    
    
    
    
    <a href="https://www.youtube.com/channel/UCb31KOJ6Wqu0sRpIRi_k8Mw" target="_blank" rel="noopener" title="LUMI on YouTube" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://www.linkedin.com/company/lumi-supercomputer" target="_blank" rel="noopener" title="LUMI on LinkedIn" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://twitter.com/LUMIhpc" target="_blank" rel="noopener" title="LUMI on Twitter" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg>
    </a>
  
</div>
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.top", "navigation.indexes", "header.autohide", "toc.follow", "content.code.annotate", "content.code.copy", "content.footnote.tooltips"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
    
  </body>
</html>