


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://lumi-supercomputer.github.io/LUMI-training-materials/2day-20240502/06_Slurm/">
      
      
      
      
      <link rel="icon" href="../../assets/favicon-LUMI.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.10">
    
    
      
        <title>Slurm on LUMI - LUMI training materials</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e359304.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      
  
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
  
  <style>:root{--md-admonition-icon--demo:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M0 192h176V0h-16C71.6 0 0 71.6 0 160v32zm0 32v128c0 88.4 71.6 160 160 160h64c88.4 0 160-71.6 160-160V224H0zm384-32v-32C384 71.6 312.4 0 224 0h-16v192h176z"/></svg>');--md-admonition-icon--exercise:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M78.6 5c-9.5-7.4-23-6.5-31.6 2L7 47c-8.5 8.5-9.4 22-2.1 31.6l80 104c4.5 5.9 11.6 9.4 19 9.4H158l109 109c-14.7 29-10 65.4 14.3 89.6l112 112c12.5 12.5 32.8 12.5 45.3 0l64-64c12.5-12.5 12.5-32.8 0-45.3l-112-112c-24.2-24.2-60.6-29-89.6-14.3L192 158v-54c0-7.5-3.5-14.5-9.4-19L78.6 5zM19.9 396.1C7.2 408.8 0 426.1 0 444.1 0 481.6 30.4 512 67.9 512c18 0 35.3-7.2 48-19.9l117.8-117.8c-7.8-20.9-9-43.6-3.6-65.1l-61.7-61.7L19.9 396.1zM512 144c0-10.5-1.1-20.7-3.2-30.5-2.4-11.2-16.1-14.1-24.2-6l-63.9 63.9c-3 3-7.1 4.7-11.3 4.7H352c-8.8 0-16-7.2-16-16v-57.5c0-4.2 1.7-8.3 4.7-11.3l63.9-63.9c8.1-8.1 5.2-21.8-6-24.2C388.7 1.1 378.5 0 368 0c-79.5 0-144 64.5-144 144v.8l85.3 85.3c36-9.1 75.8.5 104 28.7l15.7 15.7c49-23 83-72.8 83-130.5zM56 432a24 24 0 1 1 48 0 24 24 0 1 1-48 0z"/></svg>');--md-admonition-icon--remark:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M256 448c141.4 0 256-93.1 256-208S397.4 32 256 32 0 125.1 0 240c0 45.1 17.7 86.8 47.7 120.9-1.9 24.5-11.4 46.3-21.4 62.9-5.5 9.2-11.1 16.6-15.2 21.6-2.1 2.5-3.7 4.4-4.9 5.7-.6.6-1 1.1-1.3 1.4l-.3.3c-4.6 4.6-5.9 11.4-3.4 17.4 2.5 6 8.3 9.9 14.8 9.9 28.7 0 57.6-8.9 81.6-19.3 22.9-10 42.4-21.9 54.3-30.6 31.8 11.5 67 17.9 104.1 17.9zM128 208a32 32 0 1 1 0 64 32 32 0 1 1 0-64zm128 0a32 32 0 1 1 0 64 32 32 0 1 1 0-64zm96 32a32 32 0 1 1 64 0 32 32 0 1 1-64 0z"/></svg>');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M320 0c17.7 0 32 14.3 32 32v64h120c39.8 0 72 32.2 72 72v272c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72h120V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16h-32zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16h-32zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16h-32zM264 256a40 40 0 1 0-80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224h16v192H48c-26.5 0-48-21.5-48-48v-96c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48h-16V224h16z"/></svg>');--md-admonition-icon--audience:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M72 88a56 56 0 1 1 112 0 56 56 0 1 1-112 0zm-8 157.7c-10 11.2-16 26.1-16 42.3s6 31.1 16 42.3v-84.6zm144.4-49.3C178.7 222.7 160 261.2 160 304c0 34.3 12 65.8 32 90.5V416c0 17.7-14.3 32-32 32H96c-17.7 0-32-14.3-32-32v-26.8C26.2 371.2 0 332.7 0 288c0-61.9 50.1-112 112-112h32c24 0 46.2 7.5 64.4 20.3zM448 416v-21.5c20-24.7 32-56.2 32-90.5 0-42.8-18.7-81.3-48.4-107.7C449.8 183.5 472 176 496 176h32c61.9 0 112 50.1 112 112 0 44.7-26.2 83.2-64 101.2V416c0 17.7-14.3 32-32 32h-64c-17.7 0-32-14.3-32-32zm8-328a56 56 0 1 1 112 0 56 56 0 1 1-112 0zm120 157.7v84.7c10-11.3 16-26.1 16-42.3s-6-31.1-16-42.3zM320 32a64 64 0 1 1 0 128 64 64 0 1 1 0-128zm-80 272c0 16.2 6 31 16 42.3v-84.6c-10 11.3-16 26.1-16 42.3zm144-42.3v84.7c10-11.3 16-26.1 16-42.3s-6-31.1-16-42.3zm64 42.3c0 44.7-26.2 83.2-64 101.2V448c0 17.7-14.3 32-32 32h-64c-17.7 0-32-14.3-32-32v-42.8c-37.8-18-64-56.5-64-101.2 0-61.9 50.1-112 112-112h32c61.9 0 112 50.1 112 112z"/></svg>');--md-admonition-icon--solution:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M234.7 42.7 197 56.8c-3 1.1-5 4-5 7.2s2 6.1 5 7.2l37.7 14.1 14.1 37.7c1.1 3 4 5 7.2 5s6.1-2 7.2-5l14.1-37.7L315 71.2c3-1.1 5-4 5-7.2s-2-6.1-5-7.2l-37.7-14.1L263.2 5c-1.1-3-4-5-7.2-5s-6.1 2-7.2 5l-14.1 37.7zM46.1 395.4c-18.7 18.7-18.7 49.1 0 67.9l34.6 34.6c18.7 18.7 49.1 18.7 67.9 0l381.3-381.4c18.7-18.7 18.7-49.1 0-67.9l-34.6-34.5c-18.7-18.7-49.1-18.7-67.9 0L46.1 395.4zM484.6 82.6l-105 105-23.3-23.3 105-105 23.3 23.3zM7.5 117.2C3 118.9 0 123.2 0 128s3 9.1 7.5 10.8L64 160l21.2 56.5c1.7 4.5 6 7.5 10.8 7.5s9.1-3 10.8-7.5L128 160l56.5-21.2c4.5-1.7 7.5-6 7.5-10.8s-3-9.1-7.5-10.8L128 96l-21.2-56.5c-1.7-4.5-6-7.5-10.8-7.5s-9.1 3-10.8 7.5L64 96 7.5 117.2zm352 256c-4.5 1.7-7.5 6-7.5 10.8s3 9.1 7.5 10.8L416 416l21.2 56.5c1.7 4.5 6 7.5 10.8 7.5s9.1-3 10.8-7.5L480 416l56.5-21.2c4.5-1.7 7.5-6 7.5-10.8s-3-9.1-7.5-10.8L480 352l-21.2-56.5c-1.7-4.5-6-7.5-10.8-7.5s-9.1 3-10.8 7.5L416 352l-56.5 21.2z"/></svg>');--md-admonition-icon--technical:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M160 96a96 96 0 1 1 192 0 96 96 0 1 1-192 0zm80 152v264l-48.4-24.2c-20.9-10.4-43.5-17-66.8-19.3l-96-9.6C12.5 457.2 0 443.5 0 427V224c0-17.7 14.3-32 32-32h30.3c63.6 0 125.6 19.6 177.7 56zm32 264V248c52.1-36.4 114.1-56 177.7-56H480c17.7 0 32 14.3 32 32v203c0 16.4-12.5 30.2-28.8 31.8l-96 9.6c-23.2 2.3-45.9 8.9-66.8 19.3L272 512z"/></svg>');--md-admonition-icon--intermediate:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M320 32c-8.1 0-16.1 1.4-23.7 4.1L15.8 137.4C6.3 140.9 0 149.9 0 160s6.3 19.1 15.8 22.6l57.9 20.9C57.3 229.3 48 259.8 48 291.9V320c0 28.4-10.8 57.7-22.3 80.8-6.5 13-13.9 25.8-22.5 37.6-3.2 4.3-4.1 9.9-2.3 15s6 8.9 11.2 10.2l64 16c4.2 1.1 8.7.3 12.4-2s6.3-6.1 7.1-10.4c8.6-42.8 4.3-81.2-2.1-108.7-3.2-14.2-7.5-28.7-13.5-42v-24.6c0-30.2 10.2-58.7 27.9-81.5 12.9-15.5 29.6-28 49.2-35.7l157-61.7c8.2-3.2 17.5.8 20.7 9s-.8 17.5-9 20.7l-157 61.7c-12.4 4.9-23.3 12.4-32.2 21.6l159.6 57.6c7.6 2.7 15.6 4.1 23.7 4.1s16.1-1.4 23.7-4.1l280.6-101c9.5-3.4 15.8-12.5 15.8-22.6s-6.3-19.1-15.8-22.6L343.7 36.1c-7.6-2.7-15.6-4.1-23.7-4.1zM128 408c0 35.3 86 72 192 72s192-36.7 192-72l-15.3-145.4L354.5 314c-11.1 4-22.8 6-34.5 6s-23.5-2-34.5-6l-142.2-51.4L128 408z"/></svg>');--md-admonition-icon--advanced:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M219.3.5c3.1-.6 6.3-.6 9.4 0l200 40C439.9 42.7 448 52.6 448 64s-8.1 21.3-19.3 23.5L352 102.9V160c0 70.7-57.3 128-128 128S96 230.7 96 160v-57.1l-48-9.6v65.1l15.7 78.4c.9 4.7-.3 9.6-3.3 13.3S52.8 256 48 256H16c-4.8 0-9.3-2.1-12.4-5.9s-4.3-8.6-3.3-13.3L16 158.4V86.6C6.5 83.3 0 74.3 0 64c0-11.4 8.1-21.3 19.3-23.5l200-40zM111.9 327.7c10.5-3.4 21.8.4 29.4 8.5l71 75.5c6.3 6.7 17 6.7 23.3 0l71-75.5c7.6-8.1 18.9-11.9 29.4-8.5 65 20.9 112 81.7 112 153.6 0 17-13.8 30.7-30.7 30.7H30.7C13.8 512 0 498.2 0 481.3c0-71.9 47-132.7 111.9-153.6z"/></svg>');--md-admonition-icon--lumi-be:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M48 0C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h96v-80c0-26.5 21.5-48 48-48s48 21.5 48 48v80h96c26.5 0 48-21.5 48-48V48c0-26.5-21.5-48-48-48H48zm16 240c0-8.8 7.2-16 16-16h32c8.8 0 16 7.2 16 16v32c0 8.8-7.2 16-16 16H80c-8.8 0-16-7.2-16-16v-32zm112-16h32c8.8 0 16 7.2 16 16v32c0 8.8-7.2 16-16 16h-32c-8.8 0-16-7.2-16-16v-32c0-8.8 7.2-16 16-16zm80 16c0-8.8 7.2-16 16-16h32c8.8 0 16 7.2 16 16v32c0 8.8-7.2 16-16 16h-32c-8.8 0-16-7.2-16-16v-32zM80 96h32c8.8 0 16 7.2 16 16v32c0 8.8-7.2 16-16 16H80c-8.8 0-16-7.2-16-16v-32c0-8.8 7.2-16 16-16zm80 16c0-8.8 7.2-16 16-16h32c8.8 0 16 7.2 16 16v32c0 8.8-7.2 16-16 16h-32c-8.8 0-16-7.2-16-16v-32zm112-16h32c8.8 0 16 7.2 16 16v32c0 8.8-7.2 16-16 16h-32c-8.8 0-16-7.2-16-16v-32c0-8.8 7.2-16 16-16zM448 0c-17.7 0-32 14.3-32 32v480h64V192h144c8.8 0 16-7.2 16-16V48c0-8.8-7.2-16-16-16H480c0-17.7-14.3-32-32-32z"/></svg>');--md-admonition-icon--nice-to-know:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M297.2 248.9c14.4-20.6 22.8-45.7 22.8-72.9 0-70.7-57.3-128-128-128S64 105.3 64 176c0 27.2 8.4 52.3 22.8 72.9 3.7 5.3 8.1 11.3 12.8 17.7 12.9 17.7 28.3 38.9 39.8 59.8 10.4 19 15.7 38.8 18.3 57.5H109c-2.2-12-5.9-23.7-11.8-34.5-9.9-18-22.2-34.9-34.5-51.8-5.2-7.1-10.4-14.2-15.4-21.4C27.6 247.9 16 213.3 16 176 16 78.8 94.8 0 192 0s176 78.8 176 176c0 37.3-11.6 71.9-31.4 100.3-5 7.2-10.2 14.3-15.4 21.4-12.3 16.8-24.6 33.7-34.5 51.8-5.9 10.8-9.6 22.5-11.8 34.5h-48.5c2.6-18.7 7.9-38.6 18.3-57.5 11.5-20.9 26.9-42.1 39.8-59.8 4.7-6.4 9-12.4 12.7-17.7zM192 128c-26.5 0-48 21.5-48 48 0 8.8-7.2 16-16 16s-16-7.2-16-16c0-44.2 35.8-80 80-80 8.8 0 16 7.2 16 16s-7.2 16-16 16zm0 384c-44.2 0-80-35.8-80-80v-16h160v16c0 44.2-35.8 80-80 80z"/></svg>');}</style>



    
    
      
    
    
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
  
      

    

  
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      document.body.addEventListener("click", function(ev) {
        if (ev.target instanceof HTMLElement) {
          var el = ev.target.closest("a[href^=http]")
          if (el)
            ga("send", "event", "outbound", "click", el.href)
        }
      })
    })
  </script>

    
    

  
  
  
    
  

  
  

  
  <meta property="og:type" content="website" />
  <meta property="og:title" content="LUMI training materials - Slurm on LUMI" />
  <meta property="og:description" content="None" />
  <meta property="og:url" content="https://lumi-supercomputer.github.io/LUMI-training-materials/2day-20240502/06_Slurm/" />
  <meta property="og:image" content="https://lumi-supercomputer.github.io/LUMI-training-materials/assets/images/banner.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="1080" />
  <meta property="og:image:height" content="568" />

  
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:site" content="@LUMIhpc" />
  <meta name="twitter:creator" content="@LUMIhpc" />
  <meta name="twitter:title" content="LUMI training materials - Slurm on LUMI" />
  <meta name="twitter:description" content="None" />
  <meta name="twitter:image" content="https://lumi-supercomputer.github.io/LUMI-training-materials/assets/images/banner.png" />

  <style>
    [data-md-color-primary="lumi"] {
      --md-primary-fg-color: hsla(0, 0%, 100%, 1);
      --md-primary-fg-color--light: hsla(0, 0%, 100%, 0.7);
      --md-primary-fg-color--dark: hsla(0, 0%, 0%, 0.07);
      --md-primary-bg-color: hsla(0, 0%, 0%, 0.87);
      --md-primary-bg-color--light: hsla(0, 0%, 0%, 0.54);
      --md-button-bg-color: hsla(207,100%,28%, 1);
      --md-button-bg-color--light: hsla(207,100%,38%, 1);
      --md-typeset-a-color: hsla(207,100%,28%, 1);
    }
    
    [data-md-color-accent="lumi"] {
      --md-accent-fg-color: hsla(0,0%,0%, 1);
      --md-accent-fg-color--transparent: hsla(0,0%,0%, 0.1);
      --md-accent-bg-color: hsla(0, 0%, 100%, 1);
      --md-accent-bg-color--light: hsla(0, 0%, 100%, 0.7);
    }
  </style>

  


  
  

    <link
      rel="stylesheet"
      href="../../assets/stylesheets/extra-a8af0af3d0.css"
    />
    <link
      rel="stylesheet"
      href="../../assets/stylesheets/overrides-5193e6f6df.css"
    />
    <link
      rel="stylesheet"
      id="typekit-fonts-css"
      href="https://use.typekit.net/nlo5lta.css?ver=5.5.3"
      type="text/css" media="all"
    />

  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="lumi" data-md-color-accent="lumi">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#slurm-on-lumi" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LUMI training materials" class="md-header__button md-logo" aria-label="LUMI training materials" data-md-component="logo">
      
  
  <?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg width="100%" height="100%" viewBox="0 0 723 212" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
    <g id="Layer-2" serif:id="Layer 2">
        <g transform="matrix(1,0,0,1,0,175.883)">
            <path d="M0,-140.304L0,0L102.89,0L102.89,-24.599L26.658,-24.599L26.658,-140.304L0,-140.304Z" style="fill:rgb(29,29,27);fill-rule:nonzero;"/>
        </g>
        <g transform="matrix(1,0,0,1,257.262,35.5793)">
            <path d="M0,140.304C-17.585,140.304 -32.083,135.347 -43.588,125.338C-55,115.423 -60.799,103.357 -60.799,89.14L-60.799,0L-34.328,0L-34.328,81.563C-34.328,92.413 -31.054,100.645 -24.319,105.976C-17.866,111.12 -9.728,113.74 0,113.74C9.728,113.74 17.865,111.12 24.319,105.976C31.054,100.645 34.328,92.413 34.328,81.563L34.328,0L60.799,0L60.799,89.14C60.799,103.357 55.093,115.329 43.588,125.338C32.083,135.347 17.585,140.304 0,140.304" style="fill:rgb(29,29,27);fill-rule:nonzero;"/>
        </g>
        <g transform="matrix(1,0,0,1,550.374,119.668)">
            <path d="M0,-27.874L-43.588,20.671L-87.176,-27.874L-87.176,56.215L-113.74,56.215L-113.74,-84.089L-105.695,-84.089L-43.588,-13.47L18.614,-84.089L26.658,-84.089L26.658,56.215L0.094,56.215L0,-27.874Z" style="fill:rgb(29,29,27);fill-rule:nonzero;"/>
        </g>
        <rect x="695.604" y="35.486" width="26.658" height="140.397" style="fill:rgb(29,29,27);"/>
        <g transform="matrix(-1,0,0,1,722.262,-203.194)">
            <rect x="0" y="203.194" width="722.262" height="8.175" style="fill:rgb(29,29,27);"/>
        </g>
        <g transform="matrix(-1,0,0,1,722.262,203.194)">
            <rect x="0" y="0" width="722.262" height="8.175" style="fill:rgb(29,29,27);"/>
        </g>
    </g>
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LUMI training materials
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Slurm on LUMI
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LUMI training materials" class="md-nav__button md-logo" aria-label="LUMI training materials" data-md-component="logo">
      
  
  <?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg width="100%" height="100%" viewBox="0 0 723 212" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
    <g id="Layer-2" serif:id="Layer 2">
        <g transform="matrix(1,0,0,1,0,175.883)">
            <path d="M0,-140.304L0,0L102.89,0L102.89,-24.599L26.658,-24.599L26.658,-140.304L0,-140.304Z" style="fill:rgb(29,29,27);fill-rule:nonzero;"/>
        </g>
        <g transform="matrix(1,0,0,1,257.262,35.5793)">
            <path d="M0,140.304C-17.585,140.304 -32.083,135.347 -43.588,125.338C-55,115.423 -60.799,103.357 -60.799,89.14L-60.799,0L-34.328,0L-34.328,81.563C-34.328,92.413 -31.054,100.645 -24.319,105.976C-17.866,111.12 -9.728,113.74 0,113.74C9.728,113.74 17.865,111.12 24.319,105.976C31.054,100.645 34.328,92.413 34.328,81.563L34.328,0L60.799,0L60.799,89.14C60.799,103.357 55.093,115.329 43.588,125.338C32.083,135.347 17.585,140.304 0,140.304" style="fill:rgb(29,29,27);fill-rule:nonzero;"/>
        </g>
        <g transform="matrix(1,0,0,1,550.374,119.668)">
            <path d="M0,-27.874L-43.588,20.671L-87.176,-27.874L-87.176,56.215L-113.74,56.215L-113.74,-84.089L-105.695,-84.089L-43.588,-13.47L18.614,-84.089L26.658,-84.089L26.658,56.215L0.094,56.215L0,-27.874Z" style="fill:rgb(29,29,27);fill-rule:nonzero;"/>
        </g>
        <rect x="695.604" y="35.486" width="26.658" height="140.397" style="fill:rgb(29,29,27);"/>
        <g transform="matrix(-1,0,0,1,722.262,-203.194)">
            <rect x="0" y="203.194" width="722.262" height="8.175" style="fill:rgb(29,29,27);"/>
        </g>
        <g transform="matrix(-1,0,0,1,722.262,203.194)">
            <rect x="0" y="0" width="722.262" height="8.175" style="fill:rgb(29,29,27);"/>
        </g>
    </g>
</svg>

    </a>
    LUMI training materials
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../User-Updates/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    User Updates
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            User Updates
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../User-Updates/Update-202311/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    October-November 2023
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../User-Updates/Update-202308/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    August 2023
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            August 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../User-Updates/Update-202308/lumig-lownoise/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Low-noise mode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../User-Updates/Update-202308/lumig-devg/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    dev-g and eap
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../User-Updates/Update-202308/responsible-use/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Responsible use
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../User-Coffee-Breaks/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    User Coffee Breaks
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            User Coffee Breaks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Supercomputing with LUMI May 2024
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Supercomputing with LUMI May 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../4day-20240423/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Comprehensive LUMI April 2024
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Comprehensive LUMI April 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../1day-20240208/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    1-day February 2024
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            1-day February 2024
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_01_LUMI_Architecture/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LUMI Architecture
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_02_HPE_Cray_Programming_Environment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    HPE Cray PE
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_03_Modules_on_LUMI/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_04_LUMI_Software_Stacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Software stacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/05_Exercises_1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exercises 1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_06_Running_Jobs_on_LUMI/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Running jobs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/07_Exercises_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exercises 2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_08_Introduction_to_Lustre_and_Best_Practices/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    I/O and file systems
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/video_09_LUMI_User_Support/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LUMI support
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20240208/A01_Documentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Documentation
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Profiling-20231122/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Profiling November 2023
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Profiling November 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../4day-20231003/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Comprehensive LUMI October 2023
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Comprehensive LUMI October 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Partner courses
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            Partner courses
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://zenodo.org/records/10610643" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GROMACS workshop 2024
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
        
          
          <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Archive
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../1day-20230921/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    1-day September 2023
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_10_1" id="__nav_10_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_1">
            <span class="md-nav__icon md-icon"></span>
            1-day September 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/01_Architecture/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LUMI Architecture
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/02_CPE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    HPE Cray PE
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/03_Modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/04_Software_stacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Software stacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/05_Exercises_1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exercises 1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/06_Running_jobs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Running jobs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/07_Exercises_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exercises 2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/08_Lustre_intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    I/O and file systems
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/09_LUMI_support/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LUMI support
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230921/A01_Documentation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Documentation
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../4day-20230530/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Comprehensive LUMI May-June 2023
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_2">
            <span class="md-nav__icon md-icon"></span>
            Comprehensive LUMI May-June 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../1day-20230509/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    1-day May 2023
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_10_3" id="__nav_10_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_3">
            <span class="md-nav__icon md-icon"></span>
            1-day May 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/01_Architecture/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LUMI Architecture
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/02_CPE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    HPE Cray PE
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/03_Modules/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modules
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/04_Software_stacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Software stacks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/05_Exercises_1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exercises 1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/06_Running_jobs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Running jobs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/07_Exercises_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exercises 2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/08_Lustre_intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    I/O and file systems
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../1day-20230509/09_LUMI_support/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LUMI support
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Hackathon-20230417/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Hackathon April 2023
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_4">
            <span class="md-nav__icon md-icon"></span>
            Hackathon April 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../Profiling-20230413/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Profiling April 2013
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_5">
            <span class="md-nav__icon md-icon"></span>
            Profiling April 2013
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../4day-20230214/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Comprehensive LUMI February 2023
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_6">
            <span class="md-nav__icon md-icon"></span>
            Comprehensive LUMI February 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../LUMI-G-20230111/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    LUMI-G January 2023
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_7">
            <span class="md-nav__icon md-icon"></span>
            LUMI-G January 2023
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../PEAP-Q-20221123/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEAP-Q November 2022
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_10_8" id="__nav_10_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_8">
            <span class="md-nav__icon md-icon"></span>
            PEAP-Q November 2022
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../PEAP-Q-20221123/schedule/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Schedule
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_9" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../LUMI-G-20220823/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    LUMI-G August 2022
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_10_9" id="__nav_10_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_9">
            <span class="md-nav__icon md-icon"></span>
            LUMI-G August 2022
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LUMI-G-20220823/hackmd_notes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hackmd notes
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_10" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../EasyBuild-CSC-20220509/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    EasyBuild May 2022
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_10_10" id="__nav_10_10_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_10">
            <span class="md-nav__icon md-icon"></span>
            EasyBuild May 2022
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://klust.github.io/easybuild-tutorial/2022-CSC_and_LO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Course notes
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10_11" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../PEAP-Q-20220427/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    PEAP-Q April 2022
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_10_11" id="__nav_10_11_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_10_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10_11">
            <span class="md-nav__icon md-icon"></span>
            PEAP-Q April 2022
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../PEAP-Q-20220427/hackmd_notes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hackmd notes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../PEAP-Q-20220427/software_stacks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LUMI Software Stacks
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-slurm" class="md-nav__link">
    <span class="md-ellipsis">
      What is Slurm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slurm-concepts-physical-resources" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm concepts: Physical resources
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slurm-concepts-logical-resources" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm concepts: Logical resources
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slurm-is-first-and-foremost-a-batch-scheduler" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm is first and foremost a batch scheduler
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-slurm-batch-script" class="md-nav__link">
    <span class="md-ellipsis">
      A Slurm batch script
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#partitions" class="md-nav__link">
    <span class="md-ellipsis">
      Partitions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#queueing-and-fairness" class="md-nav__link">
    <span class="md-ellipsis">
      Queueing and fairness
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#accounting-of-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Accounting of jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#managing-slurm-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Managing Slurm jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#creating-a-slurm-job" class="md-nav__link">
    <span class="md-ellipsis">
      Creating a Slurm job
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#passing-options-to-srun-salloc-and-sbatch" class="md-nav__link">
    <span class="md-ellipsis">
      Passing options to srun, salloc and sbatch
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-options" class="md-nav__link">
    <span class="md-ellipsis">
      Specifying options
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#some-common-options-to-all-partitions" class="md-nav__link">
    <span class="md-ellipsis">
      Some common options to all partitions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#redirecting-output" class="md-nav__link">
    <span class="md-ellipsis">
      Redirecting output
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#requesting-resources-cpus-and-gpus" class="md-nav__link">
    <span class="md-ellipsis">
      Requesting resources: CPUs and GPUs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resources-for-per-node-allocations" class="md-nav__link">
    <span class="md-ellipsis">
      Resources for per-node allocations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#per-node-allocations-starting-a-job-step" class="md-nav__link">
    <span class="md-ellipsis">
      Per-node allocations: Starting a job step
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Per-node allocations: Starting a job step">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-warning-for-gpu-applications" class="md-nav__link">
    <span class="md-ellipsis">
      A warning for GPU applications
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#turning-simultaneous-multithreading-on-or-off" class="md-nav__link">
    <span class="md-ellipsis">
      Turning simultaneous multithreading on or off
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#per-core-allocations" class="md-nav__link">
    <span class="md-ellipsis">
      Per-core allocations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Per-core allocations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#when-to-use" class="md-nav__link">
    <span class="md-ellipsis">
      When to use?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resource-request" class="md-nav__link">
    <span class="md-ellipsis">
      Resource request
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#different-job-steps-in-a-single-job" class="md-nav__link">
    <span class="md-ellipsis">
      Different job steps in a single job
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-job-environment" class="md-nav__link">
    <span class="md-ellipsis">
      The job environment
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#automatic-requeueing" class="md-nav__link">
    <span class="md-ellipsis">
      Automatic requeueing
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-dependencies" class="md-nav__link">
    <span class="md-ellipsis">
      Job dependencies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interactive-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Interactive jobs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Interactive jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#interactive-jobs-with-salloc" class="md-nav__link">
    <span class="md-ellipsis">
      Interactive jobs with salloc
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interactive-jobs-with-srun" class="md-nav__link">
    <span class="md-ellipsis">
      Interactive jobs with srun
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inspecting-a-running-job" class="md-nav__link">
    <span class="md-ellipsis">
      Inspecting a running job
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-arrays" class="md-nav__link">
    <span class="md-ellipsis">
      Job arrays
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#heterogeneous-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Heterogeneous jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simultaneous-job-steps" class="md-nav__link">
    <span class="md-ellipsis">
      Simultaneous job steps
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slurm-job-monitoring-commands" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm job monitoring commands
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Slurm job monitoring commands">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-sstat-command" class="md-nav__link">
    <span class="md-ellipsis">
      The sstat command
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-sacct-command" class="md-nav__link">
    <span class="md-ellipsis">
      The sacct command
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-sreport-command" class="md-nav__link">
    <span class="md-ellipsis">
      The sreport command
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#other-trainings-and-materials" class="md-nav__link">
    <span class="md-ellipsis">
      Other trainings and materials
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="slurm-on-lumi">Slurm on LUMI<a class="headerlink" href="#slurm-on-lumi" title="Permanent link">&para;</a></h1>
<!-- BELGIUM 
!!! Audience "Who is this for?"
    We assume some familiarity with job scheduling in this section. The notes will cover
    some of the more basic aspects of Slurm also, though as this version of the notes is
    intended for Belgian users and since all but one HPC site in Belgium currently teaches
    Slurm to their users, some elements will be covered only briefly.

    Even if you have a lot of experience with Slurm, it may still be useful to have a quick
    look at this section as Slurm is not always configured in the same way.
-->

<!-- GENERAL More general version -->
<div class="admonition audience">
<p class="admonition-title">Who is this for?</p>
<p>We assume some familiarity with job scheduling in this section. The notes will cover
some of the more basic aspects of Slurm also, though it may be rather brief on some
aspects of Slurm as we assume the majority of users is already rather familiar with
Slurm.</p>
<p>Even if you have a lot of experience with Slurm, it may still be useful to have a quick
look at this section as Slurm is not always configured in the same way.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Links to Slurm material</p>
<p>Links to Slurm material on this web page are all for the version on LUMI at the time of
the course. Links in the PDF of the slides however are to the newest version.</p>
</div>
<h2 id="what-is-slurm">What is Slurm<a class="headerlink" href="#what-is-slurm" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide What is Slurm" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/WhatIsSlurm.png" /></p>
</figure>
<p>Slurm is both a resource manager and job scheduler for supercomputers in a single package.</p>
<p>A resource manager manages all user-exposed resources on a supercomputer: cores, GPUs or other
accelerators, nodes, ... It sets up the resources to run a job and cleans up after the job,
and may also give additional facilities to start applications in a job. Slurm does all this.</p>
<p>But Slurm is also a job scheduler. It will assign jobs to resources, following policies set
by sysadmins to ensure a good use of the machine and a fair distribution of resources among
projects.</p>
<p>Slurm is the most popular resource manager and job scheduler at the moment and is used on more
than 50% of all big supercomputers. It is an open source package with commercial support.
Slurm is a very flexible and configurable tool with the help of tens or even hundreds of
plugins. This also implies that Slurm installations on different machines can also differ
a lot and that not all features available on one computer are also available on another.
So do not expect that Slurm will behave the same on LUMI as on that other computer you're
familiar with, even if that other computer may have hardware that is very similar to LUMI.</p>
<p>Slurm is starting to show its age and has trouble dealing in an elegant and proper way with
the deep hierarcy of resources in modern supercomputers. So Slurm will not always be as
straightforward to use as we would like it, and some tricks will be needed on LUMI. Yet there
is no better option at this moment that is sufficiently mature.</p>
<!-- BELGIUM 
!!! lumi-be "Other systems in Belgium"
    Previously at the VSC Torque was used as the resource manager and Moab as the scheduler.
    All VSC sites now use Slurm, though at UGent it is still hidden behind wrappers that
    mimic Torque/Moab. As we shall see in this and the next session, Slurm, which is a
    more modern resource manager and scheduler than the Torque-Moab combination, has 
    already trouble dealing well with the hierarchy of resources in a modern supercomputer.
    Yet it is still a lot better at it than Torque and Moab. So no, the wrappers used on
    the HPC systems managed by UGent will not be installed on LUMI.
-->

<div class="admonition nice-to-know">
<p class="admonition-title">Nice to know...</p>
<p>Lawrence Livermore National Laboratory, the USA national laboratory that 
originally developed Slurm is now working on the 
development of andother resource and job management framework called 
<a href="https://computing.llnl.gov/projects/flux-building-framework-resource-management">flux</a>.
It will be used on the third USA exascale supercomputer El Capitan which is currently
being assembled. </p>
</div>
<h2 id="slurm-concepts-physical-resources">Slurm concepts: Physical resources<a class="headerlink" href="#slurm-concepts-physical-resources" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Slurm concepts 1" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/SlurmConceptsPhys.png" /></p>
</figure>
<p>The machine model of Slurm is bit more limited than what we would like for LUMI. </p>
<p>On the CPU side it knows:</p>
<ul>
<li>
<p>A <strong>node</strong>: The hardware that runs a single operating system image</p>
</li>
<li>
<p>A <strong>socket</strong>: On LUMI a Slurm socket corresponds to a physical socket, so there are two sockets on the 
    CPU nodes and a single socket on a GPU node.</p>
<p>Alternatively a cluster could be configured to let a Slurm socket correspond to a NUMA domain or 
L3 cache domain, but this is something that sysadmins need to do so even if this would be useful for
your job, you cannot do so.</p>
</li>
<li>
<p>A <strong>core</strong> is a physical core in the system</p>
</li>
<li>
<p>A <strong>thread</strong> is a hardware thread in the system (virtual core)</p>
</li>
<li>
<p>A <strong>CPU</strong> is a "consumable resource" and the unit at which CPU processing capacity is allocated to a job.
    On LUMI a Slurm CPU corresponds to a physical core, but Slurm could also be configured to let it correspond
    to a hardware thread.</p>
</li>
</ul>
<p>The first three bullets already show the problem we have with Slurm on LUMI: For three levels in the hierarchy
of CPU resources on a node: the socket, the NUMA domain and the L3 cache domain, there is only one concept in
Slurm, so we are not able to fully specify the hierarchy in resources that we want when sharing nodes with 
other jobs.</p>
<p>A <strong>GPU</strong> in Slurm is an accelerator and on LUMI corresponds to one GCD of an MI250X, so one half of an MI250X.</p>
<h2 id="slurm-concepts-logical-resources">Slurm concepts: Logical resources<a class="headerlink" href="#slurm-concepts-logical-resources" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Slurm concepts 2" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/SlurmConceptsLog.png" /></p>
</figure>
<ul>
<li>
<p>A <strong>partition</strong>: is a job queue with limits and access control. Limits include maximum
    wall time for a job, the maximum number of nodes a single job can use, or the maximum
    number of jobs a user can run simultaneously in the partition. The access control 
    mechanism determines who can run jobs in the partition.</p>
<p>It is different from what we call LUMI-C and LUMI-G, or the <code>partition/C</code> and <code>partition/G</code>
modules in the LUMI software stacks.</p>
<p>Each partition covers a number of nodes, but partitions can overlap. This is not the case
for the partitions that are visible to users on LUMI. Each partition covers a disjunct set
of nodes. There are hidden partitions though that overlap with other partitions, but they
are not accessible to regular users.</p>
</li>
<li>
<p>A <strong>job</strong> in Slurm is basically only a resource allocation request.</p>
</li>
<li>
<p>A <strong>job step</strong> is a set of (possibly parallel) tasks within a job</p>
<ul>
<li>
<p>Each batch job always has a special job step called the batch job step which runs
    the job script on the first node of a job allocation.</p>
</li>
<li>
<p>An MPI application will typically run in its own job step.</p>
</li>
<li>
<p>Serial or shared memory applications are often run in the batch job step but there
    can be good reasons to create a separate job step for those applications.</p>
</li>
</ul>
</li>
<li>
<p>A <strong>task</strong> executes in a job step and corresponds to a Linux process (and possibly subprocesses)</p>
<ul>
<li>
<p>A shared memory program is a single task</p>
</li>
<li>
<p>In an MPI application: Each rank (so each MPI process) is a task</p>
<ul>
<li>
<p>Pure MPI: Each task uses a single CPU (which is also a core for us)</p>
</li>
<li>
<p>Hybrid MPI/OpenMP applications: Each task can use multiple CPUs</p>
</li>
</ul>
</li>
</ul>
<p>Of course a task cannot use more CPUs than available in a single node as a process can only run
within a single operating system image.</p>
</li>
</ul>
<h2 id="slurm-is-first-and-foremost-a-batch-scheduler">Slurm is first and foremost a batch scheduler<a class="headerlink" href="#slurm-is-first-and-foremost-a-batch-scheduler" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Slurm is first and foremost a batch scheduler" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/BatchScheduler.png" /></p>
</figure>
<p>And LUMI is in the first place a batch processing supercomputer.</p>
<p>A supercomputer like LuMI is a very large and very expensive machine. This implies that it also has to be
used as efficiently as possible which in turn implies that we don't want to wast time waiting for input
as is the case in an interactive program.</p>
<p>On top of that, very few programs can use the whole capacity of the supercomputer, so in practice a
supercomputer is a shared resource and each simultaneous user gets a fraction on the machine depending
on the requirements that they specify. Yet, as parallel applications work best when performance is predictable,
it is also important to isolate users enough from each other.</p>
<p>Research supercomputers are also typically very busy with lots of users so one often has to wait a little 
before resources are available. This may be different on some commercial supercomputers and is also different
on commercial cloud infrastructures, but the "price per unit of work done on the cluster" is also very 
different from an academic supercomputer and few or no funding agencies are willing to carry that cost.</p>
<p>Due to all this the preferred execution model on supercomputer is via batch jobs as they don't have to wait
for input from the user, specified via batch scripts with resource specification where the user asks 
precisely the amount of resources needed for the job, submitted to a queueing system with a scheduler
to select the next job in a fair way based on available resources and scheduling policies set by the 
compute centre.</p>
<p>LUMI does have some facilities for interactive jobs, and with the introduction of Open On Demand some more
may be available. But it is far from ideal, and you will also be billed for the idle time of the resources
you request. In fact, if you only need some interactive resources for a quick 10-minute experiment and don't 
need too many resources, the wait may be minimal thanks to a scheduler mechanism called backfill where the
scheduler looks for small and short jobs to fill up the gaps left while the scheduler is collecting resources
for a big job.</p>
<h2 id="a-slurm-batch-script">A Slurm batch script<a class="headerlink" href="#a-slurm-batch-script" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide A Slurm batch script" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/BatchScript.png" /></p>
</figure>
<p>Slurm batch scripts (also called job scripts) are conceptually not that different from batch scripts for other HPC schedulers.
A typical batch script will have 4 parts:</p>
<ol>
<li>
<p>The shebang line with the shell to use. We advise to use the bash shell (<code>/bin/bash</code> or <code>/usr/bin/bash</code>)
    If omitted, a very restricted shell will be used and some commands (e.g., related to modules)
    may fail. In principle any shell language that uses a hashtag to denote comments can be used, but
    we would advise against experimenting and the LUMI User Support Team and VSC support teams will only
    support bash.</p>
</li>
<li>
<p>Specification of resources and some other instructions for the scheduler and resource manager. This part
    is also optional as one can also pass the instructions via the command line of <code>sbatch</code>, the command to
    submit a batch job. But again, we would advise against omitting this block as specifying all options on
    the command line can be very tedious.</p>
</li>
<li>
<p>Building a suitable environment for the job. This part is also optional as on LUMI, Slurm will copy the
    environment from the node from which the job was submitted. This may not be the ideal envrionment for your job,
    and if you later resubmit the job you may do so accidentally from a different environment so it is a good practice
    to specify the environment.</p>
</li>
<li>
<p>The commands you want to execute.</p>
</li>
</ol>
<p>Blocks 3 and 4 can of course be mixed as you may want to execute a second command in a different environment.</p>
<p>On the following slides we will explore in particular the second block and to some extent how to start programs
(the fourth block).</p>
<div class="admonition note">
<p class="admonition-title">lumi-CPEtools module</p>
<p>The <a href="https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/l/lumi-CPEtools/"><code>lumi-CPEtools</code> module</a>
will be used a lot in this session of the course and in the next one on binding. It contains among
other things a number of programs to quickly visualise how a serial, OpenMP, MPI or hybrid OpenMP/MPI
application would run on LUMI and which cores and GPUs would be used. It is a very useful tool to 
discover how Slurm options work without using a lot of billing units and we would advise you to 
use it whenever you suspect Slurm isn't doing what you meant to do.</p>
<p>It has its <a href="https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/l/lumi-CPEtools/">documentation page in the LUMI Software Library</a>.</p>
</div>
<h2 id="partitions">Partitions<a class="headerlink" href="#partitions" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Partitions 1" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/Partitions_1.png" /></p>
</figure>
<div class="admonition remark">
<p class="admonition-title">Remark</p>
<p>Jobs run in partitions so the first thing we should wonder when setting up a job is which partition
to use for a job (or sometimes partitions in case of a heterogeneous job which will be discussed
later).</p>
</div>
<p>Slurm partitions are possibly overlapping groups of nodes with similar resources or associated limits. 
Each partition typically targets a particular job profile. E.g., LUMI has partitions for large multi-node jobs,
for smaller jobs that often cannot fill a node, for some quick debug work and for some special resources that 
are very limited (the nodes with 4TB of memory and the nodes with GPUs for visualisation). The number of jobs
a user can have running simultaneously in each partition or have in the queue, the maximum wall time for a job,
the number of nodes a job can use are all different for different partitions.</p>
<p>There are two types of partitions on LUMI:</p>
<ul>
<li>
<p><strong>Exclusive node use by a single job.</strong> This ensures that parallel jobs can have a clean environment
    with no jitter caused by other users running on the node and with full control of how tasks and threads
    are mapped onto the available resources. This may be essential for the performance of a lot of codes.</p>
</li>
<li>
<p><strong>Allocatable by resources (CPU and GPU).</strong> In these partitions nodes are shared by multiple users and
    multiple jobs, though in principle it is possible to ask for exclusive use which will however increase
    your waiting time in the queue. The cores you get are not always continuously numbered, nor do you 
    always get the minimum number of nodes needed for the number of tasks requested. A proper mapping 
    of cores onto GPUs is also not ensured at all. The fragmentation of resources is a real problem on
    these nodes and this may be an issue for the performance of your code.</p>
</li>
</ul>
<p>It is also important to realise that the default settings for certain Slurm parameters may differ
between partitions and hence a node in a partition allocatable by resource but for which exclusive 
access was requested may still react differently to a node in the exclusive partitions.</p>
<p>In general it is important to use some common sense when requesting resources and to have some understanding
of what each Slurm parameter really means. Overspecifying resources (using more parameters than needed for the
desired effect) may result in unexpected conflicts between parameters and error messages.</p>
<!--
<figure markdown style="border: 1px solid #000">
  ![Slide Partitions 2](https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/Partitions_2.png){ loading=lazy }
</figure>
-->

<figure style="border: 1px solid #000">
<p><img alt="Slide Partitions allocatable per node" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PartitionsPerNode.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide Partitions allocatable by resources" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PartitionsByResources.png" /></p>
</figure>
<p>For the overview of Slurm partitions, see the <a href="https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/partitions/">LUMI documentation, "Slurm partitions" page</a>.
In the overview on the slides we did not mention partitions that are hidden to regular users.</p>
<p>The policies for partitions and the available partitions may change over time to fine tune the
operation of LUMI and depending on needs observed by the system administrators and LUMI
User Support Team, so don't take the above tables in the slide for granted.</p>
<figure style="border: 1px solid #000">
<p><img alt="Slide Partitions 3" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PartitionsCommands.png" /></p>
</figure>
<p>Some useful commands with respect to Slurm partitions:</p>
<ul>
<li>
<p>To request information about the available partitions, use <code>sinfo -s</code>: </p>
<div class="highlight"><pre><span></span><code>$ sinfo -s
PARTITION      AVAIL  TIMELIMIT   NODES(A/I/O/T) NODELIST
debug             up      30:00          0/8/0/8 nid[002500-002501,002504-002506,002595-002597]
interactive       up    8:00:00          4/0/0/4 nid[002502,002507,002594,002599]
ju-standard       up 2-00:00:00    571/11/26/608 nid[001256-001511,002024-002279,002600-002695]
ju-strategic      up 2-00:00:00    16/227/13/256 nid[001000-001255]
q_fiqci           up      15:00          0/1/0/1 nid002598
q_industry        up      15:00          0/1/0/1 nid002598
q_nordiq          up      15:00          0/1/0/1 nid002503
small             up 3-00:00:00      300/1/5/306 nid[002280-002499,002508-002593]
standard          up 2-00:00:00    730/87/47/864 nid[001512-002023,002696-003047]
dev-g             up    3:00:00       19/24/5/48 nid[005002-005025,007954-007977]
ju-standard-g     up 2-00:00:00    818/77/65/960 nid[005280-005495,005620-005991,006240-006487,007728-007851]
ju-strategic-g    up 7-00:00:00    329/52/23/404 nid[005124-005279,005992-006239]
small-g           up 3-00:00:00     161/32/5/198 nid[005026-005123,007852-007951]
standard-g        up 2-00:00:00  910/31/423/1364 nid[005496-005619,006488-007727]
largemem          up 1-00:00:00          5/0/1/6 nid[000101-000106]
lumid             up    4:00:00          0/7/1/8 nid[000016-000023]
</code></pre></div>
<p>The fourth column shows 4 numbers: The number of nodes that are currently fully or partially allocated
to jobs, the number of idle nodes, the number of nodes in one of the other possible states (and not
user-accessible) and the total number of nodes in the partition. When these nodes were written
a large number of nodes were suffering from hardware problems with the Slingshot network cards and
we were waiting for replacement parts which explains the thigh number of CPU nodes in the "other"
field.</p>
<!-- BELGIUM 
Note that this overview may show partitions that are not hidden but also not accessible to everyone. E.g., 
the <code>q_nordic</code> and <code>q_fiqci</code> partitions are used to access experimental quantum computers that are only
available to some users of those countries that paid for those machines, which does not include Belgium.
-->
<p><!-- GENERAL More general version -->
Note that this overview may show partitions that are not hidden but also not accessible to everyone. E.g., 
the <code>q_nordic</code> and <code>q_fiqci</code> partitions are used to access experimental quantum computers that are only
available to some users of those countries that paid for those machines.</p>
<p>The <code>eap</code> partition will likely be phased out over time and is a remainder of a platform used for early 
development before LUMI-G was attached to the machine. At the moment it allows users to experiment freely 
with the GPU nodes.</p>
<p>It is not clear to the LUMI Support Team what the <code>interactive</code> partition, that uses dome GPU nodes, is 
meant for as it was introduced without informting the support. The resources in that partition are very
limited so it is not meant for widespread use.</p>
</li>
<li>
<p>For technically-oriented people, some more details about a partition can be obtained with
    <code>scontrol show partition &lt;partition-name&gt;</code>.</p>
</li>
</ul>
<details class="example">
<summary>Additional example with <code>sinfo</code></summary>
<p>Try</p>
<p><div class="highlight"><pre><span></span><code>$ sinfo --format &quot;%4D %10P %25f %.4c %.8m %25G %N&quot;
NODE PARTITION  AVAIL_FEATURES            CPUS   MEMORY GRES                      NODELIST
5    debug      AMD_EPYC_7763,x1005        256   229376 (null)                    nid[002500-002501,002504-002506]
3    debug      AMD_EPYC_7763,x1006        256   229376 (null)                    nid[002595-002597]
2    interactiv AMD_EPYC_7763,x1005        256   229376 (null)                    nid[002502,002507]
2    interactiv AMD_EPYC_7763,x1006        256   229376 (null)                    nid[002594,002599]
256  ju-standar AMD_EPYC_7763,x1001        256   229376 (null)                    nid[001256-001511]
256  ju-standar AMD_EPYC_7763,x1004        256   229376 (null)                    nid[002024-002279]
96   ju-standar AMD_EPYC_7763,x1006        256   229376 (null)                    nid[002600-002695]
256  ju-strateg AMD_EPYC_7763,x1000        256   229376 (null)                    nid[001000-001255]
1    q_fiqci    AMD_EPYC_7763,x1006        256   229376 (null)                    nid002598
1    q_industry AMD_EPYC_7763,x1006        256   229376 (null)                    nid002598
1    q_nordiq   AMD_EPYC_7763,x1005        256   229376 (null)                    nid002503
248  small      AMD_EPYC_7763,x1005        256  229376+ (null)                    nid[002280-002499,002508-002535]
58   small      AMD_EPYC_7763,x1006        256   229376 (null)                    nid[002536-002593]
256  standard   AMD_EPYC_7763,x1003        256   229376 (null)                    nid[001768-002023]
256  standard   AMD_EPYC_7763,x1002        256   229376 (null)                    nid[001512-001767]
256  standard   AMD_EPYC_7763,x1007        256   229376 (null)                    nid[002792-003047]
96   standard   AMD_EPYC_7763,x1006        256   229376 (null)                    nid[002696-002791]
2    dev-g      AMD_EPYC_7A53,x1405        128   491520 gpu:mi250:8               nid[007974-007975]
22   dev-g      AMD_EPYC_7A53,x1405        128   491520 gpu:mi250:8(S:0)          nid[007954-007973,007976-007977]
24   dev-g      AMD_EPYC_7A53,x1100        128   491520 gpu:mi250:8(S:0)          nid[005002-005025]
2    ju-standar AMD_EPYC_7A53,x1102        128   491520 gpu:mi250:8               nid[005356-005357]
7    ju-standar AMD_EPYC_7A53,x1103        128   491520 gpu:mi250:8               nid[005472-005473,005478-005479,005486-005487,005493]
8    ju-standar AMD_EPYC_7A53,x1105        128   491520 gpu:mi250:8               nid[005648-005649,005679,005682-005683,005735,005738-005739]
2    ju-standar AMD_EPYC_7A53,x1200        128   491520 gpu:mi250:8               nid[005810-005811]
3    ju-standar AMD_EPYC_7A53,x1204        128   491520 gpu:mi250:8               nid[006301,006312-006313]
1    ju-standar AMD_EPYC_7A53,x1205        128   491520 gpu:mi250:8               nid006367
2    ju-standar AMD_EPYC_7A53,x1404        128   491520 gpu:mi250:8               nid[007760-007761]
9    ju-standar AMD_EPYC_7A53,x1201        128   491520 gpu:mi250:8               nid[005881,005886-005887,005897,005917,005919,005939,005969,005991]
90   ju-standar AMD_EPYC_7A53,x1102        128   491520 gpu:mi250:8(S:0)          nid[005280-005355,005358-005371]
117  ju-standar AMD_EPYC_7A53,x1103        128   491520 gpu:mi250:8(S:0)          nid[005372-005471,005474-005477,005480-005485,005488-005492,005494-005495]
116  ju-standar AMD_EPYC_7A53,x1105        128   491520 gpu:mi250:8(S:0)          nid[005620-005647,005650-005678,005680-005681,005684-005734,005736-005737,005740-005743]
122  ju-standar AMD_EPYC_7A53,x1200        128   491520 gpu:mi250:8(S:0)          nid[005744-005809,005812-005867]
115  ju-standar AMD_EPYC_7A53,x1201        128   491520 gpu:mi250:8(S:0)          nid[005868-005880,005882-005885,005888-005896,005898-005916,005918,005920-005938,005940-005968,005970-005990]
121  ju-standar AMD_EPYC_7A53,x1204        128   491520 gpu:mi250:8(S:0)          nid[006240-006300,006302-006311,006314-006363]
123  ju-standar AMD_EPYC_7A53,x1205        128   491520 gpu:mi250:8(S:0)          nid[006364-006366,006368-006487]
122  ju-standar AMD_EPYC_7A53,x1404        128   491520 gpu:mi250:8(S:0)          nid[007728-007759,007762-007851]
3    ju-strateg AMD_EPYC_7A53,x1101        128   491520 gpu:mi250:8               nid[005224,005242-005243]
8    ju-strateg AMD_EPYC_7A53,x1203        128   491520 gpu:mi250:8               nid[006136-006137,006153,006201,006214-006215,006236-006237]
5    ju-strateg AMD_EPYC_7A53,x1202        128   491520 gpu:mi250:8               nid[006035,006041,006047,006080-006081]
121  ju-strateg AMD_EPYC_7A53,x1101        128   491520 gpu:mi250:8(S:0)          nid[005124-005223,005225-005241,005244-005247]
32   ju-strateg AMD_EPYC_7A53,x1102        128   491520 gpu:mi250:8(S:0)          nid[005248-005279]
116  ju-strateg AMD_EPYC_7A53,x1203        128   491520 gpu:mi250:8(S:0)          nid[006116-006135,006138-006152,006154-006200,006202-006213,006216-006235,006238-006239]
119  ju-strateg AMD_EPYC_7A53,x1202        128   491520 gpu:mi250:8(S:0)          nid[005992-006034,006036-006040,006042-006046,006048-006079,006082-006115]
1    small-g    AMD_EPYC_7A53,x1100        128   491520 gpu:mi250:8               nid005059
97   small-g    AMD_EPYC_7A53,x1100        128   491520 gpu:mi250:8(S:0)          nid[005026-005058,005060-005123]
100  small-g    AMD_EPYC_7A53,x1405        128   491520 gpu:mi250:8(S:0)          nid[007852-007951]
2    standard-g AMD_EPYC_7A53,x1104        128   491520 gpu:mi250:8               nid[005554-005555]
117  standard-g AMD_EPYC_7A53,x1300        128   491520 gpu:mi250:8(S:0)          nid[006488-006505,006510-006521,006524-006550,006552-006611]
7    standard-g AMD_EPYC_7A53,x1300        128   491520 gpu:mi250:8               nid[006506-006509,006522-006523,006551]
121  standard-g AMD_EPYC_7A53,x1301        128   491520 gpu:mi250:8(S:0)          nid[006612-006657,006660-006703,006705-006735]
3    standard-g AMD_EPYC_7A53,x1301        128   491520 gpu:mi250:8               nid[006658-006659,006704]
117  standard-g AMD_EPYC_7A53,x1302        128   491520 gpu:mi250:8(S:0)          nid[006736-006740,006744-006765,006768-006849,006852-006859]
7    standard-g AMD_EPYC_7A53,x1302        128   491520 gpu:mi250:8               nid[006741-006743,006766-006767,006850-006851]
8    standard-g AMD_EPYC_7A53,x1304        128   491520 gpu:mi250:8               nid[007000-007001,007044-007045,007076-007077,007092-007093]
5    standard-g AMD_EPYC_7A53,x1305        128   491520 gpu:mi250:8               nid[007130-007131,007172-007173,007211]
2    standard-g AMD_EPYC_7A53,x1400        128   491520 gpu:mi250:8               nid[007294-007295]
1    standard-g AMD_EPYC_7A53,x1401        128   491520 gpu:mi250:8               nid007398
1    standard-g AMD_EPYC_7A53,x1403        128   491520 gpu:mi250:8               nid007655
122  standard-g AMD_EPYC_7A53,x1104        128   491520 gpu:mi250:8(S:0)          nid[005496-005553,005556-005619]
124  standard-g AMD_EPYC_7A53,x1303        128   491520 gpu:mi250:8(S:0)          nid[006860-006983]
116  standard-g AMD_EPYC_7A53,x1304        128   491520 gpu:mi250:8(S:0)          nid[006984-006999,007002-007043,007046-007075,007078-007091,007094-007107]
119  standard-g AMD_EPYC_7A53,x1305        128   491520 gpu:mi250:8(S:0)          nid[007108-007129,007132-007171,007174-007210,007212-007231]
122  standard-g AMD_EPYC_7A53,x1400        128   491520 gpu:mi250:8(S:0)          nid[007232-007293,007296-007355]
123  standard-g AMD_EPYC_7A53,x1401        128   491520 gpu:mi250:8(S:0)          nid[007356-007397,007399-007479]
124  standard-g AMD_EPYC_7A53,x1402        128   491520 gpu:mi250:8(S:0)          nid[007480-007603]
123  standard-g AMD_EPYC_7A53,x1403        128   491520 gpu:mi250:8(S:0)          nid[007604-007654,007656-007727]
6    largemem   AMD_EPYC_7742              256 4096000+ (null)                    nid[000101-000106]
8    lumid      AMD_EPYC_7742              256  2048000 gpu:a40:8,nvme:40000      nid[000016-000023]
</code></pre></div>
(Output may vary over time)</p>
<p>This shows more information about the system. The <code>xNNNN</code> feature corresponds to groups in 
the Slingshot interconnect and may be useful if you want to try to get a job running in
a single group (which is too advanced for this course).</p>
<p>The memory size is given in megabyte (MiB, multiples of 1024). The "+" in the second group
of the small partition is because that partition also contains the 512 GB and 1 TB regular 
compute nodes. The memory reported is always 32 GB less than you would expect from the 
node specifications. This is because 32 GB on each node is reserved for the OS and the 
RAM disk it needs.</p>
</details>
<h2 id="queueing-and-fairness">Queueing and fairness<a class="headerlink" href="#queueing-and-fairness" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Fairness of queueing" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/Fairness.png" /></p>
</figure>
<div class="admonition remark">
<p class="admonition-title">Remark</p>
<p>Jobs are queued until they can run so we should wonder how that system works.</p>
</div>
<p>LUMI is a pre-exascale machine meant to foster research into exascale applications. 
As a result the scheduler setup of LUMI favours large jobs (though some users with large
jobs will claim that it doesn't do so enough yet). Most nodes are reserved for larger 
jobs (in the <code>standard</code> and <code>standard-g</code> partitions), and the priority computation
also favours larger jobs (in terms of number of nodes).</p>
<p>When you submit a job, it will be queued until suitable resources are available for the
requested time window. Keep in mind that you may see a lot of free nodes on LUMI yet your 
small job may not yet start immediately as the scheduler may be gathering nodes for a
big job.</p>
<p>The command to check the status of the queue is <code>squeue</code>. Two command line flags are useful:</p>
<ul>
<li>
<p><code>--me</code> will restrict the output to your jobs only</p>
</li>
<li>
<p><code>--start</code> will give an estimated start time for each job. Note that this really doesn't say 
    much as the scheduler cannot predict the future. On one hand, other jobs that are running
    already or scheduled to run before your job, may have overestimated the time they need and
    end early. But on the other hand, the scheduler does not use a "first come, first serve" policy
    so another user may submit a job that gets a higher priority than yours, pushing back the start
    time of your job. So it is basically a random number generator.</p>
</li>
</ul>
<p>The <code>sprio</code> command will list the different elements that determine the priority of your job but
is basically a command for system administrators as users cannot influence those numbers nor do 
they say a lot unless you understand all intricacies of the job policies chosen by the site,
and those policies may be fine-tuned over time to optimise the operation of the cluster.
The fairshare parameter influences the priority of jobs depending on how much users or projects
(this is not clear to us at the moment) have been running jobs in the past few days and is a
very dangerous parameter on a supercomputer where the largest project is over 1000 times the size
of the smallest projects, as treating all projects equally for the fair share would make it impossible
for big projects to consume all their CPU time.</p>
<p>Another concept of the scheduler on LUMI is <strong>backfill</strong>. On a system supporting very large jobs as LUMI,
the scheduler will often be collecting nodes to run those large jobs, and this may take a while,
particularly since the maximal wall time for a job in the standard partitions is rather large
for such a system. If you need one quarter of the nodes for a big job on a partition on which most 
users would launch jobs that use the full two days of walltime, one can expect that it takes half
a day to gather those nodes. However, the LUMI scheduler will schedule short jobs even though they have a lower
priority on the nodes already collected if it expects that those jobs will be finisehd before it expects
to have all nodes for the big job. This mechanism is called backfill and is the reason why
short experiments of half an hour or so often start quickly on LUMI even though the queue is very long.</p>
<h2 id="accounting-of-jobs">Accounting of jobs<a class="headerlink" href="#accounting-of-jobs" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Accounting of jobs" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/Accounting.png" /></p>
</figure>
<div class="admonition remark">
<p class="admonition-title">Billing of jobs</p>
<p>Jobs are billed against your allocation, so how does this work?</p>
</div>
<p>The use of resources by a job is billed to projects, not users. All management is also
done at the project level, not at the "user-in-a-project" level.
As users can have multiple projects, the system cannot know to which project a job
should be billed, so it is mandatory to specify a project account (of the form
<code>project_46YXXXXXX</code>) with every command that creates an allocation.</p>
<p>Billing on LUMI is not based on which resources you effectively use, but on the
amount of resources that others cannot use well because of your allocation. 
This assumes that you make proportional use of CPU cores, CPU memory and GPUs (actually GCDs).
If you job makes a disproportionally high use of one of those resources, you will be billed
based on that use. For the CPU nodes, the billing is based on both the number of cores you
request in your allocation and the amount of memory compared to the amount of memory per core
in a regular node, and the highest of the two numbers is used. For the GPU nodes, the formula
looks at the number of cores compared to he number of cores per GPU, the amount of CPU memory compared 
to the amount of memory per GCD (so 64 GB), and the amount of GPUs and the highest amount
determines for how many GCDs you will be billed (with a cost of 0.5 GPU-hour per hour per GCD).
For jobs in job-exclusive partitions you are automatically billed for the full node as no other
job can use that node, so 128 core-hours per hour for the standard partition or
4 GPU-hours per hour for the standard-g partition.</p>
<p>E.g., if you would ask for only one core but 128 GB of memory, half of what a regular LUMI-C node has,
you'd be billed for the use of 64 cores. Or assume you want to use only one GCD but want to use 16 cores
and 256 GB of system RAM with it, then you would be billed for 4 GPUs/GCDs: 256 GB of memory makes it impossible
for other users to use 4 GPUs/GCDs in the system, and 16 cores make it impossible to use 2 GPUs/GCDs,
so the highest number of those is 4, which means that you will pay 2 GPU-hours per hour that you use the
allocation (as GPU-hours are based on a full MI250x and not on a GCD which is the GPU for Slurm).</p>
<div class="admonition remark">
<p class="admonition-title">This billing policy is unreasonable!</p>
<p>Users who have no experience with performance optimisation may think this way of
billing is unfair. After all, there may be users who need far less than 2 GB of memory
per core so they could still use the other cores on a node where I am using only
one core but 128 GB of memory, right? Well, no, and this has everything to do with
the very hierarchical nature of a modern compute node, with on LUMI-C 2 sockets,
4 NUMA domains per socket, and 2 L3 cache domains per NUMA domain. Assuming your
job would get the first core on the first socket (called core 0 and socket 0 as
computers tend to number from 0). Linux will then allocate the memory of the job
as close as possible to that core, so it will fill up the 4 NUMA domains of that
socket. It can migrate unused memory to the other socket, but let's assume your 
code does not only need 128 GB but also accesses bits and pieces from it everywhere
all the time. Another application running on socket 0 may then get part or all
of its memory on socket 1, and the latency to access that memory is more than 
3 times higher, so performance of that application will suffer. In other words,
the other cores in socket 0 cannot be used with full efficiency.</p>
<p>This is not a hypothetical scenario. The author of this text has seem benchmarks
run on one of the largest systems in Flanders that didn't scale at all and for
some core configuration ran at only 10% of the speed they should have been
running at...</p>
<p>Still, even with this billing policy Slurm oon LUMI is a far from perfect scheduler
and core, GPU and memory allocation on the non-exclusive partitions are far from
optimal. Which is why we spend a section of the course on binding applications
to resources.</p>
</div>
<p>The billing is done in a postprocessing step in the system based on data from the Slurm 
job database, but the Slurm accounting features do not produce the correct numbers. 
E.g., Slurm counts the core hours based on the virtual cores so the numbers are double
of what they should be. There are two ways to check the state of an allocation, though
both work with some delay.</p>
<ul>
<li>
<p>The <code>lumi-workspaces</code> and <code>lumi-allocations</code> commands show the total amount of 
    billing units consumed. In regular operation of the system these numbers are updated
    approximately once an hour.</p>
<p><code>lumi-workspaces</code> is the all-in command that intends to show all information that is 
useful to a regular user, while <code>lumi-allocations</code> is a specialised tool that only
shows billing units, but he numbers shown by both tools come from the same database
and are identical.</p>
</li>
<li>
<p>For projects managed via Puhuri, Puhuri can show billing unit use per month, but the
    delay is larger than with the <code>lumi-workspaces</code> command.</p>
</li>
</ul>
<div class="admonition remark">
<p class="admonition-title">Billing unit use per user in a project</p>
<p>The current project management system in LUMI cannot show the use of billing units
per person within a project.</p>
<p>For storage quota this would be very expensive to organise as quota are managed
by Lustre on a group basis. </p>
<p>For CPU and GPU billing units it would in principle be possible as the Slurm
database contains the necessary information, but there are no plans to implement
such a feature. It is assumed that every PI makes sure that members of their 
projects use LUMI in a responsible way and ensures that they have sufficient 
experience to realise what they are doing.</p>
</div>
<h2 id="managing-slurm-jobs">Managing Slurm jobs<a class="headerlink" href="#managing-slurm-jobs" title="Permanent link">&para;</a></h2>
<p>Before experimenting with jobs on LUMI, it is good to discuss how to manage those jobs.
We will not discuss the commands in detail and instead refer to the pretty decent manual pages
that in fact can also be found on the web.</p>
<p>Remember that each job is identified by a unique job ID, a number that will be shown when you
submit a job and is also shown in the output of <code>squeue</code>.  This job ID will be used to
manage jobs.</p>
<ul>
<li>
<p>To delete a job, use <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/scancel.html"><code>scancel &lt;jobID&gt;</code></a></p>
</li>
<li>
<p>An important command to manage jobs while they are running is 
    <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/sstat.html"><code>sstat -j &lt;jobID&gt;</code></a>.
    This command display real-time information directly gathered from the resource manager
    component of Slurm and can also be used to show information about individual job steps using
    the job step identifier (which is in most case <code>&lt;jobID&gt;.0</code> for the first rebular job step and so on).
    The <code>sstat</code> command can display a lot more information than is shown by default. The output can
    be adapted via the <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/sstat.html#OPT_format"><code>--format</code> or <code>-o</code> command line option</a>
    with a list of options in the <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/sstat.html#SECTION_Job-Status-Fields">"Job status fields" section of the manual page</a>.</p>
</li>
<li>
<p>The <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/sacct.html"><code>sacct -j &lt;jobID&gt;</code> command</a> can be used both while the
    job is running and when the job has finished. It is the main command to get information about a job
    after the job has finished. All information comes from a database, also while the job is running, so 
    the information is available with some delay compared to the information obtained with <code>sstat</code> for
    a running job. It will also produce information about individual job steps. Just as with <code>sstat</code>, the
    fields to display can be selected via the
    <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/sacct.html#OPT_format"><code>--format</code> or <code>-o</code> command line option</a> with an even
    longer list in the <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/sacct.html#SECTION_Job-Accounting-Fields">"Job accounting fields" section of the manual page</a>.</p>
</li>
</ul>
<p>The <code>sacct</code> command will also be used in various examples in this section of the tutorial to investigate
the behaviour of Slurm.</p>
<h2 id="creating-a-slurm-job">Creating a Slurm job<a class="headerlink" href="#creating-a-slurm-job" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Creating a Slurm job" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/CreateJob.png" /></p>
</figure>
<p>Slurm has three main commands to create jobs and job steps. 
Remember that a job is just a request for an allocation. Your applications always have to
run inside a job step.</p>
<p>The <code>salloc</code> command only creates an allocation but does not create a job step.
<strong>The behaviour of <code>salloc</code> differs between clusters!</strong> 
On LUMI, <code>salloc</code> will put you in a new shell on the node from which you issued
the <code>salloc</code> command, typically the login node. Your allocation will exist until
you exit that shell with the <code>exit</code> command or with the CONTROL-D key combination.
Creating an allocation with <code>salloc</code> is good for interactive work.</p>
<div class="admonition remark">
<p class="admonition-title">Differences in <code>salloc</code> behaviour.</p>
<p>On some systems <code>salloc</code> does not only create a job allocation but will
also create a job step, the so-called "interactive job step" on a node of
the allocation, similar to the way that the <code>sbatch</code> command discussed later
will create a so-called "batch job step".</p>
</div>
<p>The main purpose of the <code>srun</code> command is to create a job step in an allocation.
When run outside of a job (outside an allocation) it will also create a job allocation.
However, be careful when using this command to also create the job in which the job step
will run as some options work differently as for the commands meant to create an allocation.
When creating a job with <code>salloc</code> you will have to use <code>srun</code> to start anything on the
node(s) in the allocation as it is not possible to, e.g., reach the nodes with <code>ssh</code>.</p>
<p>The <code>sbatch</code> command both creates a job and then start a job step, teh so-called batch
job step, to run the job script on the first node of the job allocation.
In principle it is possible to start both sequential and shared memory processes
directly in the batch job step without creating a new job step with <code>srun</code>, 
but keep in mind that the resources may be different from what you expect to see
in some cases as some of the options given with the <code>sbatch</code> command will only be
enforced when starting another job step from the batch job step. To run any
multi-process job (e.g., MPI) you will have to use <code>srun</code> or a process starter that
internally calls <code>srun</code> to start the job.
<strong>When using Cray MPICH as the MPI implementation (and it is the only one that is fully
supported on LUMI) you will have to use <code>srun</code> as the process starter.</strong></p>
<h2 id="passing-options-to-srun-salloc-and-sbatch">Passing options to srun, salloc and sbatch<a class="headerlink" href="#passing-options-to-srun-salloc-and-sbatch" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Passing options to srun, salloc and sbatch" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PassingOptions.png" /></p>
</figure>
<p>There are several ways to pass options and flags to the <code>srun</code>, <code>salloc</code> and <code>sbatch</code> command.</p>
<p>The lowest priority way and only for the <code>sbatch</code> command is specifying the options (mostly resource-related)
in the batch script itself on <code>#SBATCH</code> lines. These lines should not be interrupted by commands, and it is
not possible to use environment variables to specify values of options. </p>
<p>Higher in priority is specifying options and flags through environment variables. 
For the <code>sbatch</code> command this are the <code>SBATCH_*</code> environment variables, for <code>salloc</code>
the <code>SALLOC_*</code> environment variables and for <code>srun</code> the <code>SLURM_*</code> and some <code>SRUN_*</code> environment variables.
For the <code>sbatch</code> command this will overwrite values on the <code>#SBATCH</code> lines. You can find
lists in the manual pages of the 
<a href="https://slurm.schedmd.com/archive/slurm-22.05.8/sbatch.html"><code>sbatch</code></a>,
<a href="https://slurm.schedmd.com/archive/slurm-22.05.8/salloc.html"> <code>salloc</code></a> and
<a href="https://slurm.schedmd.com/archive/slurm-22.05.8/srun.html"><code>srun</code></a> command.
Specifying command line options via environment variables that are hidden in your
<code>.profile</code> or <code>.bashrc</code> file or any script that you run before starting your work,
is not free of risks. Users often forget that they set those environment variables and
are then surprised that the Slurm commands act differently then expected. E.g., it
is very tempting to set the project account to use in environment variables but if you 
then get a second project you may be running inadvertently in the wrong project.</p>
<p>The highest priority is for flags and options given on the command line. The position of 
those options is important though. With the <code>sbatch</code> command they have to be specified before
the batch script as otherwise they will be passed to the batch script as command line options for 
that script. Likewise, with <code>srun</code> they have to be specified before the command you want to execute
as otherwise they would be passed to that command as flags and options.</p>
<p>Several options specified to <code>sbatch</code> or <code>salloc</code> are also forwarded to <code>srun</code> via <code>SLURM_*</code> environment
variables set in the job by these commands.</p>
<h2 id="specifying-options">Specifying options<a class="headerlink" href="#specifying-options" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Specifying options" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/SpecifyingOptions.png" /></p>
</figure>
<p>Slurm commands have way more options and flags than we can discuss in this course or even the
4-day comprehensive course organised by the LUMI User Support Team. Moreover, if and how they work
may depend on the specific configuration of Slurm. Slurm has so many options that no two clusters
are the same. </p>
<p>Slurm command can exist in two variants:</p>
<ol>
<li>
<p>The long variant, with a double dash, is of the form <code>--long-option=&lt;value&gt;</code> or 
    <code>--long-option &lt;value&gt;</code></p>
</li>
<li>
<p>But many popular commands also have a single letter variant, with a single dash:
    <code>-S &lt;value&gt;</code> or <code>-S&lt;value&gt;</code></p>
</li>
</ol>
<p>This is no different from many popular Linux commands.</p>
<p>Slurm commands for creating allocations and job steps have many different flags for specifying
the allocation and the organisation of tasks in that allocation. Not all combinations are valid,
and it is not possible to sum up all possible configurations for all possible scenarios. Use 
common sense and if something does not work, check the manual page and try something different.
Overspecifying options is not a good idea as you may very well create conflicts, and we will see
some examples in this section and the next section on binding. However, underspecifying is not
a good idea either as some defaults may be used you didn't think of. Some combinations also just 
don't make sense, and we will warn for some on the following slides and try to bring some 
structure in the wealth of options.</p>
<!-- 
E.g., if you are running in "allocatable by resource" partitions you don't 
always know which cores on a node you will get so using options that specify specific cores for
specific tasks will result in error messages. If you want full nodes it may just be better to use
the standard and standard-g partitions unless you need one of the CPU nodes with more memory per 
node. Also, keep in mind that on "allocatable by resource" partitions Slurm already needs to
know the structure of the tasks (cores per task, GPUs per task) to be able to create a proper 
allocation as tasks and cores may be spread out within a node or across multiple nodes of LUMI.
Obviously a task in a job step needs all its resources (cores, memory and GPUs) in the same 
node, and if multiple tasks are sharing GPUs then obviously these tasks must also be on the
same node.
-->

<h2 id="some-common-options-to-all-partitions">Some common options to all partitions<a class="headerlink" href="#some-common-options-to-all-partitions" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Some common options to all partitions" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/SpecifyingCommonOptions.png" /></p>
</figure>
<p>For CPU and GPU requests, a different strategy should be used for "allocatable by node" and "allocatable by resource" partitions,
and this will be discussed later. A number of options however are common to both strategies and will be discussed first.
All are typically used on <code>#SBATCH</code> lines in job scripts, but can also be used on the command line and the first three are
certainly needed with <code>salloc</code> also.</p>
<ul>
<li>
<p>Specify the account to which the job should be billed with <code>--account=project_46YXXXXXX</code> or <code>-A project_46YXXXXXX</code>.
    This is mandatory; without this your job will not run.</p>
</li>
<li>
<p>Specify the partition: <code>--partition=&lt;partition&gt;  or</code>-p <partition>`. This option is also necessary
    on LUMI as there is currently no default partition.</p>
</li>
<li>
<p>Specify the wall time for the job: <code>--time=&lt;timespec&gt;</code> or <code>-t &lt;timespec&gt;</code>. There are multiple formats for
    the time specifications, but the most common ones are minutes (one number), minutes:seconds (two numbers separated
    by a colon) and hours:minutes:seconds (three numbers separated by a column). If not specified, the partition-dependent
    default time is used.</p>
<p>It does make sense to make a reasonable estimate for the wall time needed. It does protect you a bit in case
your application hangs for some reason, and short jobs that also don't need too many nodes have a high chance of
running quicker as they can be used as backfill while the scheduler is gathering nodes for a big job.</p>
</li>
<li>
<p>Completely optional: Specify a name for the job with <code>--job-name=&lt;name&gt;</code> or <code>-J &lt;name&gt;</code>. Short but clear
    job names help to make the output of <code>squeue</code> easier to interpret, and the name can be used to generate 
    a name for the output file that captures output to stdout and stderr also.</p>
</li>
<li>
<p>For courses or other special opportunities such as the "hero runs" (a system for projects that want to test
    extreme scalability beyond the limits of the regular partitions), reservations are used. You can specify the
    reservation (or even multiple reservations as a comma-separated list) with <code>--reservation=&lt;name&gt;</code>.</p>
<p>In principle no reservations are given to regular users for regular work as this is unfair to other users. It would
not be possible to do all work in reservations and bypass the scheduler as the scheduling would be extremely
complicated and the administration enormous. And empty reservations do not lead to efficient machine use.
Schedulers have been developed for a reason.</p>
</li>
<li>
<p>Slurm also has options to send mail to a given address when a job starts or ends or some other job-related
    events occur, but this is currently not configured on LUMI.</p>
</li>
</ul>
<h2 id="redirecting-output">Redirecting output<a class="headerlink" href="#redirecting-output" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Redirecting output" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/SpecifyingOutput.png" /></p>
</figure>
<p>Slurm has two options to redirect stdout and stderr respectively: <code>--output=&lt;template&gt;</code> or <code>-o &lt;template&gt;</code> for stdout
and <code>--error=&lt;template&gt;</code> or <code>-e &lt;template&gt;</code> for stderr. They work together in the following way:</p>
<ul>
<li>
<p>If neither <code>--output</code> not <code>--error</code> is specified, then stdout and stderr are merged and redirected to the file <code>slurm-&lt;jobid&gt;.out</code>.</p>
</li>
<li>
<p>If <code>--output</code> is specified but <code>--error</code> is not, then stdout and stderr are merged and redirected to the file given with <code>--output</code>.</p>
</li>
<li>
<p>If <code>--output</code> is not specified but <code>--error</code>, then stdout will still be redirected to <code>slurm-&lt;jobid&gt;.out</code>, but
    stderr will be redirected to the file indicated by the <code>--error</code> option.</p>
</li>
<li>
<p>If both <code>--output</code> and <code>--error</code> are specified, then stdout is redirected to the file given by <code>--output</code> and
    stderr is redirected to the file given by <code>--error</code>.</p>
</li>
</ul>
<p>It is possible to insert codes in the filename that will be replaced at runtime with the corresponding Slurm 
information. Examples are <code>%x</code> which will be replaced with the name of the job (that you can then best set with
<code>--job-name</code>) and `%j`` which will be replaced with the job ID (job number). It is recommended to always include 
the latter in the template for the filename as this ensures unique names if the same job script would be run a 
few times with different input files. Discussing all patterns that can be used for the filename is outside the
scope of this tutorial, but you can find them all in the <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/sbatch.html">sbatch manual page</a>
in the <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/sbatch.html#SECTION_%3CB%3Efilename-pattern%3C/B%3E">"filename pattern" section</a>.</p>
<h2 id="requesting-resources-cpus-and-gpus">Requesting resources: CPUs and GPUs<a class="headerlink" href="#requesting-resources-cpus-and-gpus" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Requesting resources: CPUs and GPUs" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/RequestingCPUsGPUs.png" /></p>
</figure>
<p>Slurm is very flexible in the way resources can be requested. Covering every scenario and every possible
way to request CPUs and GPUs is impossible, so we will present a scheme that works for most users and jobs.</p>
<p>First, you have to distinguish between two strategies for requesting resources, each with their own
pros and cons. We'll call them "per-node allocations" and "per-core allocations":</p>
<ol>
<li>
<p><strong>"Per-node allocations":</strong> Request suitable nodes (number of nodes and partition) with <code>sbatch</code> or <code>salloc</code>
    but postpone specifying the full structure of the job step (i.e., tasks, cpus per task, gpus per task, ...)
    until you actually start the job step with <code>srun</code>.</p>
<p>This strategy relies on job-exclusive nodes, so works on the <code>standard</code> and <code>standard-g</code> partitions that 
are "allocatable-by-node" partitions, but can
be used on the "allocatable-by-resource" partitions also it the <code>--exclusive</code> flag is used 
with <code>sbatch</code> or <code>salloc</code> (on the command line or
with and <code>#SBATCH --exclusive</code> line for <code>sbatch</code>).</p>
<p>This strategy gives you the ultimate flexibility in the job as you can run multiple job steps with a different 
structure in the same job rather than having to submit multiple jobs with job dependencies to ensure that they
are started in the proper order. E.g., you could first have an initialisation step that generates input files in
a multi-threaded shared memory program and then run a pure MPI job with a single-threaded process per rank. </p>
<p>This strategy also gives you full control over how the application is mapped onto the available hardware:
mapping of MPI ranks across nodes and within nodes, binding of threads to cores, and binding of GPUs to
MPI ranks. This will be the topic of the next section of the course and is for some applications very important
to get optimal performance on modern supercomputer nodes that have a strongly hierarchical architecture
(which in fact is not only the case for AMD processors, but will likely be an issue on some Intel Sapphire
Rapids processors also).</p>
<p>The downside is that allocations and hence billing is always per full node, so if you need only half a node 
you waste a lot of billing units. It shows that to exploit the full power of a supercomputer you really need
to have problems and applications that can at least exploit a full node.</p>
</li>
<li>
<p><strong>"Per-core allocations":</strong> Specify the full job step structure when creating the job allocation and optionally
    limit the choice fo Slurm for the resource allocation by specifying a number of nodes
    that should be used. </p>
<p>The problem is that Slurm cannot create a correct allocation on an "allocatable by resource" partition if it would
only know the total number of CPUs and total number of GPUs that you need. Slurm does not automatically allocate the
resources on the minimal number of nodes (and even then there could be problems) and cannot know how you intend to use
the resources to ensure that the resources are actually useful for you job. E.g., if you ask for 16 cores and Slurm would
spread them over two or more nodes, then they would not be useful to run a shared memory program as such a program cannot 
span nodes. Or if you really want to run an MPI application that needs 4 ranks and 4 cores per rank, then those cores
must be assigned in groups of 4 within nodes as an MPI rank cannot span nodes. The same holds for GPUs. If you would 
ask for 16 cores and 4 GPUs you may still be using them in different ways. Most users will probably intend to start an
MPI program with 4 ranks that each use 4 cores and one GPU, and in that case the allocation should be done in groups 
that each contain 4 cores and 1 GPU but can be spread over up to 4 nodes, but you may as well intend to run 
a 16-thread shared memory application that also needs 4 GPUs. </p>
<p>The upside of this is that with this strategy you will only get what you really need when used in an
"allocatable-by-resources" partition, so 
if you don't need a full node, you won't be billed for a full node (assuming of course that you
don't request that much memory that you basically need a full node's memory). </p>
<p>One downside is that you are now somewhat bound to the job structure. You can run job steps with a different structure,
but they may produce a warning or may not run at all if the job step cannot be mapped on the resources allocated to 
the job.</p>
<p>More importantly, most options to do binding (See the next session) cannot be used or don't make sense anyway as there
is no guarantee your cores will be allocated in a dense configuration.</p>
<p>However, if you can live with those restrictions and if your job size falls within the limits of the "allocatable per 
resource" partitions, and cannot fill up the minimal number of nodes that would be used, then this strategy ensures
you're only billed for the minimal amount of resources that are made unavailable by your job.</p>
</li>
</ol>
<p>This choice is something you need to think about in advance and there are no easy guidelines. Simply say "use the first 
strategy if your job fills whole nodes anyway and the second one otherwise unless you'd need more than 4 nodes" doesn't
make sense as your job may be so sensitive to its mapping to resources that it could perform very badly in the second case.
The real problem is that there is no good way in Slurm to ask for a number of L3 cache domains (CPU chiplets), a number
of NUMA domains or a number of sockets and also no easy way to always do the proper binding if you would get resources
that way (but that is something that can only be understood after the next session). If a single job needs only a half 
node and if all jobs take about the same time anyway, it might be better to bundle them by hand in jobs and do a proper
mapping of each subjob on the available resources (e.g., in case of two jobs on a CPU node, map each on a socket).</p>
<h2 id="resources-for-per-node-allocations">Resources for per-node allocations<a class="headerlink" href="#resources-for-per-node-allocations" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per-node allocations" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerNode.png" /></p>
</figure>
<p>In a per-node allocation, all you need to specify is the partition and the number of nodes needed, and in some cases,
the amount of memory. In this scenario, one should use those Slurm options that specify resources per node
also.</p>
<p>The partition is specified using <code>--partition=&lt;partition</code> or <code>-p &lt;partition&gt;</code>.</p>
<p>The number of nodes is specified with <code>--nodes=&lt;number_of_nodes&gt;</code> or its short form 
<code>-N &lt;number_of_nodes&gt;</code>.</p>
<p>IF you want to use a per-node allocation on a partition which is allocatable-by-resources such as small
and small-g, you also need to specify the <code>--exclusive</code> flag. On LUMI this flag does not have the same
effect as running on a partition that is allocatable-by-node. The <code>--exclusive</code> flag does allocate
all cores and GPUs on the node to your job, but the memory use is still limited by other parameters in
the Slurm configuration. In fact, this can also be the case for allocatable-by-node partitions, but there 
the limit is set to allow the use of all available memory. Currently the interplay between various parameters
in the Slurm configuration results in a limit of 112 GB of memory on the <code>small</code> partition and 64 GB on the
<code>standard</code> partition when running in <code>--exclusive</code> mode. It is possible to change this with the <code>--mem</code> option.</p>
<!-- 

!!! technical "Where do these memory limits come from?"
    Checking the limits:

    -   On `small` the amount of memory available is the result of the `DefMemPerCPU` parameter in the
        Slurm config file. (Check with `scontrol show config | grep DefMemPerCPU`).
        Multiplied with 128, the number of cores, this gives 112 GB.

        This takes precedence over the `DefMemPerNode=UNLIMITED` that you see in the settings of the
        partition (`scontrol show partition small`).

    -   On `small-g` the amount of memory is restricted by the `DefMemPerNode` parameter in the
        settings of the partition ()`scontrol show partition small-g`). The fact that a concrete value is
        given seems to overwrite the effect of `DefMemPerCPU` in the Slurm config.

    To check the amount of memory available on a node to the Slurm job, one can check the CPUset
    that Slurm creates for the job:

    <div class="highlight"><pre><span></span><code>cat /sys/fs/cgroup/system.slice/slurmstepd.scope/job_$SLURM_JOB_ID/memory.max
</code></pre></div>

    gives the amount of memory in bytes. This was used to verify the numbers obtained above.
-->

<p>You can request all memory on a node by using <code>--mem=0</code>. This is currently the default behaviour on nodes in
the <code>standard</code> and <code>standard-g</code> partition so not really needed there. It is needed on all of the partitions
that are allocatable-by-resource.</p>
<p>We've experienced that it may be a better option to actually specify the maximum amount of useable memory on
a node which is the memory capacity of the node you want minus 32 GB, so you can use
<code>--mem=224G</code> for a regular CPU node or <code>--mem=480G</code> for a GPU node. In the past we have had memory leaks on
compute nodes that were not detected by the node health checks, resulting in users getting nodes with less
available memory than expected, but specifying these amounts protected them against getting such nodes.
(And similarly you could use <code>--mem=480G</code> and <code>--mem=992G</code> for the 512 GB and 1 TB compute nodes in the small 
partition, but note that running on these nodes is expensive!)</p>
<details class="example">
<summary>Example jobscript (click to expand)</summary>
<p>The following job script runs a shared memory program in the batch job step, which shows that
it has access to all hardware threads and all GPUs in a node at that moment:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#! /usr/bin/bash</span>
<span class="c1">#SBATCH --job-name=slurm-perNode-minimal-small-g</span>
<span class="c1">#SBATCH --partition=small-g</span>
<span class="c1">#SBATCH --exclusive</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --mem=480G</span>
<span class="c1">#SBATCH --time=2:00</span>
<span class="c1">#SBATCH --output=%x-%j.txt</span>
<span class="c1">#SBATCH --account=project_46YXXXXXX</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/23.09<span class="w"> </span>partition/G<span class="w"> </span>lumi-CPEtools/1.1-cpeCray-23.09

gpu_check

sleep<span class="w"> </span><span class="m">2</span>
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nsacct for the job:\n</span><span class="k">$(</span>sacct<span class="w"> </span>-j<span class="w"> </span><span class="nv">$SLURM_JOB_ID</span><span class="k">)</span><span class="s2">\n&quot;</span>
</code></pre></div>
<p>As we are using small-g here instead of standard-g, we added the <code>#SBATCH --exclusive</code> and <code>#SBATCH --mem=480G</code> lines.</p>
<p>A similar job script for a CPU-node in LUMI-C and now in the standard partition would look like:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#! /usr/bin/bash</span>
<span class="c1">#SBATCH --job-name=slurm-perNode-minimal-standard</span>
<span class="c1">#SBATCH --partition=standard</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --time=2:00</span>
<span class="c1">#SBATCH --output=%x-%j.txt</span>
<span class="c1">#SBATCH --account=project_46YXXXXXX</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/23.09<span class="w"> </span>partition/C<span class="w"> </span>lumi-CPEtools/1.1-cpeCray-23.09

omp_check

sleep<span class="w"> </span><span class="m">2</span>
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nsacct for the job:\n</span><span class="k">$(</span>sacct<span class="w"> </span>-j<span class="w"> </span><span class="nv">$SLURM_JOB_ID</span><span class="k">)</span><span class="s2">\n&quot;</span>
</code></pre></div>
<p><code>gpu_check</code> and <code>omp_check</code> are two programs provided by the <code>lumi-CPEtools</code> modules to check
the allocations. Try <code>man lumi-CPEtools</code> after loading the module. The programs will be used
extensively in the next section on binding also, and are written to check how your program
would behave in the allocation without burning through tons of billing units.</p>
</details>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per-node allocations: CPUs" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerNodeCPU.png" /></p>
</figure>
<p>By default you will get all the CPUs in each node that is allocated in a per-node allocation.
The Slurm options to request CPUs on a per-node basis are not really useful on LUMI, but might be on clusters
with multiple node types in a single partition as they enable you to specify the minimum number of
sockets, cores and hardware threads a node should have.</p>
<p><strong>We advise against using the options to request CPUs on LUMI</strong> 
because it is more likely to cause problems due to user
error than to solve problems. Some of these options also conflict with options that will be used
later in the course.</p>
<p>There is no direct way to specify the number of cores per node. Instead one has to specify the number
sockets and then the number of cores per socket and one can specify even the number of hardware threads
per core, though we will favour another mechanism later in these course notes.</p>
<p>The two options are:</p>
<ol>
<li>
<p>Specify <code>--sockets-per-node=&lt;sockets</code> and <code>--cores-per-socket=&lt;cores&gt;</code> and maybe even <code>--threads-per-core=&lt;threads&gt;</code>.
    For LUMI-C the maximal specification is </p>
<div class="highlight"><pre><span></span><code>--sockets-per-node=2 --cores-per-socket-64
</code></pre></div>
<p>and for LUMI-G</p>
<div class="highlight"><pre><span></span><code>--sockets-per-node=1 --cores-per-socket=56
</code></pre></div>
<p>Note that on LUMI-G, nodes have 64 cores but one core is reserved for the operating system and drivers to 
reduce OS jitter that limits the scalability of large jobs. Requesting 64 cores will lead to error messages
or jobs getting stuck.</p>
</li>
<li>
<p>There is a shorthand for those parameters: <code>--extra-node-info=&lt;sockets&gt;[:cores]</code> or
    <code>-B --extra-node-info=&lt;sockets&gt;[:cores]</code> where the second and third number are optional.
    The full maximal specification for LUMI-C would be <code>--extra-node-info=2:64</code> and for LUMI-G
    <code>--extra-node-info=1:56</code>.</p>
</li>
</ol>
<details class="intermediate">
<summary>What about <code>--threads-per-core</code>?</summary>
<p>Slurm also has a <code>--threads-per-core</code> (or a third number with <code>--extra-node-info</code>)
which is a somewhat misleading name. On LUMI, as hardware threads 
are turned on, you would expect that you can use <code>--threads-per-core=2</code> but if you try, you will see
that your job is not accepted. This because on LUMI, the smallest allocatable processor resource 
(called the CPU in Slurm) is a core and not a hardware thread (or virtual core as they are also 
called). There is another mechanism to enable or disable hyperthreading in regular job steps that we will
discuss later.</p>
</details>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per-node allocations: GPUs" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerNodeGPU.png" /></p>
</figure>
<p>By default you will get all the GPUs in each node that is allocated in a per-node allocation. The Slurm options
to request GPUs on a per-node basis are not really useful on LUMI, but might be on clusters with multiple
types of GPUs in a single partition as they enable you to specify which type of node you want.
If you insist, slurm has several options to specify the number of GPUs for this scenario:</p>
<ol>
<li>
<p>The most logical one to use for a per-node allocation is <code>--gpus-per-node=8</code> to request 8 GPUs per node.
    You can use a lower value, but this doesn't make much sense as you will be billed for the full node anyway.</p>
<p>It also has an option to also specify the type of the GPU but that doesn't really make sense on LUMI. 
On LUMI, you could in principle use <code>--gpus-per-node=mi250:8</code>.</p>
</li>
<li>
<p><code>--gpus=&lt;number&gt;</code> or <code>-G &lt;number&gt;</code> specifies the total number of GPUs needed for the job. In our opinion
    this is a dangerous option to use as when you change the number of nodes, you likely also want to change
    the number of GPUs for the job and you may overlook this. Here again it is possible to specify the type of
    the GPU also. Moreover, if you ask for fewer GPUs than are present in the total number of nodes you request,
    you may get a very strange distribution of the available GPUs across the nodes.</p>
<details class="example">
<summary>Example of an unexpected allocation</summary>
<p>Assuming <code>SLURM_ACCOUNT</code> is set to a valid project with access to the partition used: </p>
<div class="highlight"><pre><span></span><code>module load LUMI/23.09 partition/G lumi-CPEtools
srun --partition standard-g --time 5:00 --nodes 2 --tasks-per-node 1 --gpus 8 gpu_check
</code></pre></div>
<p>returns</p>
<div class="highlight"><pre><span></span><code>MPI 000 - OMP 000 - HWT 001 - Node nid007264 - RT_GPU_ID 0,1,2,3,4,5,6 - GPU_ID 0,1,2,3,4,5,6 - Bus_ID c1,c9,ce,d1,d6,d9,dc
MPI 001 - OMP 000 - HWT 001 - Node nid007265 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1
</code></pre></div>
<p>So 7 GPUs were allocated on the first node and 1 on the second.</p>
</details>
</li>
<li>
<p>A GPU belongs to the family of "generic consumable resources" (or GRES) in Slurm and there is an option to request
    any type of GRES that can also be used. Now you also need to specify the type of the GRES. The number you 
    have to specify if on a per-node basis, so on LUMI you can use  <code>--gres=gpu:8</code> or <code>--gres=gpu:mi250:8</code>.</p>
</li>
</ol>
<p>As these options are also forwarded to <code>srun</code>, it will save you from specifying them there.</p>
<h2 id="per-node-allocations-starting-a-job-step">Per-node allocations: Starting a job step<a class="headerlink" href="#per-node-allocations-starting-a-job-step" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per-node allocations: Starting a job step" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerNodeJobStep_1.png" /></p>
</figure>
<p>Serial or shared-memory multithreaded programs in a batch script can in principle be run in 
the batch job step. As we shall see though the effect may be different from what you expect. 
However, if you are working interactively via <code>salloc</code> you are in a shell on the node on which
you called <code>salloc</code>, typically a login node, and to run anything on the compute nodes you 
will have to start a job step.</p>
<p>The command to start a new job step is <code>srun</code>. But it needs a number of arguments in most
cases. After all, a job step consists of a number of equal-sized tasks (considering only
homogeneous job steps at the moment, the typical case for most users) that each need a number
of cores or hardware threads and, in case of GPU compute, access to a number of GPUs.</p>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per-node allocations: Starting a job step (2)" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerNodeJobStep_2.png" /></p>
</figure>
<p>There are several ways telling Slurm how many tasks should be created and what the 
resources are for each individual task, but this scheme is an easy scheme:</p>
<ol>
<li>
<p>Specifying the number of tasks: You can specify per node or the total number:</p>
<ol>
<li>
<p>Specifying the total number of tasks: 
    <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/srun.html#OPT_ntasks"><code>--ntasks=&lt;ntasks</code> or <code>-n ntasks</code></a>.
    There is a risk associated to this approach which is the same as when specifying the
    total number of GPUs for a job: IF you change the number of nodes, then you should
    change the total number of tasks also. However, it is also very useful in certain cases.
    Sometimes the number of tasks cannot be easily adapted and does not fit perfectly into
    your allocation (cannot be divided by the number of nodes). In that case, specifying the
    total number of nodes makes perfect sense.</p>
</li>
<li>
<p>Specifying on a per node basis: 
    <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/srun.html#OPT_ntasks-per-core"><code>--ntasks-per-node=&lt;ntasks&gt;</code></a> 
    is possible in combination with <code>--nodes</code> according to the Slurm manual. 
    In fact, this would be a logical thing to do in a per node allocation. 
    <strong>However, we see it fail on LUMI when it is used as an option for <code>srun</code> and not with <code>sbatch</code>, 
    even though it should work
    <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/srun.html#OPT_ntasks-per-core">according to the documentation</a>.</strong></p>
<p>The reason for the failure is that Slurm when starting a batch job defines a large number of <code>SLURM_*</code> and
<code>SRUN_*</code> variables. Some only give information about the allocation, but others are picked up by <code>srun</code> as
options and some of those options have a higher priority than <code>--ntasks-per-node</code>. So the trick is to 
unset both <code>SLURM_NTASKS</code> and <code>SLURM_NPROCS</code>. The <code>--ntasks</code> option triggered by <code>SLURM_NTASKS</code> has a higher
priority than <code>--ntasks-per-node</code>.  <code>SLURM_NPROCS</code> was used in older versions of Slurm as with the same
function as the current environment variable <code>SLURM_NTASKS</code> and therefore also implicitly specifies 
<code>--ntasks</code> if <code>SLURM_NTASKS</code> is removed from the environment.</p>
<p>The option is safe to use with <code>sbatch</code> though.</p>
</li>
</ol>
<p><strong>Lesson: If you want to play it safe and not bother with modifying the environment that Slurm creates, use
the total number of tasks <code>--ntasks</code> if you want to specify the number of tasks with <code>srun</code>.</strong></p>
</li>
<li>
<p>Specifying the number of CPUs (cores on LUMI) for each task. The easiest way to do this is by
    using <code>--cpus-per-task=&lt;number_CPUs&gt;</code> or <code>-c &lt;number_CPUs&gt;</code>.</p>
</li>
<li>
<p>Specifying the number of GPUs per task. Following the Slurm manuals, the following
    seems the easiest way:</p>
<ol>
<li>
<p>Use <code>--gpus-per-task=&gt;number_GPUs&gt;</code> to bind one or more GPUs to each task.
    This is probably the most used option in this scheme.</p>
</li>
<li>
<p>If however you want multiple tasks to share a GPU, then you should use 
    <code>--ntasks-per-gpu=&lt;number_of_tasks&gt;</code>. There are use cases where this makes sense.</p>
</li>
</ol>
<p>This however does not always work...</p>
</li>
</ol>
<p>The job steps created in this simple scheme do not always run the programs at optimal efficiency. Slurm has various
strategies to assign tasks to nodes, and there is an option which we will discuss in the next session
of the course (binding) to change that. Moreover, not all clusters use the same default setting for this
strategy. Cores and CPUs are assigned in order and this is not always the best order.</p>
<p>It is particularly difficult to get a good distribution on the GPU nodes because of the single core
reserved for low noise mode, which leaves the system in a very asymmetric state: There is one L3 cache
domain with 7 available cores (the first one) and 7 with 8 available cores. Assume you want to use one
task per GPU then there is no easy way to get each task bound to its own L3 cache domain. Any
strategy trying this with 8 cores per task will fail as there are no 64 cores available, while using
7 cores per task will set the first task on the first cache domain, the second task on the second cache 
domain, but the third task will already start on the last core of the second cache domain.
There are solutions to this problem which we will discuss in the session on binding.</p>
<p>It is also possible to specify these options already on <code>#SBATCH</code> lines. Slurm will transform those
options into <code>SLURM_*</code> environment variables that will then be picked up by <code>srun</code>. However, this 
behaviour has changed in more recent versions of Slurm. E.g., <code>--cpus-per-task</code> is no longer 
automatically picked up by <code>srun</code> as there were side effects with some MPI implementations on some
clusters. CSC has modified the configuration to again forward that option (now via an <code>SRUN_*</code> 
environment variable) but certain use cases beyond the basic one described above are not covered.
And take into account that not all cluster operators will do that as there are also good reasons not
to do so. Otherwise the developers of Slurm wouldn't have changed that behaviour in the first place.</p>
<details class="note">
<summary>Demonstrator for the problems with <code>--tasks-per-node</code> (click to expand)</summary>
<p>Try the batch script:</p>
<p><!-- slurm-perNode-jobstart-standard-demo1.slurm -->
<div class="highlight"><pre><span></span><code>#! /usr/bin/bash
#SBATCH --job-name=slurm-perNode-jobstart-standard-demo1
#SBATCH --partition=standard
#SBATCH --nodes=2
#SBATCH --time=2:00
#SBATCH --output=%x-%j.txt

module load LUMI/23.09 partition/C lumi-CPEtools/1.1-cpeCray-23.09

echo &quot;Submitted from $SLURM_SUBMIT_HOST&quot;
echo &quot;Running on $SLURM_JOB_NODELIST&quot;
echo
echo -e &quot;Job script:\n$(cat $0)\n&quot;
echo &quot;SLURM_* and SRUN_* environment variables:&quot;
env | egrep ^SLURM
env | egrep ^SRUN

set -x
# This works
srun --ntasks=32 --cpus-per-task=8 hybrid_check -r

# This does not work
srun --ntasks-per-node=16 --cpus-per-task=8 hybrid_check -r

# But this works again
unset SLURM_NTASKS
unset SLURM_NPROCS
srun --ntasks-per-node=16 --cpus-per-task=8 hybrid_check -r
set +x
echo -e &quot;\nsacct for the job:\n$(sacct -j $SLURM_JOB_ID)\n&quot;
</code></pre></div></p>
</details>
<h3 id="a-warning-for-gpu-applications">A warning for GPU applications<a class="headerlink" href="#a-warning-for-gpu-applications" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per-node allocations: A warning for GPU applications" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerNodeJobstepWarningGPU.png" /></p>
</figure>
<p>Allocating GPUs with <code>--gpus-per-task</code> or <code>--tasks-per-gpu</code> may seem the most logical thing to do
when reading the Slurm manual pages. It does come with a problem though resulting of how Slurm
currently manages the AMD GPUs, and now the discussion becomes more technical.</p>
<p>Slurm currently uses a separate control group per task for the GPUs.
Now control groups are a mechanism in Linux for restricting resources available to a process and its childdren.
Putting the GPUs in a separate control group per task limits the ways in intra-node communication can be
done between GPUs, and this in turn is incompatible with some software.</p>
<p>The solution is to ensure that all tasks within a node see all GPUs in the node and then to
manually perform the binding of each task to the GPUs it needs using a different mechanism more
like affinity masks for CPUs. It can be tricky to do though as many options for <code>srun</code> do a
mapping under the hood.</p>
<p>As we need a mechanisms that are not yet discussed yet in this chapter, we refer to the
<a href="08_Binding.md">chapter "Process and thread distribution and binding"</a> for a more ellaborate
discussion and a solution.</p>
<p>Unfortunately using AMD GPUs in Slurm is more complicated then it should be (and we will see even
more problems).</p>
<h2 id="turning-simultaneous-multithreading-on-or-off">Turning simultaneous multithreading on or off<a class="headerlink" href="#turning-simultaneous-multithreading-on-or-off" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Turning hardware threading on or off" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerNodeHardwareThreading.png" /></p>
</figure>
<p>Hardware threads are enabled by default at the operating system level. In Slurm however, regular
job steps start by default with hardware threads disabled. This is not true though for the 
batch job step as the example below will show.</p>
<p>Hardware threading for a regular job step can be turned on explicitly with
<code>--hint=multhithread</code> and turned off explicitly with <code>--hint=nomultithread</code>, 
with the latter the default on LUMI. The hint should be given as an option to
<code>sbatch</code>(e.g., as a line <code>#SBATCH --hint=multithread</code>) and not as an option of
<code>srun</code>. </p>
<p>The way it works is a bit confusing though.
We've always told, and that is also what the Slurm manual tells, that a CPU is the 
smallest allocatable unit and that on LUMI, Slurm is set to use the core as the smallest
allocatable unit. So you would expect that <code>srun --cpus-per-task=4</code> combined with <code>#SBATCH --hint=multithread</code>
would give you 4 cores with in total 8 threads, but instead you will get 2 cores with 4 hardware
threads. In other words, it looks like (at least with the settings on LUMI) <code>#SBATCH --hint=multithread</code>
changes the meaning of CPU in the context of an <code>srun</code> command to a hardware thread instead of a 
core. This is illustrated with the example below.</p>
<details class="example">
<summary>Use of <code>--hint=(no)multithread</code> (click to expand)</summary>
<p>We consider the job script </p>
<p><!-- slurm-HWT-standard-multithread.slurm -->
<div class="highlight"><pre><span></span><code><span class="ch">#! /usr/bin/bash</span>
<span class="c1">#SBATCH --job-name=slurm-HWT-standard-multithread</span>
<span class="c1">#SBATCH --partition=standard</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --hint=multithread</span>
<span class="c1">#SBATCH --time=2:00</span>
<span class="c1">#SBATCH --output=%x-%j.txt</span>
<span class="c1">#SBATCH --account=project_46YXXXXXX</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/23.09<span class="w"> </span>partition/C<span class="w"> </span>lumi-CPEtools/1.1-cpeGNU-23.09

<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;Job script:\n</span><span class="k">$(</span>cat<span class="w"> </span><span class="nv">$0</span><span class="k">)</span><span class="s2">\n&quot;</span>

<span class="nb">set</span><span class="w"> </span>-x
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>omp_check<span class="w"> </span>-r
<span class="nb">set</span><span class="w"> </span>+x
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nsacct for the job:\n</span><span class="k">$(</span>sacct<span class="w"> </span>-j<span class="w"> </span><span class="nv">$SLURM_JOB_ID</span><span class="k">)</span><span class="s2">\n&quot;</span>
</code></pre></div></p>
<p>We consider three variants of this script:</p>
<ol>
<li>
<p>Without the <code>#SBATCH --hint=multithread</code> line to see the default behaviour of Slurm on LUMI.
    The relevant lines of the output are:</p>
<div class="highlight"><pre><span></span><code>+ srun -n 1 -c 4 omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001847 mask 0-3
++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001847 mask 0-3
++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001847 mask 0-3
++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001847 mask 0-3

+ set +x

sacct for the job:
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
4238727      slurm-HWT+   standard project_4+        256    RUNNING      0:0 
4238727.bat+      batch            project_4+        256    RUNNING      0:0 
4238727.0     omp_check            project_4+          8    RUNNING      0:0 
</code></pre></div>
<p>The <code>omp_check</code> program detects that it should run 4 threads (we didn't even need to
help by setting <code>OMP_NUM_THREADS</code>) and uses cores 0 till 3 which are the first 4
physical cores on the processor.</p>
<p>The output of the <code>sacct</code> command claims that the job (which is the first line of
the table) got allocated 256 CPUs. This is a confusing feature of <code>sacct</code>: it shows 
the number of hardware threads even though the Slurm CPU on LUMI is defined as a core.
The next line shows the batch job step which actually does see all hardware threads of
all cores (and in general, all hardware threads of all allocated cores of the first node
of the job). The final line, with the '.0' job step, shows that that core was using 8
hardware threads, even though <code>omp_check</code> only saw 4. This is because the default 
behaviour (as the next test will confirm) is <code>--hint=nomultithread</code>.</p>
<p>Note that <code>sacct</code> shows the last job step as running even though it has finished. This is
because <code>sacct</code> gets the information not from the compute node but from a database, and it 
looks like the full information has not yet derived in the database. A short sleep before the
<code>sacct</code> call would cure this problem.</p>
</li>
<li>
<p>Now replace the <code>#SBATCH --hint=multithread</code>  with <code>#SBATCH --hint=nomultithread</code>.
    The relevant lines of the output are now</p>
<div class="highlight"><pre><span></span><code>+ srun -n 1 -c 4 omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001847 mask 0-3
++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001847 mask 0-3
++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001847 mask 0-3
++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001847 mask 0-3

+ set +x

sacct for the job:
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
4238730      slurm-HWT+   standard project_4+        256    RUNNING      0:0 
4238730.bat+      batch            project_4+        256    RUNNING      0:0 
4238730.0     omp_check            project_4+          8    RUNNING      0:0 
</code></pre></div>
<p>The output is no different from the previous case which confirms that this is the
default behaviour.</p>
</li>
<li>
<p>Lastly, we run the above script unmodified, i.e., with <code>#SBATCH --hint=multithread</code> 
    Now the relevant lines of the output are:</p>
<div class="highlight"><pre><span></span><code>+ srun -n 1 -c 4 omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001847 mask 0-1, 128-129
++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001847 mask 0-1, 128-129
++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001847 mask 0-1, 128-129
++ omp_check: OpenMP thread   3/4   on cpu 129/256 of nid001847 mask 0-1, 128-129

+ set +x

sacct for the job:
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
4238728      slurm-HWT+   standard project_4+        256    RUNNING      0:0 
4238728.bat+      batch            project_4+        256    RUNNING      0:0 
4238728.0     omp_check            project_4+          4  COMPLETED      0:0 
</code></pre></div>
<p>The <code>omp_check</code> program again detects only 4 threads but now runs them on the first two
physical cores and the corresponding second hardware thread for these cores. 
The output of <code>sacct</code> now shows 4 in the "AllocCPUS" command for the <code>.0</code> job step,
which confirms that indeed only 2 cores with both hardware threads were allocated instead
of 4 cores.</p>
</li>
</ol>
</details>
<details class="warning">
<summary>Buggy behaviour when used with <code>srun</code></summary>
<p>Consider the following job script:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#! /usr/bin/bash</span>
<span class="c1">#SBATCH --job-name=slurm-HWT-standard-bug2</span>
<span class="c1">#SBATCH --partition=standard</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --time=2:00</span>
<span class="c1">#SBATCH --output=%x-%j.txt</span>
<span class="c1">#SBATCH --hint=multithread</span>
<span class="c1">#SBATCH --account=project_46YXXXXXX</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/22.12<span class="w"> </span>partition/C<span class="w"> </span>lumi-CPEtools/1.1-cpeGNU-22.12

<span class="nb">set</span><span class="w"> </span>-x
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>--hint<span class="o">=</span>nomultithread<span class="w"> </span>omp_check<span class="w"> </span>-r

srun<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>--hint<span class="o">=</span>multithread<span class="w"> </span>omp_check<span class="w"> </span>-r

<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">8</span><span class="w"> </span>srun<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>--hint<span class="o">=</span>multithread<span class="w"> </span>omp_check<span class="w"> </span>-r

srun<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>omp_check<span class="w"> </span>-r
<span class="nb">set</span><span class="w"> </span>+x
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nsacct for the job:\n</span><span class="k">$(</span>sacct<span class="w"> </span>-j<span class="w"> </span><span class="nv">$SLURM_JOB_ID</span><span class="k">)</span><span class="s2">\n&quot;</span>

<span class="nb">set</span><span class="w"> </span>-x
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">256</span><span class="w"> </span>--hint<span class="o">=</span>multithread<span class="w"> </span>omp_check<span class="w"> </span>-r
</code></pre></div>
<p>The relevant lines of the output are:</p>
<div class="highlight"><pre><span></span><code>+ srun -n 1 -c 4 --hint=nomultithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001246 mask 0-3
++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001246 mask 0-3
++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001246 mask 0-3
++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001246 mask 0-3

+ srun -n 1 -c 4 --hint=multithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001246 mask 0-1, 128-129
++ omp_check: OpenMP thread   1/4   on cpu 129/256 of nid001246 mask 0-1, 128-129
++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001246 mask 0-1, 128-129
++ omp_check: OpenMP thread   3/4   on cpu   1/256 of nid001246 mask 0-1, 128-129

+ OMP_NUM_THREADS=8
+ srun -n 1 -c 4 --hint=multithread omp_check -r

Running 8 threads in a single process

++ omp_check: OpenMP thread   0/8   on cpu   0/256 of nid001246 mask 0-1, 128-129
++ omp_check: OpenMP thread   1/8   on cpu 128/256 of nid001246 mask 0-1, 128-129
++ omp_check: OpenMP thread   2/8   on cpu   0/256 of nid001246 mask 0-1, 128-129
++ omp_check: OpenMP thread   3/8   on cpu   1/256 of nid001246 mask 0-1, 128-129
++ omp_check: OpenMP thread   4/8   on cpu 129/256 of nid001246 mask 0-1, 128-129
++ omp_check: OpenMP thread   5/8   on cpu 128/256 of nid001246 mask 0-1, 128-129
++ omp_check: OpenMP thread   6/8   on cpu 129/256 of nid001246 mask 0-1, 128-129
++ omp_check: OpenMP thread   7/8   on cpu   1/256 of nid001246 mask 0-1, 128-129

+ srun -n 1 -c 4 omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001246 mask 0-3
++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001246 mask 0-3
++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001246 mask 0-3
++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001246 mask 0-3

+ set +x

sacct for the job:
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
4238801      slurm-HWT+   standard project_4+        256    RUNNING      0:0 
4238801.bat+      batch            project_4+        256    RUNNING      0:0 
4238801.0     omp_check            project_4+          8  COMPLETED      0:0 
4238801.1     omp_check            project_4+          8  COMPLETED      0:0 
4238801.2     omp_check            project_4+          8  COMPLETED      0:0 
4238801.3     omp_check            project_4+          8  COMPLETED      0:0 

+ srun -n 1 -c 256 --hint=multithread omp_check -r
srun: error: Unable to create step for job 4238919: More processors requested than permitted
</code></pre></div>
<p>The first <code>omp_check</code> runs as expected. The seocnd one uses only 2 cores but all
4 hyperthreads on those cores. This is also not unexpected. In the third case
we force the use of 8 threads, and they all land on the 4 hardware threads of
2 cores. Again, this is not unexpected. And neither is the output of the last 
run of <code>omp_cehck</code> which is again with multithreading disabled as requested in
the <code>#SBATCH</code> lines. What is surprising though is the output of <code>sacct</code>: 
It claims there were 8 hardware threads, so 4 cores, allocated to the second 
(the <code>.1</code>) and third (the <code>.2</code>) job step while whatever we tried, <code>omp_check</code>
could only see 2 cores and 4 hardware threads. Indeed, if we would try to run
with <code>-c 256</code> then <code>srun</code> will fail.</p>
<p>But now try the reverse: we turn multithreading on in the <code>#SBATCH</code> lines
and try to turn it off again with <code>srun</code>:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#! /usr/bin/bash</span>
<span class="c1">#SBATCH --job-name=slurm-HWT-standard-bug2</span>
<span class="c1">#SBATCH --partition=standard</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --time=2:00</span>
<span class="c1">#SBATCH --output=%x-%j.txt</span>
<span class="c1">#SBATCH --hint=multithread</span>
<span class="c1">#SBATCH --account=project_46YXXXXXX</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/22.12<span class="w"> </span>partition/C<span class="w"> </span>lumi-CPEtools/1.1-cpeGNU-22.12

<span class="nb">set</span><span class="w"> </span>-x
srun<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>--hint<span class="o">=</span>nomultithread<span class="w"> </span>omp_check<span class="w"> </span>-r

srun<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>--hint<span class="o">=</span>multithread<span class="w"> </span>omp_check<span class="w"> </span>-r

srun<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>omp_check<span class="w"> </span>-r
<span class="nb">set</span><span class="w"> </span>+x
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nsacct for the job:\n</span><span class="k">$(</span>sacct<span class="w"> </span>-j<span class="w"> </span><span class="nv">$SLURM_JOB_ID</span><span class="k">)</span><span class="s2">\n&quot;</span>
</code></pre></div>
<p>The relevant part of the output is now</p>
<div class="highlight"><pre><span></span><code>+ srun -n 1 -c 4 --hint=nomultithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   1/256 of nid001460 mask 0-3
++ omp_check: OpenMP thread   1/4   on cpu   2/256 of nid001460 mask 0-3
++ omp_check: OpenMP thread   2/4   on cpu   3/256 of nid001460 mask 0-3
++ omp_check: OpenMP thread   3/4   on cpu   0/256 of nid001460 mask 0-3

+ srun -n 1 -c 4 --hint=multithread omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001460 mask 0-1, 128-129
++ omp_check: OpenMP thread   1/4   on cpu 129/256 of nid001460 mask 0-1, 128-129
++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001460 mask 0-1, 128-129
++ omp_check: OpenMP thread   3/4   on cpu   1/256 of nid001460 mask 0-1, 128-129

++ srun -n 1 -c 4 omp_check -r

Running 4 threads in a single process

++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001460 mask 0-1, 128-129
++ omp_check: OpenMP thread   1/4   on cpu 129/256 of nid001460 mask 0-1, 128-129
++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001460 mask 0-1, 128-129
++ omp_check: OpenMP thread   3/4   on cpu   1/256 of nid001460 mask 0-1, 128-129

+ set +x

sacct for the job:
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
4238802      slurm-HWT+   standard project_4+        256    RUNNING      0:0 
4238802.bat+      batch            project_4+        256    RUNNING      0:0 
4238802.0     omp_check            project_4+          8  COMPLETED      0:0 
4238802.1     omp_check            project_4+          4  COMPLETED      0:0 
4238802.2     omp_check            project_4+          4  COMPLETED      0:0 
</code></pre></div>
<p>And this is fully as expected. The first <code>srun</code> does not use hardware threads
as requested by <code>srun</code>, the second run does use hardware threads and only 2 cores
which is also what we requested with the <code>srun</code> command, and the last one also uses
hardware threads. The output of <code>sacct</code> (and in particular the <code>AllocCPUS</code> comumn)
not fully confirms that indeed there were only 2 cores allocated to the second and
third run.</p>
<p>So turning hardware threads on in the <code>#SBATCH</code> lines and then off again with <code>srun</code>
works as expected, but the opposite, explicitly turning it off in the <code>#SBATCH</code> lines
(or relying on the default which is off) and then trying to turn it on again, does not
work.</p>
</details>
<h2 id="per-core-allocations">Per-core allocations<a class="headerlink" href="#per-core-allocations" title="Permanent link">&para;</a></h2>
<h3 id="when-to-use">When to use?<a class="headerlink" href="#when-to-use" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per core allocations: When to use?" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerCoreWhenUse.png" /></p>
</figure>
<p>Not all jobs can use entire nodes efficiently, and therefore the LUMI setup does provide some
partitions that enable users to define jobs that use only a part of a node. This scheme enables
the user to only request the resources that are really needed for the job (and only get billed for
those at least if they are proportional to the resources that a node provides), but also comes
with the disadvantage that it is not possible to control how cores and GPUs are allocated
within a node. Codes that depend on proper mapping of threads and processes on L3 cache domains,
NUMA domains or sockets, or on shortest paths between cores in a task and the associated GPU(s) 
may see an unpredictable performance lossas (a) the mapping will rarely be optimal unless you are
very lucky (and always be suboptimal for GPUs in the current LUMI setup) and (b) will also depend
on other jobs already running on the set of nodes assigned to your job.</p>
<p>Unfortunately, </p>
<ol>
<li>
<p>Slurm does not seem to fully understand the GPU topology on LUMI and cannot take that properly into
    account when assigning resources to a job or task in a job, and</p>
</li>
<li>
<p>Slurm does not support the hierarchy in the compute nodes of LUMI. There is no way to specifically
     request all cores in a socket, NUMA domain or L3 cache domain. It is only possible on a per-node level
     which is the case that we already discussed.</p>
</li>
</ol>
<p>Instead, you have to specify the task structure in the <code>#SBATCH</code> lines of a job script or as the command line
arguments of <code>sbatch</code> and <code>salloc</code> that you will need to run the job.</p>
<h3 id="resource-request">Resource request<a class="headerlink" href="#resource-request" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per core allocations: Resource request (1)" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerCoreResources_1.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per core allocations: Resource request (2)" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerCoreResources_2.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per core allocations: Resource request (3)" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerCoreResources_3.png" /></p>
</figure>
<p>To request an allocation, you have to specify the task structure of the job step you want to run using
mostly the same options that we have discussed on the slides "Per-node allocations: Starting a job step": </p>
<ol>
<li>
<p>Now you should specify just the total amount of tasks needed using
    <code>--ntasks=&lt;number&gt;</code> or <code>-n &lt;number&gt;</code>. As the number of nodes is not fixed
    in this allocation type, <code>--ntasks-per-node=&lt;ntasks&gt;</code> does not make much sense.</p>
<p>It is possible to request a number of nodes using <code>--nodes</code>, and it can even take
two arguments: <code>--nodes=&lt;min&gt;-&lt;max&gt;</code> to specify the minimum and maximum number of
nodes that Slurm should use rather than the exact number (and there are even more options), 
but really the only case where
it makes sense to use <code>--nodes</code> with <code>--ntasks-per-node</code> in this case, is if all tasks
would fit on a single node and you also want to force them on a single node so that all
MPI communications are done through shared memory rather than via the Slingshot interconnect.</p>
<p>Restricting the choice of resources for the scheduler may increase your waiting time
in the queue though.</p>
</li>
<li>
<p>Specifying the number of CPUs (cores on LUMI) for each task. The easiest way to do this is by
    using <code>--cpus-per-task=&lt;number&gt;</code> or <code>-c &lt;number</code>.</p>
<p>Note that as has been discussed before, the standard behaviour of recent versions of Slurm is to
no longer forward <code>--cpus-per-task</code> from the <code>sbatch</code> or <code>salloc</code> level to the <code>srun</code> level
though CSC has made a configuration change in Slurm that will still try to do this though with
some limitations.</p>
</li>
<li>
<p>Specifying the number of GPUs per task. The easiest way here is:</p>
<ol>
<li>
<p>Use <code>--gpus-per-task=&gt;number_GPUs&gt;</code> to bind one or more GPUs to each task.
    This is probably the most used option in this scheme.</p>
</li>
<li>
<p>If however you want multiple tasks to share a GPU, then you should use 
    <code>--ntasks-per-gpu=&lt;number_of_tasks&gt;</code>. There are use cases where this makes sense.
    However, at the time of writing this does not work properly.</p>
</li>
</ol>
<p>While this does ensure a proper distribution of GPUs across nodes compatible with the 
distrubtions of cores to run the requested tasks, we will again run into binding issues
when these options are propagated to <code>srun</code> to create the actual job steps, and hre this
is even more tricky to solve.</p>
<p>We will again discuss a solution in the 
<a href="08_Binding.md">Chapter "Process and thread distribution and binding"</a></p>
</li>
<li>
<p>CPU memory. By default you get less than the memory per core on the node type. To change:</p>
<ol>
<li>
<p>Against the logic there is <strong>no</strong> <code>--mem-per-task=&lt;number&gt;</code>, instead memory needs to be specified in
    function of the other allocated resources.</p>
</li>
<li>
<p>Use <code>--mem-per-cpu=&lt;number&gt;</code> to request memory per CPU (use k, m, g to specify kilobytes, megabytes or gigabytes)</p>
</li>
<li>
<p>Alternatively on a GPU allocation <code>--mem-per-gpu=&lt;number&gt;</code>.
    <strong>This is still CPU memory and not GPU memory!</strong></p>
</li>
<li>
<p>Specifying memory per node with <code>--mem</code> doesn't make much sense unless the number of nodes is fixed.</p>
</li>
</ol>
</li>
</ol>
<div class="admonition bug">
<p class="admonition-title"><code>--ntasks-per-gpu=&lt;number&gt;</code> does not work</p>
<p>At the time of writing there were several problems when using <code>--ntasks-per-gpu=&lt;number&gt;</code> in combination
with <code>--ntasks=&lt;number&gt;</code>. While according to the Slurm documentation this is a valid request and
Slurm should automatically determine the right number of GPUs to allocate, it turns out that instead
you need to specify the number of GPUs with <code>--gpus=&lt;number&gt;</code> together with <code>--ntasks-per-gpu=&lt;number&gt;</code>
and let Slurm compute the number of tasks.</p>
<p>Moreover, we've seen cases where the final allocation was completely wrong, with tasks ending up with the
wrong number of GPUs or on the wrong node (like too many tasks on one and too little on another compared
to the number of GPUs set aside in each of these nodes).</p>
</div>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per core allocations: Warning: Allocations per socket?" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerCoreWarningSocket.png" /></p>
</figure>
<div class="admonition warning">
<p class="admonition-title"><code>--sockets-per-node</code> and <code>--ntasks-per-socket</code></p>
<p>If you don't read the manual pages of Slurm carefully enough you may have the impression that
you can use parameters like <code>--sockets-per-node</code> and <code>--ntasks-per-socket</code> to force all tasks
on a single socket (and get a single socket), but these options will not work as you expect.</p>
<p>The <code>--sockets-per-node</code> option is not used to request an exact resource, but to specify a 
type of node by specifying the <em>minimal</em> number of sockets a node should have.It is an irrelevant
option on LUMI as each partition does have only a single node type.</p>
<p>If you read the manual carefully, you will also see that there is a subtle difference between
<code>--ntasks-per-node</code> and <code>--ntasks-per-socket</code>: With <code>--ntasks-per-node</code> you specify the
<em>exact</em> number of tasks for each node while with <code>--tasks-per-socket</code> you specify the 
<em>maximum</em> number of tasks for each socket. So all hope that something like</p>
<div class="highlight"><pre><span></span><code>--ntasks=8 --ntasks-per-socket=8 --cpus-per-task=8
</code></pre></div>
<p>would always ensure that you get a socket for yourself with each task nicely assigned to
a single L3 cache domain, is futile.</p>
</div>
<h3 id="different-job-steps-in-a-single-job">Different job steps in a single job<a class="headerlink" href="#different-job-steps-in-a-single-job" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per core allocations: Different job steps (1)" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerCoreJobstep_1.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide Per core allocations: Different job steps (2)" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PerCoreJobstep_2.png" /></p>
</figure>
<p>It is possible to have an <code>srun</code> command with a different task structure in your job script.
This will work if no task requires more CPUs or GPUs than in the original request, and if there are
either not more tasks either or if an entire number of tasks in the new structure fits in a task
in the structure from the allocation and the total number of tasks does not exceed the original number
multiplied with that entire number. Other cases may work randomly, depending on how Slurm did the
actual allocation. In fact, this may even be abused to ensure that all tasks are allocated to a single
node, though this is done more elegantly by just specifying <code>--nodes=1</code>.</p>
<p>With GPUs though it can become very complicated to avoid binding problems if the Slurm way of implementing
GPU binding does not work for you.</p>
<details class="example">
<summary>Some examples that work and don't work (click to expand)</summary>
<p>Consider the job script:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#! /usr/bin/bash</span>
<span class="c1">#SBATCH --job-name=slurm-small-multiple-srun</span>
<span class="c1">#SBATCH --partition=small</span>
<span class="c1">#SBATCH --ntasks=4</span>
<span class="c1">#SBATCH --cpus-per-task=4</span>
<span class="c1">#SBATCH --hint=nomultithread</span>
<span class="c1">#SBATCH --time=5:00</span>
<span class="c1">#SBATCH --output %x-%j.txt</span>
<span class="c1">#SBATCH --acount=project_46YXXXXXX</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/22.12<span class="w"> </span>partition/C<span class="w"> </span>lumi-CPEtools/1.1-cpeCray-22.12

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Running on </span><span class="nv">$SLURM_JOB_NODELIST</span><span class="s2">&quot;</span>

<span class="nb">set</span><span class="w"> </span>-x

omp_check

srun<span class="w"> </span>--ntasks<span class="o">=</span><span class="m">1</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">3</span><span class="w"> </span>omp_check

srun<span class="w"> </span>--ntasks<span class="o">=</span><span class="m">2</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">4</span><span class="w"> </span>hybrid_check

srun<span class="w"> </span>--ntasks<span class="o">=</span><span class="m">4</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>mpi_check

srun<span class="w"> </span>--ntasks<span class="o">=</span><span class="m">16</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>mpi_check

srun<span class="w"> </span>--ntasks<span class="o">=</span><span class="m">1</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">16</span><span class="w"> </span>omp_check

<span class="nb">set</span><span class="w"> </span>+x
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nsacct for the job:\n</span><span class="k">$(</span>sacct<span class="w"> </span>-j<span class="w"> </span><span class="nv">$SLURM_JOB_ID</span><span class="k">)</span><span class="s2">\n&quot;</span>
</code></pre></div>
<p>In the first output example (with lots of output deleted) we got the full
allocation of 16 cores on a single node, and in fact, even 16 consecutive cores
though spread across 3 L3 cache domains. We'll go over the output in steps:</p>
<div class="highlight"><pre><span></span><code>Running on nid002154

+ omp_check

Running 32 threads in a single process

++ omp_check: OpenMP thread   0/32  on cpu  20/256 of nid002154
++ omp_check: OpenMP thread   1/32  on cpu 148/256 of nid002154
...
</code></pre></div>
<p>The first <code>omp_check</code> command was started without using <code>srun</code> and hence ran on all
hardware cores allocated to the job. This is why hardware threading is enabled and why
the executable sees 32 cores.</p>
<div class="highlight"><pre><span></span><code>+ srun --ntasks=1 --cpus-per-task=3 omp_check

Running 3 threads in a single process

++ omp_check: OpenMP thread   0/3   on cpu  20/256 of nid002154
++ omp_check: OpenMP thread   1/3   on cpu  21/256 of nid002154
++ omp_check: OpenMP thread   2/3   on cpu  22/256 of nid002154
</code></pre></div>
<p>Next <code>omp_check</code> was started via <code>srun --ntasks=1 --cpus-per-task=3</code>. One task instead of 4,
and the task is also smaller in terms of number of nodes as the tasks requested in <code>SBATCH</code>
lines, and Slurm starts the executable without problems. It runs on three cores, correctly
detects that number, and also correctly does not use hardware threading.</p>
<div class="highlight"><pre><span></span><code>+ srun --ntasks=2 --cpus-per-task=4 hybrid_check

Running 2 MPI ranks with 4 threads each (total number of threads: 8).

++ hybrid_check: MPI rank   0/2   OpenMP thread   0/4   on cpu  23/256 of nid002154
++ hybrid_check: MPI rank   0/2   OpenMP thread   1/4   on cpu  24/256 of nid002154
++ hybrid_check: MPI rank   0/2   OpenMP thread   2/4   on cpu  25/256 of nid002154
++ hybrid_check: MPI rank   0/2   OpenMP thread   3/4   on cpu  26/256 of nid002154
++ hybrid_check: MPI rank   1/2   OpenMP thread   0/4   on cpu  27/256 of nid002154
++ hybrid_check: MPI rank   1/2   OpenMP thread   1/4   on cpu  28/256 of nid002154
++ hybrid_check: MPI rank   1/2   OpenMP thread   2/4   on cpu  29/256 of nid002154
++ hybrid_check: MPI rank   1/2   OpenMP thread   3/4   on cpu  30/256 of nid002154
</code></pre></div>
<p>Next we tried to start 2 instead of 4 MPI processes with 4 cores each which also works without
problems. The allocation now starts on core 23 but that is because Slurm was still finishing
the job step on cores 20 till 22 from the previous <code>srun</code> command. This may or may not happen
and is also related to a remark we made before about using <code>sacct</code> at the end of the job where
the last job step may still be shown as running instead of completed.</p>
<div class="highlight"><pre><span></span><code>+ srun --ntasks=4 --cpus-per-task=1 mpi_check

Running 4 single-threaded MPI ranks.

++ mpi_check: MPI rank   0/4   on cpu  20/256 of nid002154
++ mpi_check: MPI rank   1/4   on cpu  21/256 of nid002154
++ mpi_check: MPI rank   2/4   on cpu  22/256 of nid002154
++ mpi_check: MPI rank   3/4   on cpu  23/256 of nid002154
</code></pre></div>
<p>Now we tried to start 4 tasks with 1 core each. This time we were lucky and the system 
considered the previous <code>srun</code> completely finished and gave us the first 4 cores of the 
allocation.</p>
<div class="highlight"><pre><span></span><code>+ srun --ntasks=16 --cpus-per-task=1 mpi_check
srun: Job 4268529 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 4268529

Running 16 single-threaded MPI ranks.

++ mpi_check: MPI rank   0/16  on cpu  20/256 of nid002154
++ mpi_check: MPI rank   1/16  on cpu  21/256 of nid002154
++ mpi_check: MPI rank   2/16  on cpu  22/256 of nid002154
++ mpi_check: MPI rank   3/16  on cpu  23/256 of nid002154
++ mpi_check: MPI rank   4/16  on cpu  24/256 of nid002154
++ mpi_check: MPI rank   5/16  on cpu  25/256 of nid002154
...
</code></pre></div>
<p>With the above <code>srun</code> command we try to start 16 single-threaded MPI processes. This fits 
perfectly in the allocation as it simply needs to put 4 of these tasks in the space reserved 
for one task in the <code>#SBATCH</code> request. The warning at the start may or may not happen. Basically
Slurm was still freeing up the cores from the previous run and therefore the new <code>srun</code> dind't 
have enough resources the first time it tried to, but it automatically tried a second time.</p>
<div class="highlight"><pre><span></span><code>+ srun --ntasks=1 --cpus-per-task=16 omp_check
srun: Job step&#39;s --cpus-per-task value exceeds that of job (16 &gt; 4). Job step may never run.
srun: Job 4268529 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 4268529

Running 16 threads in a single process

++ omp_check: OpenMP thread   0/16  on cpu  20/256 of nid002154
++ omp_check: OpenMP thread   1/16  on cpu  21/256 of nid002154
++ omp_check: OpenMP thread   2/16  on cpu  22/256 of nid002154
...
</code></pre></div>
<p>In the final <code>srun</code> command we try to run a single 16-core OpenMP run. This time Slurm produces
a warning as it would be impossible to fit a 16-cpre shared memory run in the space of 4 4-core 
tasks if the resources for those tasks would have been spread across multiple nodes. The next
warning is again for the same reason as in the previous case, but ultimately the command does run
on all 16 cores allocated and without using hardware threading.</p>
<div class="highlight"><pre><span></span><code>+ set +x

sacct for the job:
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
4268529      slurm-sma+      small project_4+         32    RUNNING      0:0 
4268529.bat+      batch            project_4+         32    RUNNING      0:0 
4268529.0     omp_check            project_4+          6  COMPLETED      0:0 
4268529.1    hybrid_ch+            project_4+         16  COMPLETED      0:0 
4268529.2     mpi_check            project_4+          8  COMPLETED      0:0 
4268529.3     mpi_check            project_4+         32  COMPLETED      0:0 
4268529.4     omp_check            project_4+         32    RUNNING      0:0 
</code></pre></div>
<p>The output of <code>sacct</code> confirms what we have been seeing. The first <code>omp_check</code>
was run without srun and ran in the original batch step which had all hardware threads
of all 16 allocated cores available. The next <code>omp_check</code> ran on 3 cores but 6 is
shwon in this scheme which is normal as the "other" hardware thread on each core is
implicitly also reserved. And the same holds for all other numbers in that column.</p>
<p>At another time I was less lucky and got the tasks spread out across 4 nodes, each 
running a single 4-core task. Let's go through the output again:</p>
<div class="highlight"><pre><span></span><code>Running on nid[002154,002195,002206,002476]

+ omp_check

Running 8 threads in a single process

++ omp_check: OpenMP thread   0/8   on cpu  36/256 of nid002154
++ omp_check: OpenMP thread   1/8   on cpu 164/256 of nid002154
++ omp_check: OpenMP thread   2/8   on cpu  37/256 of nid002154
++ omp_check: OpenMP thread   3/8   on cpu 165/256 of nid002154
++ omp_check: OpenMP thread   4/8   on cpu  38/256 of nid002154
++ omp_check: OpenMP thread   5/8   on cpu 166/256 of nid002154
++ omp_check: OpenMP thread   6/8   on cpu  39/256 of nid002154
++ omp_check: OpenMP thread   7/8   on cpu 167/256 of nid002154
</code></pre></div>
<p>The first <code>omp_check</code> now uses all hardware threads of the 4 cores allocated
in the first node of the job (while using 16 cores/32 threads in the configuration
where all cores were allocated on a single node).</p>
<div class="highlight"><pre><span></span><code>+ srun --ntasks=1 --cpus-per-task=3 omp_check

Running 3 threads in a single process

++ omp_check: OpenMP thread   0/3   on cpu  36/256 of nid002154
++ omp_check: OpenMP thread   1/3   on cpu  37/256 of nid002154
++ omp_check: OpenMP thread   2/3   on cpu  38/256 of nid002154
</code></pre></div>
<p>Running a three core OpenMP job goes without problems as it nicely fits within the
space of a single task of the <code>#SBATCH</code> allocation.</p>
<div class="highlight"><pre><span></span><code>+ srun --ntasks=2 --cpus-per-task=4 hybrid_check

Running 2 MPI ranks with 4 threads each (total number of threads: 8).

++ hybrid_check: MPI rank   0/2   OpenMP thread   0/4   on cpu  36/256 of nid002195
++ hybrid_check: MPI rank   0/2   OpenMP thread   1/4   on cpu  37/256 of nid002195
++ hybrid_check: MPI rank   0/2   OpenMP thread   2/4   on cpu  38/256 of nid002195
++ hybrid_check: MPI rank   0/2   OpenMP thread   3/4   on cpu  39/256 of nid002195
++ hybrid_check: MPI rank   1/2   OpenMP thread   0/4   on cpu  46/256 of nid002206
++ hybrid_check: MPI rank   1/2   OpenMP thread   1/4   on cpu  47/256 of nid002206
++ hybrid_check: MPI rank   1/2   OpenMP thread   2/4   on cpu  48/256 of nid002206
++ hybrid_check: MPI rank   1/2   OpenMP thread   3/4   on cpu  49/256 of nid002206
</code></pre></div>
<p>Running 2 4-thread MPI processes also goes without problems. In this case we got the second and third
task from the original allocation, likely because Slurm was still freeing up the first node
after the previous <code>srun</code> command.</p>
<div class="highlight"><pre><span></span><code>+ srun --ntasks=4 --cpus-per-task=1 mpi_check
srun: Job 4268614 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 4268614

Running 4 single-threaded MPI ranks.

++ mpi_check: MPI rank   0/4   on cpu  36/256 of nid002154
++ mpi_check: MPI rank   1/4   on cpu  36/256 of nid002195
++ mpi_check: MPI rank   2/4   on cpu  46/256 of nid002206
++ mpi_check: MPI rank   3/4   on cpu   0/256 of nid002476
</code></pre></div>
<p>Running 4 single threaded processes also goes without problems (but the fact that they are
scheduled on 4 different nodes here is likely an artifact of the way we had to force to get
more than one node as the small partition on LUMI was not very busy at that time).</p>
<div class="highlight"><pre><span></span><code>+ srun --ntasks=16 --cpus-per-task=1 mpi_check

Running 16 single-threaded MPI ranks.

++ mpi_check: MPI rank   0/16  on cpu  36/256 of nid002154
++ mpi_check: MPI rank   1/16  on cpu  37/256 of nid002154
++ mpi_check: MPI rank   2/16  on cpu  38/256 of nid002154
++ mpi_check: MPI rank   3/16  on cpu  39/256 of nid002154
++ mpi_check: MPI rank   4/16  on cpu  36/256 of nid002195
++ mpi_check: MPI rank   5/16  on cpu  37/256 of nid002195
++ mpi_check: MPI rank   6/16  on cpu  38/256 of nid002195
++ mpi_check: MPI rank   7/16  on cpu  39/256 of nid002195
++ mpi_check: MPI rank   8/16  on cpu  46/256 of nid002206
++ mpi_check: MPI rank   9/16  on cpu  47/256 of nid002206
++ mpi_check: MPI rank  10/16  on cpu  48/256 of nid002206
++ mpi_check: MPI rank  11/16  on cpu  49/256 of nid002206
++ mpi_check: MPI rank  12/16  on cpu   0/256 of nid002476
++ mpi_check: MPI rank  13/16  on cpu   1/256 of nid002476
++ mpi_check: MPI rank  14/16  on cpu   2/256 of nid002476
++ mpi_check: MPI rank  15/16  on cpu   3/256 of nid002476
</code></pre></div>
<p>16 single threaded MPI processes also works without problems.</p>
<div class="highlight"><pre><span></span><code>+ srun --ntasks=1 --cpus-per-task=16 omp_check
srun: Job step&#39;s --cpus-per-task value exceeds that of job (16 &gt; 4). Job step may never run.
srun: Warning: can&#39;t run 1 processes on 4 nodes, setting nnodes to 1
srun: error: Unable to create step for job 4268614: More processors requested than permitted
...
</code></pre></div>
<p>However, trying to run a single 16-thread process now fails. Slurm first warns us that it might fail,
then tries and lets it fail.</p>
</details>
<h2 id="the-job-environment">The job environment<a class="headerlink" href="#the-job-environment" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Job environment" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/JobEnvironment.png" /></p>
</figure>
<p>On LUMI, <code>sbatch</code>, <code>salloc</code> and <code>srun</code> will all by default copy the environment in which they run to the
job step they start (the batch job step for <code>sbatch</code>, an interactive job step for <code>salloc</code> and a regular
job step for <code>srun</code>). For <code>salloc</code> this is normal behaviour as it also starts an interactive shell on the
login nodes (and it cannot be changed with a command line parameter). For <code>srun</code>, any other behaviour
would be a pain as each job step would need to set up an environment. But for <code>sbatch</code> this may be
surprising to some as the environment on the login nodes may not be the best environment for the
compute nodes. Indeed, we do recommend to reload, e.g., the LUMI modules to use software optimised
specifically for the compute nodes or to have full support of ROCm.</p>
<p>It is possible to change this behaviour or to define extra environment variables with
<code>sbatch</code> and <code>srun</code> using the command line option <code>--export</code>: </p>
<ul>
<li>
<p><code>--export=NONE</code> will start the job (step) in a clean environment. The environment will not be inherited,
    but Slurm will attempt to re-create the user environment even if no login shell is called or used in
    the batch script. (<code>--export=NIL</code> would give you a truly empty environment.)</p>
</li>
<li>
<p>To define extra environment variables, use <code>--export=ALL,VAR1=VALUE1</code> which would pass all existing 
    environment variables and define a new one, <code>VAR1</code>, with the value <code>VALUE1</code>. It is of course also possible
    to define more environment variables using a comma-separated list (without spaces). 
    With <code>sbatch</code>, specifying <code>--export</code> on the command line that way is a way to parameterise a batch script.
    With <code>srun</code> it can be very useful with heterogeneous jobs if different parts of the job need a different 
    setting for an environment variable (e.g., <code>OMP_NUM_THREADS</code>).</p>
<p>Note however that <code>ALL</code> in the above <code>--export</code> option is essential as otherwise only the environment 
variable <code>VAR1</code> would be defined.</p>
<p>It is in fact possible to pass only select environment variables by listing them without assigning a new 
value and omitting the <code>ALL</code> but we see no practical use of that on LUMI as the list of environment variables
that is needed to have a job script in which you can work more or less normally is rather long.</p>
</li>
</ul>
<figure style="border: 1px solid #000">
<p><img alt="Slide Passing arguments" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/PassingArguments.png" /></p>
</figure>
<div class="admonition note">
<p class="admonition-title">Passing argumetns to a batch script</p>
<p>With the Slurm <code>sbatch</code> command, any argument passed after the name of the job script is passed to the
job script as an argument, so you can use regular bash shell argument processing to pass arguments to
the bash script and do not necessarily need to use <code>--export</code>. Consider the following job script to
demonstrate both options:</p>
<div class="highlight"><pre><span></span><code>!<span class="w"> </span>/usr/bin/bash
<span class="c1">#SBATCH --job-name=slurm-small-parameters</span>
<span class="c1">#SBATCH --partition=small</span>
<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --cpus-per-task=1</span>
<span class="c1">#SBATCH --hint=nomultithread</span>
<span class="c1">#SBATCH --time=5:00</span>
<span class="c1">#SBATCH --output %x-%j.txt</span>
<span class="c1">#SBATCH --account=project_46YXXXXXX</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Batch script parameter 0: </span><span class="nv">$0</span><span class="s2">&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Batch script parameter 1: </span><span class="nv">$1</span><span class="s2">&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Environment variable PAR1: </span><span class="nv">$PAR1</span><span class="s2">&quot;</span>
</code></pre></div>
<p>Now start this with (assuming the job script is saved as <code>slurm-small-parameters.slurm</code>)</p>
<div class="highlight"><pre><span></span><code>$ sbatch --export=ALL,PAR1=&quot;Hello&quot; slurm-small-parameters.slurm &#39;Wow, this works!&#39;
</code></pre></div>
<p>and check the output file when the job is completed:</p>
<div class="highlight"><pre><span></span><code>Batch script parameter 0: /var/spool/slurmd/job4278998/slurm_script
Batch script parameter 1: Wow, this works!
Environment variable PAR1: Hello
</code></pre></div>
<p>You see that you do not get the path to the job script as it was submitted (which you may expect 
to be the value of <code>$0</code>). Instead the job script is buffered when you execute <code>sbatch</code> and started
from a different directory. <code>$1</code> works as expected, and <code>PAR1</code> is also defined.</p>
<p>In fact, passing arguments through command line arguments of the bash script is a more robust
mechanism than using <code>--export</code> as can be seen from the bug discussed below...</p>
</div>
<div class="admonition bug">
<p class="admonition-title">Fragile behaviour of <code>--export</code></p>
<p>One of the problems with <code>--export</code> is that you cannot really assign any variable to a new
environment variable the way you would do it on the bash command line. It is not clear what
internal processing is going on, but the value is not always what you would expect. 
In particular, problems can be expected when the value of the variable contains a semicolon.</p>
<p>E.g., try the command from the previous example with <code>--export=ALL,PAR1='Hello, world'</code> 
and it turns out that only <code>Hello</code> is passed as the value of the variable.</p>
</div>
<!-- BELGIUM 
!!! lumi-be "Differences with some VSC systems"
    The job environment in Slurm is different from that of some other resource managers, and in paritcular 
    Torque which was in use on VSC clusters and whose behaviour is still emulated on some. 
    LUMI uses the default settings of Slurm when it comes to environment management which is to start
    a job or job step in the environment from which the Slurm command was called.
-->

<h2 id="automatic-requeueing">Automatic requeueing<a class="headerlink" href="#automatic-requeueing" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Automatic requeueing" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/AutomaticRequeueing.png" /></p>
</figure>
<p>LUMI has the Slurm automatic requeueing of jobs upon node failure enabled. So jobs will be
automatically resubmitted when one of the allocated nodes fails. For this an identical job ID
is used and by default the prefious output will be truncated when the requeueed job starts.</p>
<p>There are some options to influence this behaviour:</p>
<ul>
<li>
<p>Automatic requeueing can be disabled at job submission with the <code>--no-requeue</code> option
    of the <code>sbatch</code> command.</p>
</li>
<li>
<p>Truncating of the output files can be avoided by specifying <code>--open-mode=append</code>.</p>
</li>
<li>
<p>It is also possible to detect in a job script if a job has been restarted or not. For this
    Slurm sets the environment variable <code>SLURM_RESTART_COUNT</code> which is 0 the first time a job 
    script runs and augmented by one at every restart.</p>
</li>
</ul>
<h2 id="job-dependencies">Job dependencies<a class="headerlink" href="#job-dependencies" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Job dependencies" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/JobDependencies.png" /></p>
</figure>
<p>The maximum wall time that a job can run on LUMI is fairly long for a Tier-0 system. Many other big systems in 
Europe will only allow a maximum wall time of 24 hours. Despite this, this is not yet enough for some users.
One way to deal with this is ensure that programs end in time and write the necessary restart information in
a file, then start a new job that continues from that file. </p>
<p>You don't have to wait to submit that second job. Instead, it is possible to tell Slurm that the second job
should not start before the first one has ended (and ended successfully). This is done through job dependencies.
It would take us too far to discuss all possible cases in this tutorial.</p>
<p>One example is</p>
<div class="highlight"><pre><span></span><code>$ sbatch --dependency=afterok:&lt;jobID&gt; jobdepend.slurm 
</code></pre></div>
<p>With this statement, the job defined by the job script <code>jobdpend.slurm</code> will not start until the job with the
given jobID has ended successfully (and you may have to clean up the queue if it never ends successfully). But 
there are other possibilities also, e.g., start another job after a list of jobs has ended, or after a job has
failed. We refer to the 
<a href="https://slurm.schedmd.com/archive/slurm-22.05.8/sbatch.html">sbatch manual page</a> where you should 
<a href="https://slurm.schedmd.com/archive/slurm-22.05.8/sbatch.html#OPT_dependency">look for <code>--dependency</code> on the page</a>.</p>
<p>It is also possible to automate the process of submitting a chain of dependent jobs. For this the
<code>sbatch</code> flag <a href="https://slurm.schedmd.com/archive/slurm-22.05.8/sbatch.html#OPT_parsable"><code>--parsable</code></a>
can be used which on LUMI will only print the job number of the job being submitted. So to 
let the job defined by <code>jobdepend.slurm</code> run after the job defined by <code>jobfirst.slurm</code> while 
submitting both at the same time, you can use something like</p>
<div class="highlight"><pre><span></span><code><span class="nv">first</span><span class="o">=</span><span class="k">$(</span>sbatch<span class="w"> </span>--parsable<span class="w"> </span>jobfirst.slurm<span class="k">)</span>
sbatch<span class="w"> </span>--dependency<span class="o">=</span>afterok:<span class="nv">$first</span><span class="w"> </span>jobdepend.slurm
</code></pre></div>
<h2 id="interactive-jobs">Interactive jobs<a class="headerlink" href="#interactive-jobs" title="Permanent link">&para;</a></h2>
<p>Interactive jobs can have several goals, e.g.,</p>
<ol>
<li>
<p>Simply testing a code or steps to take to get a code to run while developing a job script.
    In this case you will likely want an allocation in which you can also easily run parallel MPI
    jobs.</p>
</li>
<li>
<p>Compiling a code usually works better interactively, but here you only need an allocation for
    a single task supporting multiple cores if your code supports a parallel build process.
    Building on the compute nodes is needed if architecture-specific optimisations are desired
    while the code building process does not support cross-compiling (e.g., because the build process
    adds <code>-march=native</code> or a similar compiler switch even if it is told not to do so) or ie you want
    to compile software for the GPUs that during the configure or build process needs a GPU to be 
    present in the node to detect its features.</p>
</li>
<li>
<p>Attaching to a running job to inspect how it is doing.</p>
</li>
</ol>
<h3 id="interactive-jobs-with-salloc">Interactive jobs with salloc<a class="headerlink" href="#interactive-jobs-with-salloc" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide Interactive jobs with salloc" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/JobInteractiveSalloc.png" /></p>
</figure>
<p>This is a very good way of working for the first scenario described above. </p>
<p>Using <code>salloc</code> will create a pool of resources reserved for interactive execution, and
will start a new shell on the node where you called <code>salloc</code>(usually a login node).
As such it does not take resources away from other job steps that you will create so
the shell is a good environment to test most stuff that you would execute in the 
batch job step of a job script.</p>
<p>To execute any code on one of the allocated compute nodes, be it a large sequential program,
a shared memory program, distributed memory program or hybrid code, you can use <code>srun</code> in
the same way as we have discussed for job scripts.</p>
<p>It is possible to obtain an interactive shell on the first allocated compute node with</p>
<div class="highlight"><pre><span></span><code>srun --pty $SHELL
</code></pre></div>
<p>(which is nothing more is specified would give you a single core for the shell),
but keep in mind that this takes away resources from other job steps so if you try to
start further job steps from that interactive shell you will note that you have fewer 
resources available, and will have to force overlap (with <code>--overlap</code>),
so it is not very practical to work that way.</p>
<p>To terminate the allocation, simply exit the shell that was created by <code>salloc</code> with <code>exzit</code> or 
the CTRL-D key combination (and the same holds for the interactive shell in the previous
paragraph).</p>
<details class="example">
<summary>Example with <code>salloc</code> and a GPU code (click to expand)</summary>
<div class="highlight"><pre><span></span><code>$ salloc --account=project_46YXXXXXX --partition=standard-g --nodes=2 --time=15
salloc: Pending job allocation 4292946
salloc: job 4292946 queued and waiting for resources
salloc: job 4292946 has been allocated resources
salloc: Granted job allocation 4292946
$ module load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12

...

$ srun -n 16 -c 2 --gpus-per-task 1 gpu_check
MPI 000 - OMP 000 - HWT 001 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1
MPI 000 - OMP 001 - HWT 002 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1
MPI 001 - OMP 000 - HWT 003 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6
MPI 001 - OMP 001 - HWT 004 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6
MPI 002 - OMP 000 - HWT 005 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9
MPI 002 - OMP 001 - HWT 006 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9
MPI 003 - OMP 000 - HWT 007 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID ce
MPI 003 - OMP 001 - HWT 008 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID ce
MPI 004 - OMP 000 - HWT 009 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1
MPI 004 - OMP 001 - HWT 010 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1
MPI 005 - OMP 000 - HWT 011 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6
MPI 005 - OMP 001 - HWT 012 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6
MPI 006 - OMP 000 - HWT 013 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9
MPI 006 - OMP 001 - HWT 014 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9
MPI 007 - OMP 000 - HWT 015 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc
MPI 007 - OMP 001 - HWT 016 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc
MPI 008 - OMP 000 - HWT 001 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1
MPI 008 - OMP 001 - HWT 002 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1
MPI 009 - OMP 000 - HWT 003 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6
MPI 009 - OMP 001 - HWT 004 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6
MPI 010 - OMP 000 - HWT 005 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9
MPI 010 - OMP 001 - HWT 006 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9
MPI 011 - OMP 000 - HWT 007 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID ce
MPI 011 - OMP 001 - HWT 008 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID ce
MPI 012 - OMP 000 - HWT 009 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1
MPI 012 - OMP 001 - HWT 010 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1
MPI 013 - OMP 000 - HWT 011 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6
MPI 013 - OMP 001 - HWT 012 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6
MPI 014 - OMP 000 - HWT 013 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9
MPI 014 - OMP 001 - HWT 014 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9
MPI 015 - OMP 000 - HWT 015 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc
MPI 015 - OMP 001 - HWT 016 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc
</code></pre></div>
</details>
<h3 id="interactive-jobs-with-srun">Interactive jobs with srun<a class="headerlink" href="#interactive-jobs-with-srun" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide Interactive jobs with srun" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/JobInteractiveSrun.png" /></p>
</figure>
<p>Starting an interactive job with <code>srun</code> is good to get an interactive shell in which you want
to do some work without starting further job steps, e.g., for compilation on the compute nodes
or to run an interactive shared memory program such as R. It is not ideal if you want to spawn
further job steps with <code>srun</code> within the same allocation as the interactive shell already
fills a task slot, so you'd have to overlap if you want to use all resources of the job in the 
next job step. </p>
<p>For this kind of work you';ll rarely need a whole node so small and small-g will likely be your
partitions of choice.</p>
<p>To start such a job, you'd use </p>
<div class="highlight"><pre><span></span><code>srun --account=project_46YXXXXXX --partition=&lt;partition&gt; --ntasks=1 --cpus-per-task=&lt;number&gt; --time=&lt;time&gt; --pty=$SHELL
</code></pre></div>
<p>or with the short options</p>
<div class="highlight"><pre><span></span><code>srun -A project_46YXXXXXX -p &lt;partition&gt; -n 1 -c &lt;number&gt; -t &lt;time&gt; --pty $SHELL
</code></pre></div>
<p>For the GPU nodes you'd also add a <code>--gpus-per-task=&lt;number&gt;</code> to request a number of GPUs.</p>
<p>To end the interactive job, all you need to do is to leave the shell with <code>exit</code> or the
CTRL-D key combination.</p>
<h3 id="inspecting-a-running-job">Inspecting a running job<a class="headerlink" href="#inspecting-a-running-job" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide Inpecting a running job" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/JobInteractiveAttach.png" /></p>
</figure>
<p>On LUMI it is not possible to use <code>ssh</code> to log on to a compute node in use by one of your jobs.
Instead you need to use Slurm to attach a shell to an already running job. This can be done with
<code>srun</code>, but there are two differences with the previous scenario. First, you do not need a new
allocation but need to tell <code>srun</code> to use an existing allocation. As there is already an allocation,
<code>srun</code> does not need your project account in this case. Second, usually the job will be using all its
resources so there is no room in the allocation to create another job step with the interactive shell.
This is solved by teslling <code>srun</code> that the resources should overlap with those already in use.</p>
<p>To start an interactive shell on the first allocated node of a specific job/allocation, use</p>
<div class="highlight"><pre><span></span><code>srun --jobid=&lt;jobID&gt; --overlap --pty $SHELL
</code></pre></div>
<p>and to start an interactive shell on another node of the jobm simply add a <code>-w</code> or <code>--nodelist</code> argument:</p>
<div class="highlight"><pre><span></span><code>srun --jobid=&lt;jobID&gt; --nodelist=nid00XXXX --overlap --pty $SHELL
srun --jobid=&lt;jobID&gt; -w nid00XXXX --overlap --pty $SHELL
</code></pre></div>
<!--
TODO: I guess you need to specify --gpus-per-task also to gain access to the GPUs should you need this?
-->

<p>Instead of starting a shell, you could also just run a command, e.g., <code>top</code>, to inspect what the nodes are doing.</p>
<p>Note that you can find out the nodes allocated to your job using <code>squeue</code> (probably the easiest as the nodes are
shown by default), <code>sstat</code> or <code>salloc</code>.</p>
<!--
TODO: Add some examples of how to use sstat or salloc for this?
-->

<h2 id="job-arrays">Job arrays<a class="headerlink" href="#job-arrays" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Job arrays" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/JobArrays.png" /></p>
</figure>
<p>Job arrays is a mechanism to submit a large number of related jobs with the same batch script in a
single <code>sbatch</code> operation.</p>
<p>As an example, consider the job script <code>job_array.slurm</code></p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --account=project_46YXXXXXX</span>
<span class="c1">#SBATCH --partition=small</span>
<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --cpus-per-task=1</span>
<span class="c1">#SBATCH --mem-per-cpu=1G</span>
<span class="c1">#SBATCH --time=15:00</span>

<span class="nv">INPUT_FILE</span><span class="o">=</span><span class="s2">&quot;input_</span><span class="si">${</span><span class="nv">SLURM_ARRAY_TASK_ID</span><span class="si">}</span><span class="s2">.dat&quot;</span>
<span class="nv">OUTPUT_FILE</span><span class="o">=</span><span class="s2">&quot;output_</span><span class="si">${</span><span class="nv">SLURM_ARRAY_TASK_ID</span><span class="si">}</span><span class="s2">.dat&quot;</span>

./test_set<span class="w"> </span>-input<span class="w"> </span><span class="si">${</span><span class="nv">INPUT_FILE</span><span class="si">}</span><span class="w"> </span>-output<span class="w"> </span><span class="si">${</span><span class="nv">OUTPUT_FILE</span><span class="si">}</span>
</code></pre></div>
<p>Note that Slurm defines the environment variable <code>SLURM_ARRAY_TASK_ID</code> which will have a unique
value for each job of the job array, varying in the range given at job submission.
This enables to distinguish between the different runs and can be used to generate names of
input and output files.</p>
<p>Submitting this job script and running it for values of <code>SLURM_ARRAY_TASK_ID</code> going from 1 to 100
could be done with </p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>sbatch<span class="w"> </span>--array<span class="w"> </span><span class="m">1</span>-100<span class="w"> </span>job_array.slurm
</code></pre></div>
<p>Note that this will count for 100 Slurm jobs so the size of your array jobs on LUMI is limited by the
rather strict limit on job size. LUMI is made as a system for big jobs, and is a system with a lot of
users, and there are only that many simultaneous jobs that a scheduler can deal with. Users doing 
throughput computing should do some kind of hierarchical scheduling, running a subscheduler in the 
job that then further start subjobs.</p>
<h2 id="heterogeneous-jobs">Heterogeneous jobs<a class="headerlink" href="#heterogeneous-jobs" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Heterogeneous jobs" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/HeterogeneousJobs.png" /></p>
</figure>
<p>A heterogeneous job is one in which multiple executables run in a single <code>MPI_COMM_WORLD</code>, or a single
executable runs in different combinations (e.g., some multithreaded and some single-threaded MPI ranks where
the latter take a different code path from the former and do a different task). Onme example is large 
simulation codes that use separate I/O servers to take care of the parallel IO ot the file system.</p>
<p>There are two ways to start such a job:</p>
<ol>
<li>
<p>Create groups in the <code>SBATCH</code> lines, separated by <code>#SBATCH hetjob</code> lines, and then recall these groups with
    <code>srun</code>. This is the most powerful mechanism as in principle one could use nodes in different partitions
    for different parts of the heterogeneous job.</p>
</li>
<li>
<p>Request the total number of nodes needed with the <code>#SBATCH</code> lines and then do the rest entirely with
    <code>srun</code>, when starting the heterogeneous job step. The different blocks in <code>srun</code> are separated by a colon.
    In this case we can only use a single partition.</p>
</li>
</ol>
<p>The Slurm support for heterogeneous jobs is not very good and problems to often occur, or new
bugs are being introduced.</p>
<ul>
<li>
<p>The different parts of heterogeneous jobs in the first way of specifying them, are treated as different
    jobs which may give problems with the scheduling.</p>
</li>
<li>
<p>When using the <code>srun</code> method, these are still separate job steps and it looks like a second job is created
    internally to run these, and on a separate set of nodes.</p>
</li>
</ul>
<figure style="border: 1px solid #000">
<p><img alt="Slide Heterogeneous jobs: Example with #SBATCH" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/HeterogeneousJobsExampleSBATCH.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide Heterogeneous jobs: Example with srun" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/HeterogeneousJobsExampleSrun.png" /></p>
</figure>
<details class="example">
<summary>Let's show with an example (worked out more in the text than in the slides)</summary>
<p>Consider the following case of a 2-component job:</p>
<ul>
<li>
<p>Part 1: Application A on 1 node with 32 tasks with 4 OpenMP threads each</p>
</li>
<li>
<p>Part 2: Application B on 2 nodes with 4 tasks per node with 32 OpenMP threads each</p>
</li>
</ul>
<p>We will simulate this case with the <code>hybrid_check</code> program from the <code>lumi-CPEtools</code> module 
that we have used in earlier examples also.</p>
<p>The job script for the first method would look like:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#! /usr/bin/bash</span>
<span class="kp">#SBATCH --job-name=slurm-herterogeneous-sbatch</span>
<span class="kp">#SBATCH --time=5:00</span>
<span class="kp">#SBATCH --output %x-%j.txt</span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --nodes=1</span>
<span class="kp">#SBATCH --ntasks-per-node=32</span>
<span class="kp">#SBATCH --cpus-per-task=4</span>
<span class="kp">#SBATCH hetjob</span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --nodes=2</span>
<span class="kp">#SBATCH --ntasks-per-node=4</span>
<span class="kp">#SBATCH --cpus-per-task=32</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/22.12<span class="w"> </span>partition/C<span class="w"> </span>lumi-CPEtools/1.1-cpeCray-22.12

<span class="nb">srun</span><span class="w"> </span>--het-group<span class="o">=</span><span class="m">0</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK_HET_GROUP_0</span><span class="w"> </span>--export<span class="o">=</span>ALL,OMP_NUM_THREADS<span class="o">=</span><span class="m">4</span><span class="w">  </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>app_A<span class="w"> </span>:<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--het-group<span class="o">=</span><span class="m">1</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK_HET_GROUP_1</span><span class="w"> </span>--export<span class="o">=</span>ALL,OMP_NUM_THREADS<span class="o">=</span><span class="m">32</span><span class="w"> </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>app_B

<span class="nb">srun</span><span class="w"> </span>--het-group<span class="o">=</span><span class="m">0</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK_HET_GROUP_0</span><span class="w"> </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>app_A<span class="w"> </span>:<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--het-group<span class="o">=</span><span class="m">1</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK_HET_GROUP_1</span><span class="w"> </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>app_B

<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nsacct for the job:\n</span><span class="k">$(</span>sacct<span class="w"> </span>-j<span class="w"> </span><span class="nv">$SLURM_JOB_ID</span><span class="k">)</span><span class="s2">\n&quot;</span>
</code></pre></div>
<p>There is a single <code>srun</code> command. <code>--het-group=0</code> tells <code>srun</code> to pick up the settings for the first
heterogeneous group (before the <code>#SBATCH hetjob</code> line), and use that to start the <code>hybrid_check</code> program
with the command line arguments <code>-l app_A</code>.  Next we have the column to tell <code>srun</code> that we start with the 
second group, which is done in the same way. Note that since recent versions of Slurm do no longer 
propagate the value for <code>--cpus-per-task</code>, we need to specify the value here explicitly which we can do
via an environment variable. This is one of the cases where the patch to work around this new behaviour on
LUMI does not work.</p>
<p>This job script shows also demonstrates how a different value of a variable can be passed to each
component using <code>--export</code>, even though this was not needed as the second case would show.</p>
<p>The output of this job script would look lik (with a lot omitted):</p>
<div class="highlight"><pre><span></span><code>srun: Job step&#39;s --cpus-per-task value exceeds that of job (32 &gt; 4). Job step may never run.

Running 40 MPI ranks with between 4 and 32 threads each (total number of threads: 384).

++ app_A: MPI rank   0/40  OpenMP thread   0/4   on cpu   0/256 of nid001083
++ app_A: MPI rank   0/40  OpenMP thread   1/4   on cpu   1/256 of nid001083
...
++ app_A: MPI rank  31/40  OpenMP thread   2/4   on cpu 126/256 of nid001083
++ app_A: MPI rank  31/40  OpenMP thread   3/4   on cpu 127/256 of nid001083
++ app_B: MPI rank  32/40  OpenMP thread   0/32  on cpu   0/256 of nid001544
++ app_B: MPI rank  32/40  OpenMP thread   1/32  on cpu   1/256 of nid001544
...
++ app_B: MPI rank  35/40  OpenMP thread  30/32  on cpu 126/256 of nid001544
++ app_B: MPI rank  35/40  OpenMP thread  31/32  on cpu 127/256 of nid001544
++ app_B: MPI rank  36/40  OpenMP thread   0/32  on cpu   0/256 of nid001545
++ app_B: MPI rank  36/40  OpenMP thread   1/32  on cpu   1/256 of nid001545
...
++ app_B: MPI rank  39/40  OpenMP thread  30/32  on cpu 126/256 of nid001545
++ app_B: MPI rank  39/40  OpenMP thread  31/32  on cpu 127/256 of nid001545
... (second run produces identical output)

sacct for the job:
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
4285795+0    slurm-her+   standard project_4+        256    RUNNING      0:0 
4285795+0.b+      batch            project_4+        256    RUNNING      0:0 
4285795+0.0  hybrid_ch+            project_4+        256  COMPLETED      0:0 
4285795+0.1  hybrid_ch+            project_4+        256  COMPLETED      0:0 
4285795+1    slurm-her+   standard project_4+        512    RUNNING      0:0 
4285795+1.0  hybrid_ch+            project_4+        512  COMPLETED      0:0 
4285795+1.1  hybrid_ch+            project_4+        512  COMPLETED      0:0 
</code></pre></div>
<p>The warning at the start can be safely ignored. It jsut shows how heterogeneous job
were an afterthought in Slurm and likely implemented in a very dirty way. We see that
we get what we expected: 32 MPI ranks on the first node of the allocation, then 4 on 
each of the other two nodes.</p>
<p>The output of <code>sacct</code> is somewhat surprising. Slurm has essnetially started two jobs,
with jobIDs that end with <code>+0</code> and <code>+1</code>, and it first shows all job steps for the first
job, which is the batch job step and the first group of both <code>srun</code> commands, and then
shows the second job and its job steps, again indicating that heterogeneous jobs are
not really treated as a single job.</p>
<p>The same example can also be done by just allocating 3 nodes and then using more arguments
with <code>srun</code> to start the application:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#! /usr/bin/bash</span>
<span class="kp">#SBATCH --job-name=slurm-herterogeneous-srun</span>
<span class="kp">#SBATCH --time=5:00</span>
<span class="kp">#SBATCH --output %x-%j.txt</span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --nodes=3</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/22.12<span class="w"> </span>partition/C<span class="w"> </span>lumi-CPEtools/1.1-cpeCray-22.12

<span class="nb">srun</span><span class="w"> </span>--ntasks<span class="o">=</span><span class="m">32</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">4</span><span class="w">  </span>--export<span class="o">=</span>ALL,OMP_NUM_THREADS<span class="o">=</span><span class="m">4</span><span class="w">  </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>app_A<span class="w"> </span>:<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--ntasks<span class="o">=</span><span class="m">8</span><span class="w">  </span>--cpus-per-task<span class="o">=</span><span class="m">32</span><span class="w"> </span>--export<span class="o">=</span>ALL,OMP_NUM_THREADS<span class="o">=</span><span class="m">32</span><span class="w"> </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>app_B

<span class="nb">srun</span><span class="w"> </span>--ntasks<span class="o">=</span><span class="m">32</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">4</span><span class="w">  </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>app_A<span class="w"> </span>:<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--ntasks<span class="o">=</span><span class="m">8</span><span class="w">  </span>--cpus-per-task<span class="o">=</span><span class="m">32</span><span class="w"> </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>app_B

<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nsacct for the job:\n</span><span class="k">$(</span>sacct<span class="w"> </span>-j<span class="w"> </span><span class="nv">$SLURM_JOB_ID</span><span class="k">)</span><span class="s2">\n&quot;</span>
</code></pre></div>
<p>The output of the two <code>srun</code> commands is essentially the same as before, but the output 
of <code>sacct</code> is different:</p>
<div class="highlight"><pre><span></span><code>sacct for the job:
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
4284021      slurm-her+   standard project_4+        768    RUNNING      0:0 
4284021.bat+      batch            project_4+        256    RUNNING      0:0 
4284021.0+0  hybrid_ch+            project_4+        256  COMPLETED      0:0 
4284021.0+1  hybrid_ch+            project_4+        512  COMPLETED      0:0 
4284021.1+0  hybrid_ch+            project_4+        256  COMPLETED      0:0 
4284021.1+1  hybrid_ch+            project_4+        512  COMPLETED      0:0 
</code></pre></div>
<p>We now get a single job ID but the job step for each of the <code>srun</code> commands is split 
in two separate job steps, a <code>+0</code> and a <code>+1</code>. </p>
</details>
<div class="admonition warning">
<p class="admonition-title">Erratic behaviour of <code>--nnodes=&lt;X&gt; --ntasks-per-node=&lt;Y&gt;</code> </p>
<p>One can wonder if in the second case we could still specify resources on a per-node
basis in the <code>srun</code> command:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#! /usr/bin/bash</span>
<span class="kp">#SBATCH --job-name=slurm-herterogeneous-srun</span>
<span class="kp">#SBATCH --time=5:00</span>
<span class="kp">#SBATCH --output %x-%j.txt</span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --nodes=3</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/22.12<span class="w"> </span>partition/C<span class="w"> </span>lumi-CPEtools/1.1-cpeCray-22.12

<span class="nb">srun</span><span class="w"> </span>--nodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">32</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">4</span><span class="w">  </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>app_A<span class="w"> </span>:<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--nodes<span class="o">=</span><span class="m">2</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">4</span><span class="w">  </span>--cpus-per-task<span class="o">=</span><span class="m">32</span><span class="w"> </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>hybrid_check<span class="w"> </span>-l<span class="w"> </span>app_B
</code></pre></div>
<p>It turns out that this does not work at all. Both components get the wrong number of tasks.
For some reason only 3 copies were started of the first application on the first node of the
allocation, the 2 32-thread processes on the second node and one 32-thread process on the third
node, also with an unexpected thread distribution.</p>
<p>This shows that before starting a big application it may make sense to check with the
tools from the <code>lumi-CPEtools</code> module if the allocation would be what you expect as Slurm
is definitely not free of problems when it comes to hetereogeneous jobs.</p>
</div>
<h2 id="simultaneous-job-steps">Simultaneous job steps<a class="headerlink" href="#simultaneous-job-steps" title="Permanent link">&para;</a></h2>
<figure style="border: 1px solid #000">
<p><img alt="Slide Simultaneous job steps" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/SimultaneousJobSteps.png" /></p>
</figure>
<p>It is possible to run multiple job steps in parallel on LUMI. The core of your job script would look
something like:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#! /usr/bin/bash</span>
...
<span class="c1">#SBATCH partition=standard</span>
...
srun<span class="w"> </span>-n4<span class="w"> </span>-c16<span class="w"> </span>exe1<span class="w"> </span><span class="p">&amp;</span>
sleep<span class="w"> </span><span class="m">2</span>
srun<span class="w"> </span>-n8<span class="w"> </span>-c8<span class="w"> </span>exe2<span class="w"> </span><span class="p">&amp;</span>
<span class="nb">wait</span>
</code></pre></div>
<p>The first <code>srun</code> statement will start a hybrid job of 4 tasks with 16 cores each on the first 64 cores
of the node, the second <code>srun</code> statement would start a hybrid job of 8 tasks with 8 cores each on
the remaining 64 cores. The <code>sleep 2</code> statement is used because we have experienced that from time
to time even though the second <code>srun</code> statement cannot be executed immediately as the resource manager
is busy with the first one. The <code>wait</code> command at the end is essential, as otherwise the batch job step
would end without waiting for the two <code>srun</code> commands to finish the work they started, and the whole
job would be killed.</p>
<p>Running multiple job steps in parallel in a single job can be useful if you want to ensure a proper
binding and hence do not want to use the "allocate by resources" partition, while a single job step
is not enough to fill an exclusive node. It does turn out to be tricky though, especially when GPU nodes
are being used, and with proper binding of the resources. In some cases the <code>--overlap</code> parameter of
<code>srun</code> may help a bit. (And some have reported that in some cases <code>--exact</code> is needed instead, but this
parameter is already implied if <code>--cpus-per-task</code> can be used.)</p>
<details class="example">
<summary>A longer example</summary>
<p>Consider the bash job script for an exclusive CPU node:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#! /usr/bin/bash</span>
<span class="c1">#SBATCH --job-name=slurm-simultaneous-CPU-1</span>
<span class="c1">#SBATCH --partition=standard</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --hint=nomultithread</span>
<span class="c1">#SBATCH --time=2:00</span>
<span class="c1">#SBATCH --output %x-%j.txt</span>

module<span class="w"> </span>load<span class="w"> </span>LUMI/23.09<span class="w"> </span>partition/C<span class="w"> </span>lumi-CPEtools/1.1-cpeCray-23.09

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Submitted from </span><span class="nv">$SLURM_SUBMIT_HOST</span><span class="s2">&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Running on </span><span class="nv">$SLURM_JOB_NODELIST</span><span class="s2">&quot;</span>
<span class="nb">echo</span>
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;Job script:\n</span><span class="k">$(</span>cat<span class="w"> </span><span class="nv">$0</span><span class="k">)</span><span class="s2">\n&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;SLURM_* environment variables:&quot;</span>
env<span class="w"> </span><span class="p">|</span><span class="w"> </span>egrep<span class="w"> </span>^SLURM

<span class="k">for</span><span class="w"> </span>i<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">$(</span>seq<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">7</span><span class="k">)</span>
<span class="k">do</span><span class="w"> </span>
<span class="w">    </span>srun<span class="w"> </span>--ntasks<span class="o">=</span><span class="m">1</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">16</span><span class="w"> </span>--output<span class="o">=</span><span class="s2">&quot;slurm-simultaneous-CPU-1-</span><span class="nv">$SLURM_JOB_ID</span><span class="s2">-</span><span class="nv">$i</span><span class="s2">.txt&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>bash<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;export ROCR_VISIBLE_DEVICES=</span><span class="si">${</span><span class="nv">GPU_BIND</span><span class="p">[</span><span class="nv">$i</span><span class="p">]</span><span class="si">}</span><span class="s2"> &amp;&amp; omp_check -w 30&quot;</span><span class="w"> </span><span class="p">&amp;</span>

<span class="w">    </span>sleep<span class="w"> </span><span class="m">2</span>
<span class="k">done</span>

<span class="nb">wait</span>

sleep<span class="w"> </span><span class="m">2</span>
<span class="nb">echo</span><span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;\nsacct for the job:\n</span><span class="k">$(</span>sacct<span class="w"> </span>-j<span class="w"> </span><span class="nv">$SLURM_JOB_ID</span><span class="w"> </span>--format<span class="w"> </span>JobID%-13,Start,End,AllocCPUS,NCPUS,TotalCPU,MaxRSS<span class="w"> </span>--units<span class="o">=</span>M<span class="w"> </span><span class="k">)</span><span class="s2">\n&quot;</span>
</code></pre></div>
<p>It will start 8 parallel job steps and in total create 9 files: One file with the output of the job script itself,
and then one file for each job step with the output specific to that job step.
the <code>sacct</code> command at the end shows that the 8 job parallel job steps indeed overlap, as can be seen from the
start and end time of each, with the <code>TotalCPU</code> column confirming that they are also consuming CPU time during that
time. The last bit of the output of the main batch file looks like:</p>
<div class="highlight"><pre><span></span><code>sacct for the job:
JobID                       Start                 End  AllocCPUS      NCPUS   TotalCPU     MaxRSS 
------------- ------------------- ------------------- ---------- ---------- ---------- ---------- 
6849913       2024-04-09T16:15:45             Unknown        256        256   01:04:07            
6849913.batch 2024-04-09T16:15:45             Unknown        256        256   00:00:00            
6849913.0     2024-04-09T16:15:54 2024-04-09T16:16:25         32         32  08:00.834      6.92M 
6849913.1     2024-04-09T16:15:56 2024-04-09T16:16:26         32         32  08:00.854      6.98M 
6849913.2     2024-04-09T16:15:58 2024-04-09T16:16:29         32         32  08:00.859      6.76M 
6849913.3     2024-04-09T16:16:00 2024-04-09T16:16:30         32         32  08:00.793      6.76M 
6849913.4     2024-04-09T16:16:02 2024-04-09T16:16:33         32         32  08:00.870      6.59M 
6849913.5     2024-04-09T16:16:04 2024-04-09T16:16:34         32         32  08:01.046      8.57M 
6849913.6     2024-04-09T16:16:06 2024-04-09T16:16:36         32         32  08:01.133      6.76M 
6849913.7     2024-04-09T16:16:08 2024-04-09T16:16:39         32         32  08:00.793      6.57M 
</code></pre></div>
<p>Obviously as we execute the <code>sacct</code> command in the job the end time of the batch job step and hence
the job as a whole are still unknown. We ask <code>omp_check</code> to do some computations during 30 seconds on 
each thread, and so we see that the CPU time consumed by each 16-core job is indeed around 8 minutes,
while start and end time of each job step showed that they executed for roughly 30s each and nicely 
overlapped.</p>
</details>
<h2 id="slurm-job-monitoring-commands">Slurm job monitoring commands<a class="headerlink" href="#slurm-job-monitoring-commands" title="Permanent link">&para;</a></h2>
<p>Slurm has two useful commands to monitor jobs that we want to discuss a bit further:</p>
<ul>
<li>
<p><code>sstat</code> is a command to monitor jobs that are currently running. It gets its information
    directly from the resource manager component of Slurm.</p>
</li>
<li>
<p><code>sacct</code> is a command to get information about terminated jobs. It gets its information from
    the Slurm accounting database. As that database is not continuously updated, information about
    running jobs may already be present but is far from real-time.</p>
</li>
</ul>
<p>Some users may also be familiar with the <code>sreport</code> command, but it is of limited use on LUMI.</p>
<h3 id="the-sstat-command">The <code>sstat</code> command<a class="headerlink" href="#the-sstat-command" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide Job monitoring with sstat 1" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/MonitoringSstat_1.png" /></p>
</figure>
<figure style="border: 1px solid #000">
<p><img alt="Slide Job monitoring with sstat 2" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/MonitoringSstat_2.png" /></p>
</figure>
<p>The <code>sstat</code> command is a command to get real-time information about a running job.
That information is obtained from the resource manager components in Slurm and not from the 
accounting database. The command can only produce information about job steps that are currently 
being executed and cannot be used to get information about jobs tha thave already been terminated,
or job steps that have terminated from jobs that are still running.</p>
<p>In its most simple form, you'd likely use the <code>-j</code> (or <code>--jobs</code>) flag to specify the job for which you want information:</p>
<div class="highlight"><pre><span></span><code>sstat -j 1234567
</code></pre></div>
<p>and you may like to add the <code>-a</code> flag to get information about all job steps for which information is available.
You can also restrict to a single job step, e.g.,</p>
<div class="highlight"><pre><span></span><code>sstat -j 1234567.0
</code></pre></div>
<p>The command produces a lot of output though and it is nearly impossible to interpret the output, even on a very
wide monitor.</p>
<p>To restrict that output to something that can actually be handled, you can use the <code>-o</code> or <code>--format</code> flag
to specify the columns that you want to see.</p>
<p>E.g., the following variant would show for each job step the minimum amount of CPU time that a task has
consumed, and the average across all tasks. These numbers should be fairly close if the job has a good
load balance.</p>
<div class="highlight"><pre><span></span><code>$ sstat -a -j 1234567 -o JobID,MinCPU,AveCPU
JobID            MinCPU     AveCPU
------------ ---------- ----------
1234567.bat+   00:00:00   00:00:00
1234567.1      00:23:44   00:26:02
</code></pre></div>
<p>The above output is from an MPI job that has two job steps in it. The first step was a quick initialisation
step and that one has terminated already, so we get no information about that step. The <code>1234567.1</code> step
is the currently executing one, and we do note a slight load inbalance in this case. No measurable amount
of time has been consumed running the batch script itself outside the <code>srun</code> commands in this case.</p>
<p>It can also be used to monitor memory use of the application. E.g.,</p>
<div class="highlight"><pre><span></span><code>$ sstat -a -j 1234567 -o JobID,MaxRSS,MaxRSSTask,MaxRSSNode
JobID            MaxRSS MaxRSSTask MaxRSSNode
------------ ---------- ---------- ----------
1234567.bat+     25500K          0  nid001522
1234567.1       153556K          0  nid001522
</code></pre></div>
<p>will show the maximum amount of resident memory used by any of the tasks, and also tell you
which task that is and on which node it is running.</p>
<p>You can get a list of output fields using <code>sstat -e</code> or <code>sstat --helpformat</code>. 
Or check the 
<a href="https://slurm.schedmd.com/archive/slurm-22.05.10/sstat.html#SECTION_Job-Status-Fields">"Job Status Fields" section in the <code>sstat</code> manual page</a>. That page also contains further examples.</p>
<h3 id="the-sacct-command">The <code>sacct</code> command<a class="headerlink" href="#the-sacct-command" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide Job information with sacct 1" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/MonitoringSacct_1.png" /></p>
</figure>
<p>The <code>sacct</code> command shows information kept in the Slurm job accounting database.
Its main use is to extract information about jobs or job steps that have already 
terminated. It will however also provide information about running jobs and job steps,
but that information if not real-time and only pushed periodically to the accounting
database.</p>
<p>If you know the job ID of the job you want to investigate, you can specify it
directly using the <code>-j</code> or <code>--jobs</code> flag. E.g.,</p>
<!-- Output taken from job 1234567 -->
<div class="highlight"><pre><span></span><code>$ sacct -j 1234567
JobID           JobName  Partition    Account  AllocCPUS      State ExitCode
------------ ---------- ---------- ---------- ---------- ---------- --------
1234567      healthy_u+   standard project_4+        512  COMPLETED      0:0
1234567.bat+      batch            project_4+        256  COMPLETED      0:0
1234567.0     gmx_mpi_d            project_4+          2  COMPLETED      0:0
1234567.1     gmx_mpi_d            project_4+        512  COMPLETED      0:0
</code></pre></div>
<p>This report is for a GROMACS job that ran on two nodes. The first line gives the data
for the overall job. The second line is for the batch job step that ran the batch script.
That job got access to all resources on the first node of the job which is why 256 is
shown in the <code>AllocCPUS</code> column (as that data is reported using the number of virtual cores).
Job step <code>.0</code> was really an initialisation step that ran as a single task on a single physical
core of the node, while the <code>.1</code> step was running on both nodes (as 256 tasks each on a
physical core but that again cannot be directly derived from the output shown here).</p>
<p>You can also change the amount of output that is shown using either <code>--brief</code> (which will show a lot less)
or <code>--long</code> (which shows an unwieldly amount of information similar to <code>sstat</code>),
and just as with <code>sstat</code>, the information can be fully customised using <code>-o</code> or <code>--format</code>,
but as there is a lot more information in the accounting database, the format options are different.</p>
<figure style="border: 1px solid #000">
<p><img alt="Slide Job information with sacct 2" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/MonitoringSacct_2.png" /></p>
</figure>
<p>As an example, let's check the CPU time and memory used by a job:</p>
<div class="highlight"><pre><span></span><code>$ sacct -j 1234567 --format JobID%-13,AllocCPUS,MinCPU%15,AveCPU%15,MaxRSS,AveRSS --units=M
JobID          AllocCPUS          MinCPU          AveCPU     MaxRSS     AveRSS
------------- ---------- --------------- --------------- ---------- ----------
1234567              512
1234567.batch        256        00:00:00        00:00:00     25.88M     25.88M
1234567.0              2        00:00:00        00:00:00      5.05M      5.05M
1234567.1            512        01:20:02        01:26:19    173.08M    135.27M
</code></pre></div>
<p>This is again the two node MPI job that we've used in the previous example. We used
<code>--unbits=M</code> to get the memory use per task in megabytes, which is the proper option
here as tasks are relatively small (but not uncommonly small for an HPC system when a 
properly scaling code is used). The <code>%15</code> is used to specify the width of the field
as otherwise some of that information could be truncated (and the width of 15 would have
been needed if this were a shared memory program or a program that ran for longer than
a day). By default, specifying the field width will right justify the information in 
the columns. The <code>%-13</code> tells to use a field width of 13 and to left-justify the data
in that column.</p>
<p>You can get a list of output fields using <code>sacct -e</code> or <code>sacct --helpformat</code>. 
Or check the 
<a href="https://slurm.schedmd.com/archive/slurm-22.05.10/sacct.html#SECTION_Job-Accounting-Fields">"Job Accounting Fields" section in the <code>sacct</code> manual page</a>. That page also contains further examples.</p>
<figure style="border: 1px solid #000">
<p><img alt="Slide Job information with sacct 3" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/MonitoringSacct_3.png" /></p>
</figure>
<p>Using <code>sacct</code> is a bit harder if you don't have the job ID of the job for which you want information.
You can run <code>sacct</code> without any arguments, and in that case it will produce output for your jobs that 
have run since midnight. It is also possible to define the start time (with <code>-S</code> or <code>--starttime</code>)
and the end time (with <code>-E</code> or <code>--endtime</code>) of the time window for which job data should be shown,
and there are even more features to filter jobs, though some of them are really more useful for 
administrators.</p>
<p>This is only a very brief introduction to <code>sacct</code>, basically so that you know that it exists and what
its main purpose is. But you can find more information in the
<a href="https://slurm.schedmd.com/archive/slurm-22.05.10/sacct.html"><code>sacct</code> manual page</a></p>
<h3 id="the-sreport-command">The <code>sreport</code> command<a class="headerlink" href="#the-sreport-command" title="Permanent link">&para;</a></h3>
<figure style="border: 1px solid #000">
<p><img alt="Slide Job information from sreport" loading="lazy" src="https://462000265.lumidata.eu/2day-20240502/img/LUMI-2day-20240502-06-slurm/MonitoringSreport.png" /></p>
</figure>
<p>The <code>sreport</code> command is a command to create summary reports from data in the Slurm accounting database.
Its main use is to track consumed resources in a project.</p>
<p>On LUMI it is of little use as as the billing is not done by Slurm but by a script that runs outside
of Slurm that uses data from the Slurm accounting database. That data is gathered in a different 
database though with no direct user access, and only some summary reports are brought back to the
system (and used by the <code>lumi-workspaces</code> command and some other tools for user and project monitoring).
So the correct billing information is not available in the Slurm accounting database, nor can it be easily
derived from data in the summary reports as the billing is more complicated than some billing for individual
elements such as core use, memory use and accelerator use. E.g., one can get summary reports mentioning the
amount of core hours used per user for a project, but that is reported for all partitions together and hence 
irrelevant to get an idea of how the CPU billing units were consumed.</p>
<p>This section is mostly to discourage you to use <code>sreport</code> as its information is often misleading and certainly
it it is used to follow up your use of billing units on LUMI, but should you insist, there is more information
in the <a href="https://slurm.schedmd.com/archive/slurm-22.05.10/sreport.html"><code>sreport</code> manual page</a>.</p>
<!-- BELGIUM
## Local trainings and materials

-   Docs VSC: Slurm material for the general docs is still under development and will be published
    later in 2023.

    -   [Slurm in the UAntwerp-specific documentation](https://docs.vscentrum.be/en/latest/antwerp/uantwerp_software_specifics.html#slurm-workload-manager)

    -   [Slurm in the VUB-specific documentation](https://hpc.vub.be/docs/job-submission/)

-   Docs CÉCI: Extensive [documentation on the use of Slurm](https://support.ceci-hpc.be/doc/#submitting-jobs-to-the-cluster)

-   VSC training materials

    -   [Slurm Lunchbox training KU Leuven](https://github.com/hpcleuven/Slurm-lunchbox)

    -   [VSC@KULeuven HPC-intro training](https://hpcleuven.github.io/HPC-intro/)
        covers Slurm in the "Starting to Compute" section.

    -   [VSC@UAntwerpen covers Slurm in the "HPC@UAntwerp introduction" training](https://www.uantwerpen.be/en/research-facilities/calcua/training/)

-   CÉCI training materials: Slurm is covered in the "Learning how to use HPC infrastructure" training.

    -   [2022 session: Lecture "Preparing, submitting and managing jobs with Slurm"](https://indico.cism.ucl.ac.be/event/121/contributions/60/)
-->

<!-- GENERAL More general version -->
<h2 id="other-trainings-and-materials">Other trainings and materials<a class="headerlink" href="#other-trainings-and-materials" title="Permanent link">&para;</a></h2>
<ul>
<li>DeiC, the Danish organisation in the LUMI consortium, has develop an
    <a href="http://slurmlearning.deic.dk/">online Slurm tutorial</a></li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        




<footer class="md-footer">

  

  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">

      
    <div class="md-footer-copyright">
      <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">
        <img alt="Creative Commons License" 
             style="border-width:0" 
             src="https://i.creativecommons.org/l/by/4.0/80x15.png"
        />
      </a>
      This work is licensed under a&nbsp;
      <a rel="license" 
         href="http://creativecommons.org/licenses/by/4.0/"
      >
        Creative Commons Attribution 4.0 International License
      </a>
    </div>

      
      <div class="md-social">
  
    
    
    
    
    <a href="https://www.youtube.com/channel/UCb31KOJ6Wqu0sRpIRi_k8Mw" target="_blank" rel="noopener" title="LUMI on YouTube" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://www.linkedin.com/company/lumi-supercomputer" target="_blank" rel="noopener" title="LUMI on LinkedIn" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://twitter.com/LUMIhpc" target="_blank" rel="noopener" title="LUMI on Twitter" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
</div>
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.top", "navigation.indexes", "header.autohide", "toc.follow", "content.code.annotate", "content.code.copy"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.8fd75fb4.min.js"></script>
      
    
  </body>
</html>